use arc_swap::{ArcSwapOption, ArcSwapWeak};
use bytes::{Buf, BufMut};
use futures::FutureExt;
use futures::task::AtomicWaker;
use scc::HashMap as SccHashMap;
use std::cell::UnsafeCell;
use std::fmt::Debug;
use std::future::Future;
use std::mem::MaybeUninit;
use std::sync::OnceLock;
use std::sync::atomic::{
    AtomicBool, AtomicU8, AtomicU16, AtomicU32, AtomicU64, AtomicUsize, Ordering,
};
use std::task::{Context, Poll};
use std::{
    collections::HashMap, net::SocketAddr, pin::Pin, sync::Arc, time::Duration, time::Instant,
};
use tokio::io::{AsyncRead, AsyncReadExt, AsyncWrite, AsyncWriteExt, ReadBuf};
use tokio::net::TcpStream;
use tokio::sync::Notify;
use tokio::sync::{OwnedSemaphorePermit, Semaphore, TryAcquireError};
use tokio::task::{AbortHandle, JoinHandle};
use tracing::{debug, error, info, trace, warn};

#[cfg(any(test, feature = "test-helpers", debug_assertions))]
use sha2::{Digest, Sha256};

use crate::{
    GossipError, Result, current_timestamp, framing,
    registry::{GossipRegistry, RegistryMessage, resolve_peer_addr},
};

// ==== SINGLE SOURCE OF TRUTH FOR ALL BUFFER SIZES ====
// THIS is the ONLY place we define the master buffer size!
// Change this ONE constant to adjust ALL buffer behavior system-wide.
pub const MASTER_BUFFER_SIZE: usize = 1024 * 1024; // 1MB - THE source of truth

// All other buffer sizes derive from the master constant - NO MAGIC NUMBERS!
pub const TCP_BUFFER_SIZE: usize = MASTER_BUFFER_SIZE; // BufWriter & io_uring buffer
pub const STREAM_CHUNK_SIZE: usize = MASTER_BUFFER_SIZE; // Streaming chunk size
pub const STREAMING_THRESHOLD: usize = MASTER_BUFFER_SIZE.saturating_sub(1024); // Just under buffer limit

const WRITER_MAX_LATENCY: Duration = Duration::from_micros(50);

struct IoPerfCounters {
    read_calls: AtomicU64,
    read_ns: AtomicU64,
    actor_handle_calls: AtomicU64,
    actor_handle_ns: AtomicU64,
    response_write_calls: AtomicU64,
    response_write_ns: AtomicU64,
}

fn flush_each_actor_response() -> bool {
    static ENABLED: OnceLock<bool> = OnceLock::new();
    *ENABLED.get_or_init(|| {
        std::env::var("KAMEO_REMOTE_FLUSH_EACH_RESPONSE")
            .map(|v| v == "1" || v.eq_ignore_ascii_case("true"))
            .unwrap_or(false)
    })
}

/// Batched actor ask responses generated by the IO task.
///
/// Avoids per-response `write().await` overhead by writing many small frames in a few
/// `write_vectored` calls. Payloads are kept zero-copy via `bytes::Bytes`.
struct ResponseBatch {
    correlation_ids: Vec<u16>,
    payloads: Vec<bytes::Bytes>,
}

impl ResponseBatch {
    fn new(capacity: usize) -> Self {
        Self {
            correlation_ids: Vec::with_capacity(capacity),
            payloads: Vec::with_capacity(capacity),
        }
    }

    fn clear(&mut self) {
        self.correlation_ids.clear();
        self.payloads.clear();
    }

    fn is_empty(&self) -> bool {
        self.payloads.is_empty()
    }

    fn push_bytes(&mut self, correlation_id: u16, payload: bytes::Bytes) {
        self.correlation_ids.push(correlation_id);
        self.payloads.push(payload);
    }
}

async fn write_remaining_chunks<S: AsyncWrite + Unpin>(
    stream: &mut S,
    chunks: &[bytes::Bytes],
    mut offset: usize,
) -> std::io::Result<usize> {
    let mut wrote = 0usize;
    for chunk in chunks {
        if offset >= chunk.len() {
            offset -= chunk.len();
            continue;
        }
        stream.write_all(&chunk[offset..]).await?;
        wrote += chunk.len() - offset;
        offset = 0;
    }
    Ok(wrote)
}

async fn write_chunks_batched<S: AsyncWrite + Unpin>(
    stream: &mut S,
    chunks: &[bytes::Bytes],
) -> std::io::Result<usize> {
    if chunks.is_empty() {
        return Ok(0);
    }

    if !stream.is_write_vectored() {
        return write_remaining_chunks(stream, chunks, 0).await;
    }

    const MAX_IOV: usize = 64;
    let mut total = 0usize;
    let mut idx = 0usize;
    let mut batch_start = 0usize;
    let mut iov: [MaybeUninit<std::io::IoSlice<'_>>; MAX_IOV] = unsafe {
        MaybeUninit::<[MaybeUninit<std::io::IoSlice<'_>>; MAX_IOV]>::uninit().assume_init()
    };

    for (i, chunk) in chunks.iter().enumerate() {
        if idx == 0 {
            batch_start = i;
        }
        iov[idx].write(std::io::IoSlice::new(chunk));
        idx += 1;

        if idx == MAX_IOV {
            let batch = &chunks[batch_start..batch_start + idx];
            let batch_total: usize = batch.iter().map(|c| c.len()).sum();
            let slices = unsafe {
                std::slice::from_raw_parts(iov.as_ptr() as *const std::io::IoSlice<'_>, idx)
            };
            let n = stream.write_vectored(slices).await?;
            total += n;
            if n < batch_total {
                total += write_remaining_chunks(stream, batch, n).await?;
            }
            idx = 0;
        }
    }

    if idx > 0 {
        let batch = &chunks[batch_start..batch_start + idx];
        let batch_total: usize = batch.iter().map(|c| c.len()).sum();
        let slices =
            unsafe { std::slice::from_raw_parts(iov.as_ptr() as *const std::io::IoSlice<'_>, idx) };
        let n = stream.write_vectored(slices).await?;
        total += n;
        if n < batch_total {
            total += write_remaining_chunks(stream, batch, n).await?;
        }
    }

    Ok(total)
}

async fn write_response_batch<S>(
    stream: &mut S,
    bytes_written_counter: &Arc<AtomicUsize>,
    bytes_since_flush: &mut usize,
    batch: &mut ResponseBatch,
) -> Result<()>
where
    S: AsyncWrite + Unpin,
{
    if batch.is_empty() {
        return Ok(());
    }
    debug_assert_eq!(
        batch.correlation_ids.len(),
        batch.payloads.len(),
        "ResponseBatch must keep correlation_ids and payloads in sync"
    );

    const HDR_LEN: usize = crate::framing::ASK_RESPONSE_FRAME_HEADER_LEN;

    // Flattened slice index: [h0, p0, h1, p1, ...]
    let total_slices = batch.payloads.len().saturating_mul(2);
    let mut cur_slice = 0usize;
    let mut cur_off = 0usize;

    // Larger iov batches reduce syscalls under heavy ActorAsk load. 256 is well below typical
    // platform limits (often 1024) while keeping stack scratch buffers modest.
    const MAX_IOV: usize = 256;
    let mut iov: [MaybeUninit<std::io::IoSlice<'_>>; MAX_IOV] = unsafe {
        MaybeUninit::<[MaybeUninit<std::io::IoSlice<'_>>; MAX_IOV]>::uninit().assume_init()
    };
    // Scratch headers for this writev attempt. We regenerate headers from (corr_id, payload_len)
    // so we don't need to store per-response headers on the heap.
    let mut scratch_headers: [MaybeUninit<[u8; HDR_LEN]>; MAX_IOV] =
        unsafe { MaybeUninit::<[MaybeUninit<[u8; HDR_LEN]>; MAX_IOV]>::uninit().assume_init() };

    while cur_slice < total_slices {
        let mut iov_len = 0usize;
        // Build up to MAX_IOV slices starting from current position.
        while iov_len < MAX_IOV && (cur_slice + iov_len) < total_slices {
            let k = cur_slice + iov_len;
            let resp_idx = k / 2;
            let is_header = (k & 1) == 0;
            let slice_bytes: &[u8] = if is_header {
                let header = crate::framing::write_ask_response_header(
                    crate::MessageType::Response,
                    batch.correlation_ids[resp_idx],
                    batch.payloads[resp_idx].len(),
                );
                // Use raw pointers so the borrow checker understands per-index disjointness.
                let hdr_slot = unsafe { scratch_headers.as_mut_ptr().add(iov_len) };
                unsafe {
                    (*hdr_slot).write(header);
                    (&*(*hdr_slot).as_ptr()).as_slice()
                }
            } else {
                batch.payloads[resp_idx].as_ref()
            };
            let slice_bytes = if iov_len == 0 && cur_off != 0 {
                &slice_bytes[cur_off..]
            } else {
                slice_bytes
            };
            if slice_bytes.is_empty() {
                // Skip empty payloads/headers (shouldn't happen for headers).
                if iov_len == 0 {
                    cur_slice += 1;
                    cur_off = 0;
                    continue;
                } else {
                    break;
                }
            }
            iov[iov_len].write(std::io::IoSlice::new(slice_bytes));
            iov_len += 1;
        }

        if iov_len == 0 {
            break;
        }

        let slices = unsafe {
            std::slice::from_raw_parts(iov.as_ptr() as *const std::io::IoSlice<'_>, iov_len)
        };

        let n = stream
            .write_vectored(slices)
            .await
            .map_err(GossipError::Network)?;
        if n == 0 {
            return Err(GossipError::Network(std::io::Error::new(
                std::io::ErrorKind::WriteZero,
                "write_vectored returned 0",
            )));
        }
        bytes_written_counter.fetch_add(n, Ordering::Relaxed);
        *bytes_since_flush += n;

        // Advance across slices by `n` bytes.
        let mut remaining = n;
        while remaining > 0 && cur_slice < total_slices {
            let resp_idx = cur_slice / 2;
            let is_header = (cur_slice & 1) == 0;
            let slice_len = if is_header {
                HDR_LEN
            } else {
                batch.payloads[resp_idx].len()
            };
            let avail = slice_len.saturating_sub(cur_off);
            if remaining < avail {
                cur_off += remaining;
                remaining = 0;
            } else {
                remaining -= avail;
                cur_slice += 1;
                cur_off = 0;
            }
        }
    }

    batch.clear();
    Ok(())
}

impl IoPerfCounters {
    fn global() -> &'static IoPerfCounters {
        static PERF: OnceLock<IoPerfCounters> = OnceLock::new();
        PERF.get_or_init(|| IoPerfCounters {
            read_calls: AtomicU64::new(0),
            read_ns: AtomicU64::new(0),
            actor_handle_calls: AtomicU64::new(0),
            actor_handle_ns: AtomicU64::new(0),
            response_write_calls: AtomicU64::new(0),
            response_write_ns: AtomicU64::new(0),
        })
    }

    fn enabled() -> bool {
        static ENABLED: OnceLock<bool> = OnceLock::new();
        *ENABLED.get_or_init(|| {
            std::env::var("KAMEO_REMOTE_IO_PERF")
                .map(|v| v == "1" || v.eq_ignore_ascii_case("true"))
                .unwrap_or(false)
        })
    }

    fn interval() -> Duration {
        static INTERVAL: OnceLock<Duration> = OnceLock::new();
        *INTERVAL.get_or_init(|| {
            let ms = std::env::var("KAMEO_REMOTE_IO_PERF_MS")
                .ok()
                .and_then(|v| v.parse::<u64>().ok())
                .filter(|v| *v > 0)
                .unwrap_or(1000);
            Duration::from_millis(ms)
        })
    }

    fn snapshot_and_reset(&self) -> (u64, u64, u64, u64, u64, u64) {
        (
            self.read_calls.swap(0, Ordering::Relaxed),
            self.read_ns.swap(0, Ordering::Relaxed),
            self.actor_handle_calls.swap(0, Ordering::Relaxed),
            self.actor_handle_ns.swap(0, Ordering::Relaxed),
            self.response_write_calls.swap(0, Ordering::Relaxed),
            self.response_write_ns.swap(0, Ordering::Relaxed),
        )
    }
}

/// Tracks spawned tasks for a connection.
///
/// ## Lifecycle Rules (H-004)
/// - `set_writer`/`set_reader` should only be called when establishing NEW connections
/// - Do NOT call set_* during reconnect while messages are in flight
/// - Call `abort_all()` only when the connection is being permanently torn down
/// - For reconnection: let old tasks complete naturally, then set new handles
#[derive(Debug, Default)]
pub struct TaskTracker {
    writer: ArcSwapOption<AbortHandle>,
    reader: ArcSwapOption<AbortHandle>,
}

impl TaskTracker {
    /// Create a new empty TaskTracker.
    pub fn new() -> Self {
        Self::default()
    }

    /// Set writer handle. Only call for NEW connections, not reconnects.
    pub fn set_writer(&self, handle: AbortHandle) {
        if let Some(old) = self.writer.swap(Some(Arc::new(handle))) {
            old.abort();
        }
    }

    /// Set reader handle. Only call for NEW connections, not reconnects.
    pub fn set_reader(&self, handle: AbortHandle) {
        if let Some(old) = self.reader.swap(Some(Arc::new(handle))) {
            old.abort();
        }
    }

    /// Abort all tracked tasks.
    pub fn abort_all(&self) {
        if let Some(h) = self.writer.swap(None) {
            h.abort();
        }
        if let Some(h) = self.reader.swap(None) {
            h.abort();
        }
    }
}

impl Drop for TaskTracker {
    fn drop(&mut self) {
        self.abort_all();
    }
}

fn should_flush(
    bytes_since_flush: usize,
    elapsed: Duration,
    flush_threshold: usize,
    max_latency: Duration,
) -> bool {
    if bytes_since_flush == 0 {
        return false;
    }

    if bytes_since_flush >= flush_threshold {
        return true;
    }

    elapsed >= max_latency
}

/// Buffer configuration that encapsulates all buffer-related settings
///
/// This struct ensures that the streaming threshold is always derived from the actual buffer size,
/// preventing the disconnection between buffer size and streaming decisions that causes message loss.
#[derive(Debug, Clone)]
pub struct BufferConfig {
    tcp_buffer_size: usize, // Size in bytes for TCP buffers
    ask_inflight_limit: usize,
    write_queue_capacity: usize,
}

struct WriteQueue {
    queue: crossbeam_queue::ArrayQueue<WriteCommand>,
    data_notify: Notify,
    space_notify: Notify,
}

impl WriteQueue {
    fn new(capacity: usize) -> Arc<Self> {
        Arc::new(Self {
            queue: crossbeam_queue::ArrayQueue::new(capacity.max(128)),
            data_notify: Notify::new(),
            space_notify: Notify::new(),
        })
    }

    fn try_push(&self, command: WriteCommand) -> std::result::Result<(), WriteCommand> {
        self.queue.push(command)
    }

    async fn push(&self, mut command: WriteCommand) -> Result<()> {
        loop {
            match self.queue.push(command) {
                Ok(()) => {
                    self.data_notify.notify_one();
                    return Ok(());
                }
                Err(cmd) => {
                    command = cmd;
                    self.space_notify.notified().await;
                }
            }
        }
    }

    fn pop(&self) -> Option<WriteCommand> {
        self.queue.pop()
    }

    fn notify_space(&self) {
        self.space_notify.notify_one();
    }
}

struct StreamingQueue {
    queue: crossbeam_queue::ArrayQueue<StreamingCommand>,
    data_notify: Notify,
    space_notify: Notify,
}

impl StreamingQueue {
    fn new(capacity: usize) -> Arc<Self> {
        Arc::new(Self {
            queue: crossbeam_queue::ArrayQueue::new(capacity.max(64)),
            data_notify: Notify::new(),
            space_notify: Notify::new(),
        })
    }

    fn try_push(&self, command: StreamingCommand) -> std::result::Result<(), StreamingCommand> {
        match self.queue.push(command) {
            Ok(()) => {
                self.data_notify.notify_one();
                Ok(())
            }
            Err(cmd) => Err(cmd),
        }
    }

    async fn push(&self, mut command: StreamingCommand) -> Result<()> {
        loop {
            match self.queue.push(command) {
                Ok(()) => {
                    self.data_notify.notify_one();
                    return Ok(());
                }
                Err(cmd) => {
                    command = cmd;
                    self.space_notify.notified().await;
                }
            }
        }
    }

    fn pop(&self) -> Option<StreamingCommand> {
        self.queue.pop()
    }

    fn notify_space(&self) {
        self.space_notify.notify_one();
    }
}

#[cfg(any(test, feature = "test-helpers", debug_assertions))]
pub(crate) fn process_mock_request(request: &str) -> Vec<u8> {
    if let Some(payload) = request.strip_prefix("ECHO:") {
        return format!("ECHOED:{}", payload).into_bytes();
    }
    if let Some(payload) = request.strip_prefix("REVERSE:") {
        let reversed: String = payload.chars().rev().collect();
        return format!("REVERSED:{}", reversed).into_bytes();
    }
    if let Some(payload) = request.strip_prefix("COUNT:") {
        return format!("COUNTED:{} chars", payload.chars().count()).into_bytes();
    }
    if let Some(payload) = request.strip_prefix("HASH:") {
        let mut hasher = Sha256::new();
        hasher.update(payload.as_bytes());
        let digest = hasher.finalize();
        return format!("HASHED:{}", hex::encode(digest)).into_bytes();
    }

    format!("RECEIVED:{} bytes, content: '{}'", request.len(), request).into_bytes()
}

#[cfg(any(test, feature = "test-helpers", debug_assertions))]
pub(crate) fn process_mock_request_payload(payload: &[u8]) -> Vec<u8> {
    if payload.len() == 4 {
        let value = u32::from_be_bytes([payload[0], payload[1], payload[2], payload[3]]);
        return (value + 1).to_be_bytes().to_vec();
    }

    let request_str = String::from_utf8_lossy(payload);
    process_mock_request(&request_str)
}

impl BufferConfig {
    /// Create a new BufferConfig with validation
    ///
    /// # Arguments
    /// * `tcp_buffer_size` - The size of TCP buffers in bytes
    ///
    /// # Errors
    /// Returns an error if the buffer size is less than 256KB for safety
    pub fn new(tcp_buffer_size: usize) -> Result<Self> {
        // Enforce minimum size of 256KB for safety
        if tcp_buffer_size < 256 * 1024 {
            return Err(GossipError::InvalidConfig(format!(
                "TCP buffer must be at least 256KB, got {}KB",
                tcp_buffer_size / 1024
            )));
        }
        Ok(Self {
            tcp_buffer_size,
            ask_inflight_limit: crate::config::DEFAULT_ASK_INFLIGHT_LIMIT,
            write_queue_capacity: crate::config::DEFAULT_ASK_INFLIGHT_LIMIT.saturating_mul(8),
        })
    }

    /// Max in-flight ask permits per connection.
    pub fn ask_inflight_limit(&self) -> usize {
        self.ask_inflight_limit
    }

    /// Override the default ask inflight limit.
    pub fn with_ask_inflight_limit(mut self, limit: usize) -> Self {
        self.ask_inflight_limit = limit;
        let min_capacity = limit.saturating_mul(4).max(128);
        if self.write_queue_capacity < min_capacity {
            self.write_queue_capacity = min_capacity;
        }
        self
    }

    /// Override write queue capacity (number of queued payloads before backpressure).
    pub fn with_write_queue_capacity(mut self, capacity: usize) -> Self {
        self.write_queue_capacity = capacity.max(128);
        self
    }

    /// Capacity of the write queue used by the lock-free writer task.
    pub fn write_queue_capacity(&self) -> usize {
        self.write_queue_capacity
    }

    /// Calculate the streaming threshold based on buffer size
    ///
    /// The streaming threshold is always derived from the buffer size,
    /// leaving 1KB headroom for headers and serialization overhead.
    pub fn streaming_threshold(&self) -> usize {
        // Always derive from buffer size
        // Leave 1KB headroom for headers/overhead
        self.tcp_buffer_size.saturating_sub(1024)
    }

    /// Get the TCP buffer size for BufWriter and io_uring
    ///
    /// This ensures TCP buffers match the configured size, preventing bottlenecks.
    pub fn tcp_buffer_size(&self) -> usize {
        self.tcp_buffer_size
    }

    /// Create BufferConfig using the master buffer size (recommended)
    pub fn from_master() -> Self {
        Self {
            tcp_buffer_size: MASTER_BUFFER_SIZE,
            ask_inflight_limit: crate::config::DEFAULT_ASK_INFLIGHT_LIMIT,
            write_queue_capacity: crate::config::DEFAULT_ASK_INFLIGHT_LIMIT.saturating_mul(8),
        }
    }
}

impl Default for BufferConfig {
    fn default() -> Self {
        // Use master constant instead of magic number!
        Self {
            tcp_buffer_size: MASTER_BUFFER_SIZE,
            ask_inflight_limit: crate::config::DEFAULT_ASK_INFLIGHT_LIMIT,
            write_queue_capacity: crate::config::DEFAULT_ASK_INFLIGHT_LIMIT.saturating_mul(8),
        }
    }
}

/// Stream frame types for high-performance streaming protocol
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u8)]
pub enum StreamFrameType {
    Data = 0x01,
    Ack = 0x02,
    Close = 0x03,
    Heartbeat = 0x04,
    TellAsk = 0x05,    // Regular tell/ask messages
    StreamData = 0x06, // Dedicated streaming data
}

/// Channel IDs for stream multiplexing
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u8)]
pub enum ChannelId {
    TellAsk = 0x00,  // Regular tell/ask channel
    Stream1 = 0x01,  // Dedicated streaming channel 1
    Stream2 = 0x02,  // Dedicated streaming channel 2
    Stream3 = 0x03,  // Dedicated streaming channel 3
    Bulk = 0x04,     // Bulk data channel
    Priority = 0x05, // Priority streaming channel
    Global = 0xFF,   // Global channel for all operations
}

/// Stream frame flags
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u8)]
pub enum StreamFrameFlags {
    None = 0x00,
    More = 0x01,       // More frames to follow
    Compressed = 0x02, // Frame is compressed
    Encrypted = 0x04,  // Frame is encrypted
}

/// Stream frame header for structured messaging
#[derive(Debug, Clone, Copy, rkyv::Archive, rkyv::Serialize, rkyv::Deserialize)]
#[rkyv(derive(Debug))]
pub struct StreamFrameHeader {
    pub frame_type: u8,
    pub channel_id: u8,
    pub flags: u8,
    pub sequence_id: u16,
    pub payload_len: u32,
}

/// Lock-free connection state representation
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u32)]
pub enum ConnectionState {
    Disconnected = 0,
    Connecting = 1,
    Connected = 2,
    Failed = 3,
}

impl From<u32> for ConnectionState {
    fn from(value: u32) -> Self {
        match value {
            0 => ConnectionState::Disconnected,
            1 => ConnectionState::Connecting,
            2 => ConnectionState::Connected,
            3 => ConnectionState::Failed,
            _ => ConnectionState::Failed,
        }
    }
}

/// Direction of the TCP connection relative to this node
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ConnectionDirection {
    Inbound,
    Outbound,
}

/// Lock-free connection metadata
#[derive(Debug)]
pub struct LockFreeConnection {
    pub addr: SocketAddr,
    pub state: AtomicU32,       // ConnectionState
    pub last_used: AtomicUsize, // Timestamp
    pub bytes_written: AtomicUsize,
    pub bytes_read: AtomicUsize,
    pub failure_count: AtomicUsize,
    pub stream_handle: Option<Arc<LockFreeStreamHandle>>,
    pub(crate) correlation: Option<Arc<CorrelationTracker>>,
    pub direction: ConnectionDirection,
    /// Embedded peer_id for this connection
    /// IMPORTANT: This allows looking up peer_id for inbound connections even after
    /// addr_to_peer_id mapping has been migrated to bind address (ephemeral port removed)
    pub(crate) embedded_peer_id: Option<crate::PeerId>,
    /// Task tracker for background tasks (writer and reader)
    pub task_tracker: TaskTracker,
}

impl Clone for LockFreeConnection {
    fn clone(&self) -> Self {
        Self {
            addr: self.addr,
            state: AtomicU32::new(self.state.load(Ordering::Relaxed)),
            last_used: AtomicUsize::new(self.last_used.load(Ordering::Relaxed)),
            bytes_written: AtomicUsize::new(self.bytes_written.load(Ordering::Relaxed)),
            bytes_read: AtomicUsize::new(self.bytes_read.load(Ordering::Relaxed)),
            failure_count: AtomicUsize::new(self.failure_count.load(Ordering::Relaxed)),
            stream_handle: self.stream_handle.clone(),
            correlation: self.correlation.clone(),
            direction: self.direction,
            embedded_peer_id: self.embedded_peer_id.clone(),
            // Note: TaskTracker is not cloned - each clone gets a fresh tracker
            // This is intentional: clones are typically used for metadata snapshots,
            // not to transfer task ownership
            task_tracker: TaskTracker::new(),
        }
    }
}

impl LockFreeConnection {
    pub fn new(addr: SocketAddr, direction: ConnectionDirection) -> Self {
        Self {
            addr,
            state: AtomicU32::new(ConnectionState::Disconnected as u32),
            last_used: AtomicUsize::new(0),
            bytes_written: AtomicUsize::new(0),
            bytes_read: AtomicUsize::new(0),
            failure_count: AtomicUsize::new(0),
            stream_handle: None,
            correlation: Some(CorrelationTracker::new()),
            direction,
            embedded_peer_id: None,
            task_tracker: TaskTracker::new(),
        }
    }

    /// Abort all tracked background tasks (writer, reader).
    /// Call this when tearing down the connection to prevent resource leaks.
    pub fn abort_tasks(&self) {
        // NOTE: `JoinHandle::abort()` does not run destructors inside the task. Our IO task
        // sets `exit_flag` and cancels all pending correlation slots via an `ExitGuard` in Drop.
        // If we abort without also flipping these flags, callers can observe a "zombie handle"
        // where `ConnectionHandle::is_closed()` remains false and asks hang until timeout.
        if let Some(handle) = self.stream_handle.as_ref() {
            handle.shutdown_signal.store(true, Ordering::Release);
            handle.exit_flag.store(true, Ordering::Release);
            handle.exit_notify.notify_waiters();
        }
        if let Some(correlation) = self.correlation.as_ref() {
            correlation.cancel_all();
        }
        self.task_tracker.abort_all();
    }

    pub fn get_state(&self) -> ConnectionState {
        self.state.load(Ordering::Acquire).into()
    }

    pub fn set_state(&self, state: ConnectionState) {
        self.state.store(state as u32, Ordering::Release);
    }

    pub fn try_set_state(&self, expected: ConnectionState, new: ConnectionState) -> bool {
        self.state
            .compare_exchange(
                expected as u32,
                new as u32,
                Ordering::AcqRel,
                Ordering::Acquire,
            )
            .is_ok()
    }

    pub fn update_last_used(&self) {
        self.last_used
            .store(crate::current_timestamp() as usize, Ordering::Release);
    }

    pub fn increment_failure_count(&self) -> usize {
        self.failure_count.fetch_add(1, Ordering::AcqRel)
    }

    pub fn reset_failure_count(&self) {
        self.failure_count.store(0, Ordering::Release);
    }

    pub fn is_connected(&self) -> bool {
        self.get_state() == ConnectionState::Connected
    }

    pub fn is_failed(&self) -> bool {
        self.get_state() == ConnectionState::Failed
    }
}

/// Payloads for queued writes.
pub enum WritePayload {
    Single(bytes::Bytes),
    HeaderPayload {
        header: bytes::Bytes,
        payload: bytes::Bytes,
    },
    HeaderInline {
        header: [u8; 16],
        header_len: u8,
        payload: bytes::Bytes,
    },
    HeaderInlineAligned {
        header: [u8; 16],
        header_len: u8,
        payload: crate::AlignedBytes,
    },
    HeaderInline32 {
        header: [u8; 32],
        payload: bytes::Bytes,
    },
    HeaderPooled {
        header: bytes::Bytes,
        prefix: Option<bytes::Bytes>,
        payload: crate::typed::PooledPayload,
    },
    HeaderInlinePooled {
        header: [u8; 16],
        header_len: u8,
        prefix: Option<[u8; 16]>,
        prefix_len: u8,
        payload: crate::typed::PooledPayload,
    },
    /// DirectAsk fast path - header is [length:4][type:1][correlation_id:2][payload_len:4]
    DirectAskInline {
        header: [u8; 16], // DIRECT_ASK_FRAME_HEADER_LEN
        payload: bytes::Bytes,
    },
    Buf(Box<dyn Buf + Send>),
}

impl std::fmt::Debug for WritePayload {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            WritePayload::Single(data) => f.debug_tuple("Single").field(&data.len()).finish(),
            WritePayload::HeaderPayload { header, payload } => f
                .debug_struct("HeaderPayload")
                .field("header_len", &header.len())
                .field("payload_len", &payload.len())
                .finish(),
            WritePayload::HeaderInline {
                header_len,
                payload,
                ..
            } => f
                .debug_struct("HeaderInline")
                .field("header_len", &header_len)
                .field("payload_len", &payload.len())
                .finish(),
            WritePayload::HeaderInlineAligned {
                header_len,
                payload,
                ..
            } => f
                .debug_struct("HeaderInlineAligned")
                .field("header_len", &header_len)
                .field("payload_len", &payload.len())
                .finish(),
            WritePayload::HeaderInline32 { payload, .. } => f
                .debug_struct("HeaderInline32")
                .field("header_len", &32)
                .field("payload_len", &payload.len())
                .finish(),
            WritePayload::HeaderPooled {
                header,
                prefix,
                payload,
            } => f
                .debug_struct("HeaderPooled")
                .field("header_len", &header.len())
                .field("prefix_len", &prefix.as_ref().map(|p| p.len()).unwrap_or(0))
                .field("payload_len", &payload.len())
                .finish(),
            WritePayload::HeaderInlinePooled {
                header_len,
                prefix,
                prefix_len,
                payload,
                ..
            } => f
                .debug_struct("HeaderInlinePooled")
                .field("header_len", &header_len)
                .field(
                    "prefix_len",
                    &if prefix.is_some() { *prefix_len } else { 0 },
                )
                .field("payload_len", &payload.len())
                .finish(),
            WritePayload::Buf(_) => f.debug_tuple("Buf").field(&"<buf>").finish(),
            WritePayload::DirectAskInline { header: _, payload } => f
                .debug_struct("DirectAskInline")
                .field("header_len", &crate::framing::DIRECT_ASK_FRAME_HEADER_LEN)
                .field("payload_len", &payload.len())
                .finish(),
        }
    }
}

// `WritePayload` is passed across tasks/threads by move.
// Do not add `Sync` here: the `Buf` variant is not required to be `Sync`.

/// Memory pool for zero-allocation message handling
#[derive(Debug)]
pub struct MessageBufferPool {
    queue: crossbeam_queue::ArrayQueue<Vec<u8>>,
}

impl MessageBufferPool {
    pub fn new(pool_size: usize, buffer_size: usize) -> Self {
        let pool_size = pool_size.max(1);
        let queue = crossbeam_queue::ArrayQueue::new(pool_size);
        for _ in 0..pool_size {
            let _ = queue.push(Vec::with_capacity(buffer_size));
        }
        Self { queue }
    }

    /// Get a buffer from the pool - returns None if pool is empty
    pub fn get_buffer(&self) -> Option<Vec<u8>> {
        self.queue.pop().map(|mut buffer| {
            buffer.clear();
            buffer
        })
    }

    /// Return a buffer to the pool
    pub fn return_buffer(&self, mut buffer: Vec<u8>) {
        buffer.clear(); // Reset length but keep capacity
        let _ = self.queue.push(buffer);
    }

    pub fn available_count(&self) -> usize {
        self.queue.len()
    }

    pub fn is_empty(&self) -> bool {
        self.available_count() == 0
    }
}

/// Vectored write command for zero-copy header + payload operations
#[derive(Debug)]
pub struct VectoredSendItem {
    header: bytes::Bytes,
    payload: bytes::Bytes,
}

#[derive(Clone)]
#[doc(hidden)]
pub struct ReadContext {
    pub(crate) registry_weak: std::sync::Weak<GossipRegistry>,
    pub(crate) peer_addr: SocketAddr,
    /// Best-effort peer identity for this connection.
    ///
    /// This is used to avoid mis-attributing disconnects from stale/duplicate
    /// connections (for example tie-breaker drops during simultaneous dial).
    pub(crate) peer_id: Option<crate::PeerId>,
    pub(crate) max_message_size: usize,
    pub(crate) aligned_pool: Arc<crate::AlignedBytesPool>,
    pub(crate) response_correlation: Option<Arc<CorrelationTracker>>,
}

enum ReadState {
    ReadLen {
        buf: [u8; crate::framing::LENGTH_PREFIX_LEN],
        read: usize,
    },
    ReadBody {
        msg_len: usize,
        buffer: crate::PooledAlignedBuffer,
        read: usize,
    },
}

impl ReadState {
    fn new() -> Self {
        Self::ReadLen {
            buf: [0u8; crate::framing::LENGTH_PREFIX_LEN],
            read: 0,
        }
    }
}

async fn read_message_step<S>(
    stream: &mut S,
    state: &mut ReadState,
    ctx: &ReadContext,
) -> Result<Option<crate::handle::MessageReadResult>>
where
    S: AsyncRead + Unpin,
{
    match state {
        ReadState::ReadLen { buf, read } => {
            let n = stream.read(&mut buf[*read..]).await?;
            if n == 0 {
                return Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::UnexpectedEof,
                    "connection closed",
                )));
            }
            *read += n;
            if *read < crate::framing::LENGTH_PREFIX_LEN {
                return Ok(None);
            }

            let msg_len = u32::from_be_bytes(*buf) as usize;
            if msg_len > ctx.max_message_size {
                return Err(GossipError::MessageTooLarge {
                    size: msg_len,
                    max: ctx.max_message_size,
                });
            }

            let total_len = msg_len + crate::framing::LENGTH_PREFIX_LEN;
            let mut buffer =
                crate::PooledAlignedBuffer::with_len(total_len, ctx.aligned_pool.clone());
            buffer.as_mut_slice()[..crate::framing::LENGTH_PREFIX_LEN].copy_from_slice(buf);

            *state = ReadState::ReadBody {
                msg_len,
                buffer,
                read: 0,
            };
            Ok(None)
        }
        ReadState::ReadBody {
            msg_len,
            buffer,
            read,
        } => {
            let offset = crate::framing::LENGTH_PREFIX_LEN + *read;
            let end = crate::framing::LENGTH_PREFIX_LEN + *msg_len;
            let n = stream.read(&mut buffer.as_mut_slice()[offset..end]).await?;
            if n == 0 {
                return Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::UnexpectedEof,
                    "connection closed",
                )));
            }
            *read += n;
            if *read < *msg_len {
                return Ok(None);
            }

            let (msg_len, buffer) = match std::mem::replace(state, ReadState::new()) {
                ReadState::ReadBody {
                    msg_len, buffer, ..
                } => (msg_len, buffer),
                _ => unreachable!("read state must be ReadBody when complete"),
            };

            let result = crate::handle::parse_message_from_pooled_buffer(buffer, msg_len)?;
            Ok(Some(result))
        }
    }
}

struct ReadPollResult {
    result: Option<crate::handle::MessageReadResult>,
    progressed: bool,
}

fn poll_read_once<S>(stream: &mut S, cx: &mut Context<'_>, buf: &mut [u8]) -> Poll<Result<usize>>
where
    S: AsyncRead + Unpin,
{
    let mut read_buf = ReadBuf::new(buf);
    match Pin::new(stream).poll_read(cx, &mut read_buf) {
        Poll::Pending => Poll::Pending,
        Poll::Ready(Ok(())) => Poll::Ready(Ok(read_buf.filled().len())),
        Poll::Ready(Err(e)) => Poll::Ready(Err(GossipError::Network(e))),
    }
}

async fn read_message_step_poll<S>(
    stream: &mut S,
    state: &mut ReadState,
    ctx: &ReadContext,
    block_on_pending: bool,
) -> Result<ReadPollResult>
where
    S: AsyncRead + Unpin,
{
    futures::future::poll_fn(|cx| match state {
        ReadState::ReadLen { buf, read } => {
            let target = &mut buf[*read..];
            if target.is_empty() {
                return Poll::Ready(Ok(ReadPollResult {
                    result: None,
                    progressed: false,
                }));
            }
            match poll_read_once(stream, cx, target) {
                Poll::Pending => {
                    if block_on_pending {
                        Poll::Pending
                    } else {
                        Poll::Ready(Ok(ReadPollResult {
                            result: None,
                            progressed: false,
                        }))
                    }
                }
                Poll::Ready(Ok(0)) => Poll::Ready(Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::UnexpectedEof,
                    "connection closed",
                )))),
                Poll::Ready(Ok(n)) => {
                    *read += n;
                    if *read < crate::framing::LENGTH_PREFIX_LEN {
                        return Poll::Ready(Ok(ReadPollResult {
                            result: None,
                            progressed: true,
                        }));
                    }

                    let msg_len = u32::from_be_bytes(*buf) as usize;
                    if msg_len > ctx.max_message_size {
                        return Poll::Ready(Err(GossipError::MessageTooLarge {
                            size: msg_len,
                            max: ctx.max_message_size,
                        }));
                    }

                    let total_len = msg_len + crate::framing::LENGTH_PREFIX_LEN;
                    let mut buffer =
                        crate::PooledAlignedBuffer::with_len(total_len, ctx.aligned_pool.clone());
                    buffer.as_mut_slice()[..crate::framing::LENGTH_PREFIX_LEN].copy_from_slice(buf);

                    *state = ReadState::ReadBody {
                        msg_len,
                        buffer,
                        read: 0,
                    };
                    Poll::Ready(Ok(ReadPollResult {
                        result: None,
                        progressed: true,
                    }))
                }
                Poll::Ready(Err(e)) => Poll::Ready(Err(e)),
            }
        }
        ReadState::ReadBody {
            msg_len,
            buffer,
            read,
        } => {
            let offset = crate::framing::LENGTH_PREFIX_LEN + *read;
            let end = crate::framing::LENGTH_PREFIX_LEN + *msg_len;
            let target = &mut buffer.as_mut_slice()[offset..end];
            if target.is_empty() {
                return Poll::Ready(Ok(ReadPollResult {
                    result: None,
                    progressed: false,
                }));
            }
            match poll_read_once(stream, cx, target) {
                Poll::Pending => {
                    if block_on_pending {
                        Poll::Pending
                    } else {
                        Poll::Ready(Ok(ReadPollResult {
                            result: None,
                            progressed: false,
                        }))
                    }
                }
                Poll::Ready(Ok(0)) => Poll::Ready(Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::UnexpectedEof,
                    "connection closed",
                )))),
                Poll::Ready(Ok(n)) => {
                    *read += n;
                    if *read < *msg_len {
                        return Poll::Ready(Ok(ReadPollResult {
                            result: None,
                            progressed: true,
                        }));
                    }

                    let (msg_len, buffer) = match std::mem::replace(state, ReadState::new()) {
                        ReadState::ReadBody {
                            msg_len, buffer, ..
                        } => (msg_len, buffer),
                        _ => unreachable!("read state must be ReadBody when complete"),
                    };

                    let result = crate::handle::parse_message_from_pooled_buffer(buffer, msg_len)?;
                    Poll::Ready(Ok(ReadPollResult {
                        result: Some(result),
                        progressed: true,
                    }))
                }
                Poll::Ready(Err(e)) => Poll::Ready(Err(e)),
            }
        }
    })
    .await
}

/// Non-blocking variant of `read_message_step_poll`.
///
/// This is used in the IO task "did_work" batch path to avoid a subtle deadlock:
/// awaiting a `poll_read` future will park the entire IO task until the socket is readable,
/// which prevents draining the write queue for newly enqueued asks/tells.
///
/// Semantics: if the socket is not currently readable, returns `progressed=false` immediately.
async fn read_message_step_nonblocking<S>(
    stream: &mut S,
    state: &mut ReadState,
    ctx: &ReadContext,
) -> Result<ReadPollResult>
where
    S: AsyncRead + Unpin,
{
    futures::future::poll_fn(|cx| match state {
        ReadState::ReadLen { buf, read } => {
            let target = &mut buf[*read..];
            if target.is_empty() {
                return Poll::Ready(Ok(ReadPollResult {
                    result: None,
                    progressed: false,
                }));
            }
            match poll_read_once(stream, cx, target) {
                Poll::Pending => Poll::Ready(Ok(ReadPollResult {
                    result: None,
                    progressed: false,
                })),
                Poll::Ready(Ok(0)) => Poll::Ready(Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::UnexpectedEof,
                    "connection closed",
                )))),
                Poll::Ready(Ok(n)) => {
                    *read += n;
                    if *read < crate::framing::LENGTH_PREFIX_LEN {
                        return Poll::Ready(Ok(ReadPollResult {
                            result: None,
                            progressed: true,
                        }));
                    }

                    let msg_len = u32::from_be_bytes(*buf) as usize;
                    if msg_len > ctx.max_message_size {
                        return Poll::Ready(Err(GossipError::MessageTooLarge {
                            size: msg_len,
                            max: ctx.max_message_size,
                        }));
                    }

                    let total_len = msg_len + crate::framing::LENGTH_PREFIX_LEN;
                    let mut buffer =
                        crate::PooledAlignedBuffer::with_len(total_len, ctx.aligned_pool.clone());
                    buffer.as_mut_slice()[..crate::framing::LENGTH_PREFIX_LEN].copy_from_slice(buf);

                    *state = ReadState::ReadBody {
                        msg_len,
                        buffer,
                        read: 0,
                    };
                    Poll::Ready(Ok(ReadPollResult {
                        result: None,
                        progressed: true,
                    }))
                }
                Poll::Ready(Err(e)) => Poll::Ready(Err(e)),
            }
        }
        ReadState::ReadBody {
            msg_len,
            buffer,
            read,
        } => {
            let offset = crate::framing::LENGTH_PREFIX_LEN + *read;
            let end = crate::framing::LENGTH_PREFIX_LEN + *msg_len;
            let target = &mut buffer.as_mut_slice()[offset..end];
            if target.is_empty() {
                return Poll::Ready(Ok(ReadPollResult {
                    result: None,
                    progressed: false,
                }));
            }
            match poll_read_once(stream, cx, target) {
                Poll::Pending => Poll::Ready(Ok(ReadPollResult {
                    result: None,
                    progressed: false,
                })),
                Poll::Ready(Ok(0)) => Poll::Ready(Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::UnexpectedEof,
                    "connection closed",
                )))),
                Poll::Ready(Ok(n)) => {
                    *read += n;
                    if *read < *msg_len {
                        return Poll::Ready(Ok(ReadPollResult {
                            result: None,
                            progressed: true,
                        }));
                    }

                    let (msg_len, buffer) = match std::mem::replace(state, ReadState::new()) {
                        ReadState::ReadBody {
                            msg_len, buffer, ..
                        } => (msg_len, buffer),
                        _ => unreachable!("read state must be ReadBody when complete"),
                    };

                    let result = crate::handle::parse_message_from_pooled_buffer(buffer, msg_len)?;
                    Poll::Ready(Ok(ReadPollResult {
                        result: Some(result),
                        progressed: true,
                    }))
                }
                Poll::Ready(Err(e)) => Poll::Ready(Err(e)),
            }
        }
    })
    .await
}

async fn write_header_payload_vectored<S>(
    stream: &mut S,
    bytes_written_counter: &Arc<AtomicUsize>,
    bytes_since_flush: &mut usize,
    header: &[u8],
    payload: &[u8],
) -> Result<()>
where
    S: AsyncWrite + Unpin,
{
    if payload.is_empty() {
        stream
            .write_all(header)
            .await
            .map_err(GossipError::Network)?;
        bytes_written_counter.fetch_add(header.len(), Ordering::Relaxed);
        *bytes_since_flush += header.len();
        return Ok(());
    }

    let slices = [
        std::io::IoSlice::new(header),
        std::io::IoSlice::new(payload),
    ];
    match stream.write_vectored(&slices).await {
        Ok(n) if n == header.len() + payload.len() => {
            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
            *bytes_since_flush += n;
            Ok(())
        }
        Ok(n) => {
            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
            *bytes_since_flush += n;
            if n < header.len() {
                stream
                    .write_all(&header[n..])
                    .await
                    .map_err(GossipError::Network)?;
                bytes_written_counter.fetch_add(header.len() - n, Ordering::Relaxed);
                *bytes_since_flush += header.len() - n;
                if !payload.is_empty() {
                    stream
                        .write_all(payload)
                        .await
                        .map_err(GossipError::Network)?;
                    bytes_written_counter.fetch_add(payload.len(), Ordering::Relaxed);
                    *bytes_since_flush += payload.len();
                }
            } else {
                let payload_offset = n - header.len();
                if payload_offset < payload.len() {
                    stream
                        .write_all(&payload[payload_offset..])
                        .await
                        .map_err(GossipError::Network)?;
                    bytes_written_counter
                        .fetch_add(payload.len() - payload_offset, Ordering::Relaxed);
                    *bytes_since_flush += payload.len() - payload_offset;
                }
            }
            Ok(())
        }
        Err(e) => Err(GossipError::Network(e)),
    }
}

async fn write_actor_response_direct<S>(
    stream: &mut S,
    bytes_written_counter: &Arc<AtomicUsize>,
    bytes_since_flush: &mut usize,
    correlation_id: u16,
    response: crate::registry::ActorResponse,
) -> Result<()>
where
    S: AsyncWrite + Unpin,
{
    match response {
        crate::registry::ActorResponse::Bytes(bytes) => {
            let header = crate::framing::write_ask_response_header(
                crate::MessageType::Response,
                correlation_id,
                bytes.len(),
            );
            write_header_payload_vectored(
                stream,
                bytes_written_counter,
                bytes_since_flush,
                &header,
                &bytes,
            )
            .await?;
        }
        crate::registry::ActorResponse::Aligned(bytes) => {
            let header = crate::framing::write_ask_response_header(
                crate::MessageType::Response,
                correlation_id,
                bytes.len(),
            );
            write_header_payload_vectored(
                stream,
                bytes_written_counter,
                bytes_since_flush,
                &header,
                bytes.as_ref(),
            )
            .await?;
        }
        crate::registry::ActorResponse::Pooled {
            payload,
            prefix,
            payload_len,
        } => {
            let header = crate::framing::write_ask_response_header(
                crate::MessageType::Response,
                correlation_id,
                payload_len,
            );
            stream
                .write_all(&header)
                .await
                .map_err(GossipError::Network)?;
            bytes_written_counter.fetch_add(header.len(), Ordering::Relaxed);
            *bytes_since_flush += header.len();

            if let Some(prefix) = prefix {
                stream
                    .write_all(&prefix)
                    .await
                    .map_err(GossipError::Network)?;
                bytes_written_counter.fetch_add(prefix.len(), Ordering::Relaxed);
                *bytes_since_flush += prefix.len();
            }

            let mut payload = payload;
            while payload.has_remaining() {
                let chunk = payload.chunk();
                if chunk.is_empty() {
                    break;
                }
                stream
                    .write_all(chunk)
                    .await
                    .map_err(GossipError::Network)?;
                bytes_written_counter.fetch_add(chunk.len(), Ordering::Relaxed);
                *bytes_since_flush += chunk.len();
                payload.advance(chunk.len());
            }
        }
    }

    Ok(())
}

async fn write_streaming_response_direct<S>(
    stream: &mut S,
    bytes_written_counter: &Arc<AtomicUsize>,
    bytes_since_flush: &mut usize,
    correlation_id: u16,
    payload: bytes::Bytes,
    max_message_size: usize,
    schema_hash: Option<u64>,
) -> Result<()>
where
    S: AsyncWrite + Unpin,
{
    use bytes::BufMut;

    // Streaming frame wire format excludes the 4-byte length prefix from `msg_len`.
    // `msg_len` for stream data frames is: type(1) + corr(2) + reserved(9) + header(36) + chunk(N).
    const STREAM_FRAME_OVERHEAD: usize = 12 + crate::StreamHeader::SERIALIZED_SIZE;
    let max_chunk = max_message_size.saturating_sub(STREAM_FRAME_OVERHEAD);
    if max_chunk == 0 {
        return Err(GossipError::InvalidConfig(format!(
            "max_message_size={} too small for streaming (overhead={})",
            max_message_size, STREAM_FRAME_OVERHEAD
        )));
    }
    let chunk_size = std::cmp::min(STREAM_CHUNK_SIZE, max_chunk);

    // Generate unique stream ID for this response stream.
    let stream_id = crate::current_timestamp_nanos();

    fn build_stream_response_header(
        msg_type: crate::MessageType,
        header: &crate::StreamHeader,
        correlation_id: u16,
        chunk_len: usize,
        schema_hash: Option<u64>,
    ) -> bytes::Bytes {
        // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36]
        let inner_size = 12 + crate::StreamHeader::SERIALIZED_SIZE + chunk_len;
        let mut message =
            bytes::BytesMut::with_capacity(4 + 12 + crate::StreamHeader::SERIALIZED_SIZE);

        message.put_u32(inner_size as u32);
        message.put_u8(msg_type as u8);
        message.put_u16(correlation_id);

        let mut reserved = [0u8; 9];
        crate::framing::write_schema_hash(&mut reserved, schema_hash);
        message.put_slice(&reserved);
        message.put_slice(&header.to_bytes());

        message.freeze()
    }

    let total_len = payload.len();
    let start_header = crate::StreamHeader {
        stream_id,
        total_size: total_len as u64,
        chunk_size: 0,
        chunk_index: 0,
        type_hash: 0,
        actor_id: 0,
    };

    // StreamResponseStart
    let start_msg = build_stream_response_header(
        crate::MessageType::StreamResponseStart,
        &start_header,
        correlation_id,
        0,
        schema_hash,
    );
    stream
        .write_all(&start_msg)
        .await
        .map_err(GossipError::Network)?;
    bytes_written_counter.fetch_add(start_msg.len(), Ordering::Relaxed);
    *bytes_since_flush += start_msg.len();

    // StreamResponseData
    let num_chunks = total_len.div_ceil(chunk_size);
    for idx in 0..num_chunks {
        let start = idx * chunk_size;
        let end = std::cmp::min(start + chunk_size, total_len);
        let chunk_len = end - start;
        let chunk_data = payload.slice(start..end);

        let data_header = crate::StreamHeader {
            stream_id,
            total_size: total_len as u64,
            chunk_size: chunk_len as u32,
            chunk_index: idx as u32,
            type_hash: 0,
            actor_id: 0,
        };

        let header_bytes = build_stream_response_header(
            crate::MessageType::StreamResponseData,
            &data_header,
            correlation_id,
            chunk_len,
            schema_hash,
        );

        write_header_payload_vectored(
            stream,
            bytes_written_counter,
            bytes_since_flush,
            &header_bytes,
            chunk_data.as_ref(),
        )
        .await?;
    }

    // StreamResponseEnd
    let end_msg = build_stream_response_header(
        crate::MessageType::StreamResponseEnd,
        &start_header,
        correlation_id,
        0,
        schema_hash,
    );
    stream
        .write_all(&end_msg)
        .await
        .map_err(GossipError::Network)?;
    bytes_written_counter.fetch_add(end_msg.len(), Ordering::Relaxed);
    *bytes_since_flush += end_msg.len();

    stream.flush().await.map_err(GossipError::Network)?;
    *bytes_since_flush = 0;

    Ok(())
}

async fn write_streaming_response_direct_pooled<S>(
    stream: &mut S,
    bytes_written_counter: &Arc<AtomicUsize>,
    bytes_since_flush: &mut usize,
    correlation_id: u16,
    mut payload: crate::typed::PooledPayload,
    prefix: Option<[u8; 16]>,
    payload_len: usize,
    max_message_size: usize,
    schema_hash: Option<u64>,
) -> Result<()>
where
    S: AsyncWrite + Unpin,
{
    use bytes::BufMut;

    // Streaming frame wire format excludes the 4-byte length prefix from `msg_len`.
    // `msg_len` for stream data frames is: type(1) + corr(2) + reserved(9) + header(36) + chunk(N).
    const STREAM_FRAME_OVERHEAD: usize = 12 + crate::StreamHeader::SERIALIZED_SIZE;
    let max_chunk = max_message_size.saturating_sub(STREAM_FRAME_OVERHEAD);
    if max_chunk == 0 {
        return Err(GossipError::InvalidConfig(format!(
            "max_message_size={} too small for streaming (overhead={})",
            max_message_size, STREAM_FRAME_OVERHEAD
        )));
    }
    let chunk_size = std::cmp::min(STREAM_CHUNK_SIZE, max_chunk);

    let prefix_len = prefix.as_ref().map(|p| p.len()).unwrap_or(0);
    if prefix_len > payload_len {
        return Err(GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::InvalidData,
            "pooled response prefix_len exceeds payload_len",
        )));
    }
    let expected_payload_bytes = payload_len - prefix_len;
    if payload.remaining() < expected_payload_bytes {
        return Err(GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::InvalidData,
            "pooled response payload shorter than payload_len",
        )));
    }

    // Generate unique stream ID for this response stream.
    let stream_id = crate::current_timestamp_nanos();

    fn build_stream_response_header(
        msg_type: crate::MessageType,
        header: &crate::StreamHeader,
        correlation_id: u16,
        chunk_len: usize,
        schema_hash: Option<u64>,
    ) -> bytes::Bytes {
        // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36]
        let inner_size = 12 + crate::StreamHeader::SERIALIZED_SIZE + chunk_len;
        let mut message =
            bytes::BytesMut::with_capacity(4 + 12 + crate::StreamHeader::SERIALIZED_SIZE);

        message.put_u32(inner_size as u32);
        message.put_u8(msg_type as u8);
        message.put_u16(correlation_id);

        let mut reserved = [0u8; 9];
        crate::framing::write_schema_hash(&mut reserved, schema_hash);
        message.put_slice(&reserved);
        message.put_slice(&header.to_bytes());

        message.freeze()
    }

    let total_len = payload_len;
    let start_header = crate::StreamHeader {
        stream_id,
        total_size: total_len as u64,
        chunk_size: 0,
        chunk_index: 0,
        type_hash: 0,
        actor_id: 0,
    };

    // StreamResponseStart
    let start_msg = build_stream_response_header(
        crate::MessageType::StreamResponseStart,
        &start_header,
        correlation_id,
        0,
        schema_hash,
    );
    stream
        .write_all(&start_msg)
        .await
        .map_err(GossipError::Network)?;
    bytes_written_counter.fetch_add(start_msg.len(), Ordering::Relaxed);
    *bytes_since_flush += start_msg.len();

    // StreamResponseData
    let mut prefix_pos = 0usize;
    let prefix_bytes: Option<&[u8]> = prefix.as_ref().map(|p| p.as_slice());

    let mut remaining_total = total_len;
    let mut idx = 0usize;
    while remaining_total > 0 {
        let this_chunk = std::cmp::min(chunk_size, remaining_total);

        let data_header = crate::StreamHeader {
            stream_id,
            total_size: total_len as u64,
            chunk_size: this_chunk as u32,
            chunk_index: idx as u32,
            type_hash: 0,
            actor_id: 0,
        };

        let header_bytes = build_stream_response_header(
            crate::MessageType::StreamResponseData,
            &data_header,
            correlation_id,
            this_chunk,
            schema_hash,
        );

        stream
            .write_all(&header_bytes)
            .await
            .map_err(GossipError::Network)?;
        bytes_written_counter.fetch_add(header_bytes.len(), Ordering::Relaxed);
        *bytes_since_flush += header_bytes.len();

        // Write chunk bytes from: prefix (if any) then pooled payload Buf.
        let mut remaining_in_chunk = this_chunk;
        if let Some(prefix) = prefix_bytes {
            if prefix_pos < prefix.len() && remaining_in_chunk > 0 {
                let take = std::cmp::min(remaining_in_chunk, prefix.len() - prefix_pos);
                stream
                    .write_all(&prefix[prefix_pos..prefix_pos + take])
                    .await
                    .map_err(GossipError::Network)?;
                bytes_written_counter.fetch_add(take, Ordering::Relaxed);
                *bytes_since_flush += take;
                prefix_pos += take;
                remaining_in_chunk -= take;
            }
        }

        while remaining_in_chunk > 0 {
            let chunk = payload.chunk();
            if chunk.is_empty() {
                return Err(GossipError::Network(std::io::Error::new(
                    std::io::ErrorKind::InvalidData,
                    "pooled payload returned empty chunk while bytes remain",
                )));
            }
            let take = std::cmp::min(remaining_in_chunk, chunk.len());
            stream
                .write_all(&chunk[..take])
                .await
                .map_err(GossipError::Network)?;
            bytes_written_counter.fetch_add(take, Ordering::Relaxed);
            *bytes_since_flush += take;
            payload.advance(take);
            remaining_in_chunk -= take;
        }

        remaining_total -= this_chunk;
        idx += 1;
    }

    // StreamResponseEnd
    let end_msg = build_stream_response_header(
        crate::MessageType::StreamResponseEnd,
        &start_header,
        correlation_id,
        0,
        schema_hash,
    );
    stream
        .write_all(&end_msg)
        .await
        .map_err(GossipError::Network)?;
    bytes_written_counter.fetch_add(end_msg.len(), Ordering::Relaxed);
    *bytes_since_flush += end_msg.len();

    stream.flush().await.map_err(GossipError::Network)?;
    *bytes_since_flush = 0;

    Ok(())
}

async fn process_read_result_io<S>(
    result: crate::handle::MessageReadResult,
    streaming_state: &mut crate::protocol::StreamingState,
    registry: &Arc<GossipRegistry>,
    peer_addr: SocketAddr,
    response_correlation: Option<&CorrelationTracker>,
    stream: &mut S,
    bytes_written_counter: &Arc<AtomicUsize>,
    bytes_since_flush: &mut usize,
    response_batch: &mut ResponseBatch,
    perf: Option<&IoPerfCounters>,
) -> Result<()>
where
    S: AsyncWrite + Unpin,
{
    match result {
        crate::handle::MessageReadResult::Actor {
            msg_type,
            correlation_id,
            actor_id,
            type_hash,
            schema_hash,
            payload,
        } => {
            if let Some(expected) = registry.config.schema_hash {
                if schema_hash != Some(expected) {
                    warn!(
                        peer = %peer_addr,
                        expected = format_args!("{:016x}", expected),
                        received = schema_hash
                            .map(|hash| format!("{hash:016x}"))
                            .unwrap_or_else(|| "none".to_string()),
                        "Rejected actor payload due to schema hash mismatch"
                    );
                    return Ok(());
                }
            }

            let corr_id = if msg_type == crate::MessageType::ActorAsk as u8 {
                correlation_id
            } else {
                0
            };
            let correlation_opt = if corr_id == 0 { None } else { Some(corr_id) };
            let handle_start = perf.map(|_| Instant::now());
            let response = registry
                .handle_actor_message(actor_id, type_hash, payload, correlation_opt)
                .await;
            if let (Some(perf), Some(start)) = (perf, handle_start) {
                perf.actor_handle_calls.fetch_add(1, Ordering::Relaxed);
                perf.actor_handle_ns
                    .fetch_add(start.elapsed().as_nanos() as u64, Ordering::Relaxed);
            }
            if let Ok(Some(response)) = response {
                if corr_id != 0 {
                    let write_start = perf.map(|_| Instant::now());
                    let inline_payload_limit = registry
                        .config
                        .max_message_size
                        .saturating_sub(crate::framing::ASK_RESPONSE_HEADER_LEN);
                    let schema_hash = registry.config.schema_hash;
                    match response {
                        // Hot path (console bench): zero-copy batchable payloads.
                        crate::registry::ActorResponse::Bytes(payload) => {
                            let should_stream = payload.len() > inline_payload_limit
                                || payload.len() > STREAMING_THRESHOLD;
                            if should_stream {
                                write_streaming_response_direct(
                                    stream,
                                    bytes_written_counter,
                                    bytes_since_flush,
                                    corr_id,
                                    payload,
                                    registry.config.max_message_size,
                                    schema_hash,
                                )
                                .await?;
                                if flush_each_actor_response() {
                                    stream.flush().await.map_err(GossipError::Network)?;
                                    *bytes_since_flush = 0;
                                }
                            } else {
                                response_batch.push_bytes(corr_id, payload);
                            }
                        }
                        crate::registry::ActorResponse::Aligned(payload) => {
                            let len = payload.len();
                            let should_stream =
                                len > inline_payload_limit || len > STREAMING_THRESHOLD;
                            if should_stream {
                                write_streaming_response_direct(
                                    stream,
                                    bytes_written_counter,
                                    bytes_since_flush,
                                    corr_id,
                                    payload.into_bytes(),
                                    registry.config.max_message_size,
                                    schema_hash,
                                )
                                .await?;
                                if flush_each_actor_response() {
                                    stream.flush().await.map_err(GossipError::Network)?;
                                    *bytes_since_flush = 0;
                                }
                            } else {
                                response_batch.push_bytes(corr_id, payload.into_bytes());
                            }
                        }
                        // Less common: keep correctness, allow existing slow-path writes.
                        other => {
                            let should_stream = match &other {
                                crate::registry::ActorResponse::Pooled { payload_len, .. } => {
                                    *payload_len > inline_payload_limit
                                        || *payload_len > STREAMING_THRESHOLD
                                }
                                _ => false,
                            };
                            if should_stream {
                                if let crate::registry::ActorResponse::Pooled {
                                    payload,
                                    prefix,
                                    payload_len,
                                } = other
                                {
                                    // Stream pooled responses directly from the Buf (no materialization copy).
                                    write_streaming_response_direct_pooled(
                                        stream,
                                        bytes_written_counter,
                                        bytes_since_flush,
                                        corr_id,
                                        payload,
                                        prefix,
                                        payload_len,
                                        registry.config.max_message_size,
                                        schema_hash,
                                    )
                                    .await?;
                                } else {
                                    // Non-pooled non-hot-path variant: stream by converting to Bytes.
                                    let bytes = match other {
                                        crate::registry::ActorResponse::Bytes(b) => b,
                                        crate::registry::ActorResponse::Aligned(b) => {
                                            b.into_bytes()
                                        }
                                        crate::registry::ActorResponse::Pooled { .. } => {
                                            unreachable!()
                                        }
                                    };
                                    write_streaming_response_direct(
                                        stream,
                                        bytes_written_counter,
                                        bytes_since_flush,
                                        corr_id,
                                        bytes,
                                        registry.config.max_message_size,
                                        schema_hash,
                                    )
                                    .await?;
                                }
                                if flush_each_actor_response() {
                                    stream.flush().await.map_err(GossipError::Network)?;
                                    *bytes_since_flush = 0;
                                }
                            } else {
                                write_actor_response_direct(
                                    stream,
                                    bytes_written_counter,
                                    bytes_since_flush,
                                    corr_id,
                                    other,
                                )
                                .await?;
                                if flush_each_actor_response() {
                                    stream.flush().await.map_err(GossipError::Network)?;
                                    *bytes_since_flush = 0;
                                }
                            }
                        }
                    }
                    if let (Some(perf), Some(start)) = (perf, write_start) {
                        perf.response_write_calls.fetch_add(1, Ordering::Relaxed);
                        perf.response_write_ns
                            .fetch_add(start.elapsed().as_nanos() as u64, Ordering::Relaxed);
                    }
                }
            }
            Ok(())
        }
        other => {
            crate::protocol::process_read_result(
                other,
                streaming_state,
                registry,
                peer_addr,
                response_correlation,
                None,
            )
            .await
        }
    }
}

/// Commands for the per-connection writer.
#[derive(Debug)]
enum WriteCommand {
    /// Queued payload writes (tell/ask/control frames).
    Payload(WritePayload),
    /// Payload writes that hold an ask permit until written.
    PayloadWithPermit {
        payload: WritePayload,
        permit: OwnedSemaphorePermit,
    },
}

/// Commands for streaming operations.
#[derive(Debug)]
enum StreamingCommand {
    /// Direct write bytes for streaming.
    WriteBytes(bytes::Bytes),
    /// Flush the writer.
    Flush,
    /// Vectored write for header + payload (zero-copy).
    VectoredWrite(VectoredSendItem),
    /// Batch of owned chunks for streaming (zero-copy).
    OwnedChunks(Vec<bytes::Bytes>),
}

/// Truly lock-free streaming handle with dedicated background writer
#[derive(Clone)]
pub struct LockFreeStreamHandle {
    /// Unique per-handle id used to ignore disconnect callbacks from stale connections.
    instance_id: u64,
    addr: SocketAddr,
    channel_id: ChannelId,
    sequence_counter: Arc<AtomicUsize>,
    frame_sequence: Arc<AtomicUsize>,
    bytes_written: Arc<AtomicUsize>, // This tracks actual TCP bytes written
    shutdown_signal: Arc<AtomicBool>,
    exit_flag: Arc<AtomicBool>,
    exit_notify: Arc<Notify>,
    flush_pending: Arc<AtomicBool>,
    ask_permits: Arc<Semaphore>,
    /// Atomic flag for coordinating streaming mode
    streaming_active: Arc<AtomicBool>,
    /// Lock-free write queue for payload writes
    write_queue: Arc<WriteQueue>,
    /// Bounded streaming command queue for background task.
    streaming_queue: Arc<StreamingQueue>,
    /// Buffer configuration that determines sizes and thresholds
    buffer_config: BufferConfig,
    /// Max allowed frame payload size (msg_len, excluding 4-byte length prefix).
    ///
    /// This comes from the registry config and is used to bound streaming chunk sizes so
    /// streaming frames themselves never exceed the reader limit.
    max_message_size: usize,
    /// Optional schema/version hash for protocol guardrails.
    schema_hash: Option<u64>,
}

impl LockFreeStreamHandle {
    fn next_instance_id() -> u64 {
        static NEXT_ID: AtomicU64 = AtomicU64::new(1);
        NEXT_ID.fetch_add(1, Ordering::Relaxed)
    }

    pub fn instance_id(&self) -> u64 {
        self.instance_id
    }

    /// Create a new lock-free streaming handle with background writer task
    ///
    /// Returns a tuple of (Self, JoinHandle) where the JoinHandle can be used
    /// to track and abort the background writer task (H-004 task tracking).
    pub fn new<S>(
        stream: S,
        addr: SocketAddr,
        channel_id: ChannelId,
        buffer_config: BufferConfig,
        schema_hash: Option<u64>,
        read_context: Option<ReadContext>,
    ) -> (Self, JoinHandle<()>)
    where
        S: AsyncRead + AsyncWrite + Unpin + Send + 'static,
    {
        let instance_id = Self::next_instance_id();
        let shutdown_signal = Arc::new(AtomicBool::new(false));
        let streaming_active = Arc::new(AtomicBool::new(false));
        let flush_pending = Arc::new(AtomicBool::new(false));
        let exit_flag = Arc::new(AtomicBool::new(false));
        let exit_notify = Arc::new(Notify::new());
        let ask_limit = buffer_config.ask_inflight_limit().max(1);
        let ask_permits = Arc::new(Semaphore::new(ask_limit));

        // Create shared counter for actual TCP bytes written
        let bytes_written = Arc::new(AtomicUsize::new(0));

        // Create lock-free queue for payload writes and channel for streaming commands
        let write_queue = WriteQueue::new(buffer_config.write_queue_capacity());
        let streaming_queue = StreamingQueue::new(buffer_config.write_queue_capacity());

        let max_message_size = read_context
            .as_ref()
            .map(|ctx| ctx.max_message_size)
            .unwrap_or(MASTER_BUFFER_SIZE);
        // Spawn background writer task with exclusive TCP access - NO MUTEX!
        let writer_handle = {
            let shutdown_signal = shutdown_signal.clone();
            let bytes_written_for_task = bytes_written.clone();
            let streaming_active_for_task = streaming_active.clone();
            let flush_pending_for_task = flush_pending.clone();
            let exit_flag_for_task = exit_flag.clone();
            let exit_notify_for_task = exit_notify.clone();
            let writer_addr = addr;
            let writer_channel_id = channel_id;
            let write_queue = write_queue.clone();
            let streaming_queue = streaming_queue.clone();

            tokio::spawn(async move {
                info!(
                    addr = %writer_addr,
                    channel_id = ?writer_channel_id,
                    " Background writer task started"
                );
                Self::io_task(
                    stream,
                    shutdown_signal,
                    bytes_written_for_task,
                    flush_pending_for_task,
                    streaming_active_for_task,
                    write_queue,
                    streaming_queue,
                    read_context,
                    instance_id,
                    exit_flag_for_task,
                    exit_notify_for_task,
                )
                .await;
                // CRITICAL: Log when writer exits - this helps diagnose silent writer deaths
                warn!(
                    addr = %writer_addr,
                    channel_id = ?writer_channel_id,
                    " Background writer task EXITED - no more writes possible on this connection!"
                );
            })
        };

        (
            Self {
                instance_id,
                addr,
                channel_id,
                sequence_counter: Arc::new(AtomicUsize::new(0)),
                frame_sequence: Arc::new(AtomicUsize::new(0)),
                bytes_written, // This now tracks actual TCP bytes written
                shutdown_signal,
                flush_pending,
                exit_flag,
                exit_notify,
                ask_permits,
                streaming_active,
                write_queue,
                streaming_queue,
                buffer_config,
                max_message_size,
                schema_hash,
            },
            writer_handle,
        )
    }

    /// Single IO task - owns the TLS stream for both read and write.
    /// OPTIMIZED FOR MAXIMUM THROUGHPUT - NO MUTEX NEEDED!
    #[allow(clippy::too_many_arguments)]
    async fn io_task<S>(
        stream: S,
        shutdown_signal: Arc<AtomicBool>,
        bytes_written_counter: Arc<AtomicUsize>, // Track ALL bytes written to TCP
        flush_pending: Arc<AtomicBool>,
        streaming_active: Arc<AtomicBool>,
        write_queue: Arc<WriteQueue>,
        streaming_queue: Arc<StreamingQueue>,
        read_context: Option<ReadContext>,
        instance_id: u64,
        exit_flag: Arc<AtomicBool>,
        exit_notify: Arc<Notify>,
    ) where
        S: AsyncRead + AsyncWrite + Unpin + Send + 'static,
    {
        // CRITICAL_PATH: owner-batched send queue + vectored TLS writes.
        use std::io::IoSlice;

        async fn write_vectored_all<S>(
            stream: &mut S,
            slices: &[IoSlice<'_>],
        ) -> std::io::Result<usize>
        where
            S: AsyncWrite + Unpin,
        {
            let total_len: usize = slices.iter().map(|s| s.len()).sum();
            if total_len == 0 {
                return Ok(0);
            }

            // If the underlying stream doesn't support vectored writes, preserve ordering by
            // writing each slice sequentially.
            if !stream.is_write_vectored() {
                for s in slices {
                    stream.write_all(s.as_ref()).await?;
                }
                return Ok(total_len);
            }

            let n = stream.write_vectored(slices).await?;
            if n == total_len {
                return Ok(n);
            }

            // Short write: complete the remainder sequentially without allocating a combined buffer.
            let mut idx = 0usize;
            let mut off = n;
            while idx < slices.len() && off >= slices[idx].len() {
                off -= slices[idx].len();
                idx += 1;
            }
            if idx < slices.len() {
                if off < slices[idx].len() {
                    let b = &slices[idx].as_ref()[off..];
                    stream.write_all(b).await?;
                    idx += 1;
                }
                while idx < slices.len() {
                    stream.write_all(slices[idx].as_ref()).await?;
                    idx += 1;
                }
            }
            Ok(total_len)
        }

        struct ExitGuard {
            flag: Arc<AtomicBool>,
            notify: Arc<Notify>,
            response_correlation: Option<Arc<CorrelationTracker>>,
            registry_weak: Option<std::sync::Weak<GossipRegistry>>,
            peer_addr: Option<SocketAddr>,
            peer_id: Option<crate::PeerId>,
            instance_id: u64,
        }

        impl Drop for ExitGuard {
            fn drop(&mut self) {
                self.flag.store(true, Ordering::Release);
                self.notify.notify_waiters();
                if let Some(correlation) = self.response_correlation.as_ref() {
                    correlation.cancel_all();
                }
                if let (Some(registry_weak), Some(peer_addr)) =
                    (self.registry_weak.as_ref(), self.peer_addr)
                {
                    if let Some(registry) = registry_weak.upgrade() {
                        // Guard against mis-attributing disconnects from stale/duplicate
                        // connections. Tie-breaker drops and replacements are expected and
                        // must not mark the peer failed or drop the currently active link.
                        let expected_instance = self.instance_id;
                        let peer_id_hint = self.peer_id.clone();
                        tokio::spawn(async move {
                            let pool = &registry.connection_pool;
                            let peer_id =
                                peer_id_hint.or_else(|| pool.get_peer_id_by_addr(&peer_addr));

                            if let Some(peer_id) = peer_id {
                                if let Some(current) = pool.get_connection_by_peer_id(&peer_id) {
                                    if let Some(handle) = current.stream_handle.as_ref() {
                                        if handle.instance_id() != expected_instance {
                                            debug!(
                                                peer = %peer_addr,
                                                peer_id = %peer_id,
                                                exiting_instance = expected_instance,
                                                current_instance = handle.instance_id(),
                                                "IO task exited for stale connection; skipping failure handling"
                                            );
                                            return;
                                        }
                                    }
                                }
                            } else if let Some(current) = pool.get_lock_free_connection(peer_addr) {
                                if let Some(handle) = current.stream_handle.as_ref() {
                                    if handle.instance_id() != expected_instance {
                                        debug!(
                                            peer = %peer_addr,
                                            exiting_instance = expected_instance,
                                            current_instance = handle.instance_id(),
                                            "IO task exited for stale addr-mapped connection; skipping failure handling"
                                        );
                                        return;
                                    }
                                }
                            }

                            if let Err(e) = registry.handle_peer_connection_failure(peer_addr).await
                            {
                                warn!(
                                    peer = %peer_addr,
                                    error = %e,
                                    "IO task failure handling failed"
                                );
                            }
                        });
                    }
                }
            }
        }

        let _exit_guard = ExitGuard {
            flag: exit_flag,
            notify: exit_notify,
            response_correlation: read_context
                .as_ref()
                .and_then(|ctx| ctx.response_correlation.clone()),
            registry_weak: read_context.as_ref().map(|ctx| ctx.registry_weak.clone()),
            peer_addr: read_context.as_ref().map(|ctx| ctx.peer_addr),
            peer_id: read_context.as_ref().and_then(|ctx| ctx.peer_id.clone()),
            instance_id,
        };

        let perf = if IoPerfCounters::enabled() {
            Some(IoPerfCounters::global())
        } else {
            None
        };
        let mut perf_last = Instant::now();
        let perf_interval = perf
            .map(|_| IoPerfCounters::interval())
            .unwrap_or_else(|| Duration::from_secs(1));
        if perf.is_some() {
            if let Some(ctx) = read_context.as_ref() {
                info!(peer = %ctx.peer_addr, "IO PERF enabled");
            } else {
                info!("IO PERF enabled (no read context)");
            }
        }

        // Buffered stream for both reads and writes.
        //
        // On tokio-rustls, direct `write_vectored` to the TLS stream tends to degenerate into
        // per-slice writes (effectively non-vectored), which destroys ActorAsk throughput.
        // `BufStream` coalesces writes efficiently and keeps the two-terminal benchmark fast.
        let mut stream =
            tokio::io::BufStream::with_capacity(TCP_BUFFER_SIZE, TCP_BUFFER_SIZE, stream);

        // Larger batches for higher throughput (reduced syscalls)
        const OWNER_BATCH_SIZE: usize = 32;
        const READ_BATCH_LIMIT: usize = 512;
        const FLUSH_THRESHOLD: usize = 4 * 1024; // 4KB before flush (much lower for ask/reply)

        let mut bytes_since_flush = 0;
        let mut last_flush = std::time::Instant::now();

        // Pre-allocate reusable buffers to avoid allocations in the hot loop
        let mut write_chunks: Vec<bytes::Bytes> = Vec::with_capacity(OWNER_BATCH_SIZE * 2);
        let mut permits: Vec<OwnedSemaphorePermit> = Vec::with_capacity(OWNER_BATCH_SIZE);
        let mut owner_batch: Vec<WriteCommand> = Vec::with_capacity(OWNER_BATCH_SIZE);
        let mut response_batch = ResponseBatch::new(READ_BATCH_LIMIT);
        let mut pending_cmd: Option<WriteCommand> = None;
        let mut read_state = read_context.as_ref().map(|_| ReadState::new());
        let mut streaming_state = read_context
            .as_ref()
            .map(|_| crate::protocol::StreamingState::new());
        let mut last_cleanup = std::time::Instant::now();

        while !shutdown_signal.load(Ordering::Relaxed) {
            let mut total_bytes_written = 0;
            let mut did_work = false;
            response_batch.clear();

            while let Some(cmd) = streaming_queue.pop() {
                did_work = true;
                match cmd {
                    StreamingCommand::WriteBytes(data) => match stream.write_all(&data).await {
                        Ok(_) => {
                            bytes_written_counter.fetch_add(data.len(), Ordering::Relaxed);
                            total_bytes_written += data.len();
                        }
                        Err(e) => {
                            error!("Streaming write error: {}", e);
                            return;
                        }
                    },
                    StreamingCommand::Flush => {
                        let _ = stream.flush().await;
                        flush_pending.store(false, Ordering::Release);
                        last_flush = std::time::Instant::now();
                        bytes_since_flush = 0;
                    }
                    StreamingCommand::VectoredWrite(cmd) => {
                        // Handle short writes by falling back to sequential write_all
                        // TCP can return partial writes under backpressure
                        let total_len = cmd.header.len() + cmd.payload.len();
                        let header_slice = std::io::IoSlice::new(&cmd.header);
                        let payload_slice = std::io::IoSlice::new(&cmd.payload);
                        let bufs = &[header_slice, payload_slice];

                        match write_vectored_all(&mut stream, bufs).await {
                            Ok(n) if n == total_len => {
                                bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                total_bytes_written += n;
                            }
                            Ok(n) => {
                                // Short write - write remaining bytes sequentially using stack buffer
                                bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                total_bytes_written += n;
                                let _remaining = total_len - n;
                                let mut offset = n;
                                // Write header portion if needed
                                if offset < cmd.header.len() {
                                    let h_rem = cmd.header.len() - offset;
                                    if let Err(_) = stream.write_all(&cmd.header[offset..]).await {
                                        return;
                                    }
                                    bytes_written_counter.fetch_add(h_rem, Ordering::Relaxed);
                                    total_bytes_written += h_rem;
                                    offset = 0;
                                } else {
                                    offset -= cmd.header.len();
                                }
                                // Write payload portion
                                if let Err(_) = stream.write_all(&cmd.payload[offset..]).await {
                                    return;
                                }
                                bytes_written_counter
                                    .fetch_add(cmd.payload.len() - offset, Ordering::Relaxed);
                                total_bytes_written += cmd.payload.len() - offset;
                            }
                            Err(e) => {
                                error!("Vectored write error: {}", e);
                                return;
                            }
                        }
                    }
                    StreamingCommand::OwnedChunks(chunks) => {
                        // Handle short writes for owned chunks
                        let total_len: usize = chunks.iter().map(|c| c.len()).sum();
                        const MAX_IOV: usize = 64;
                        let mut slice_storage: [MaybeUninit<std::io::IoSlice<'_>>; MAX_IOV] = unsafe {
                            MaybeUninit::<[MaybeUninit<std::io::IoSlice<'_>>; MAX_IOV]>::uninit()
                                .assume_init()
                        };
                        let chunk_count = chunks.len().min(MAX_IOV);
                        for (idx, chunk) in chunks.iter().take(MAX_IOV).enumerate() {
                            slice_storage[idx].write(std::io::IoSlice::new(&chunk));
                        }
                        let slices = unsafe {
                            std::slice::from_raw_parts(
                                slice_storage.as_ptr() as *const std::io::IoSlice<'_>,
                                chunk_count,
                            )
                        };

                        match write_vectored_all(&mut stream, slices).await {
                            Ok(n) if n == total_len => {
                                bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                total_bytes_written += n;
                            }
                            Ok(n) => {
                                // Short write - write remaining bytes sequentially
                                bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                total_bytes_written += n;
                                let mut remaining = total_len - n;
                                let mut chunk_idx = 0;
                                let mut offset_in_chunk = n;
                                // Find which chunk we left off in
                                while chunk_idx < chunks.len()
                                    && offset_in_chunk >= chunks[chunk_idx].len()
                                {
                                    offset_in_chunk -= chunks[chunk_idx].len();
                                    chunk_idx += 1;
                                }
                                // Continue writing from current position
                                while chunk_idx < chunks.len() && remaining > 0 {
                                    match stream.write(&chunks[chunk_idx][offset_in_chunk..]).await
                                    {
                                        Ok(written) => {
                                            bytes_written_counter
                                                .fetch_add(written, Ordering::Relaxed);
                                            total_bytes_written += written;
                                            remaining -= written;
                                            offset_in_chunk += written;
                                            if offset_in_chunk >= chunks[chunk_idx].len() {
                                                chunk_idx += 1;
                                                offset_in_chunk = 0;
                                            }
                                        }
                                        Err(e) => {
                                            error!("Chunk batch write completion error: {}", e);
                                            return;
                                        }
                                    }
                                }
                            }
                            Err(e) => {
                                error!("Chunk batch write error: {}", e);
                                return;
                            }
                        }
                    }
                }
                streaming_queue.notify_space();
            }

            if !streaming_active.load(Ordering::Acquire) {
                // Reuse pre-allocated buffers instead of creating new ones
                write_chunks.clear();
                permits.clear();
                owner_batch.clear();

                if let Some(cmd) = pending_cmd.take() {
                    owner_batch.push(cmd);
                }

                while owner_batch.len() < OWNER_BATCH_SIZE {
                    match write_queue.pop() {
                        Some(command) => owner_batch.push(command),
                        None => break,
                    }
                }

                if !owner_batch.is_empty() {
                    did_work = true;
                    for command in owner_batch.drain(..) {
                        let payload = match command {
                            WriteCommand::Payload(payload) => payload,
                            WriteCommand::PayloadWithPermit { payload, permit } => {
                                permits.push(permit);
                                payload
                            }
                        };
                        write_queue.notify_space();
                        match payload {
                            WritePayload::Single(data) => write_chunks.push(data),
                            WritePayload::HeaderPayload { header, payload } => {
                                write_chunks.push(header);
                                write_chunks.push(payload);
                            }
                            WritePayload::HeaderInline {
                                header,
                                header_len,
                                payload,
                            } => {
                                if !write_chunks.is_empty() {
                                    let bytes_written = match write_chunks_batched(
                                        &mut stream,
                                        &write_chunks,
                                    )
                                    .await
                                    {
                                        Ok(n) => n,
                                        Err(_) => return,
                                    };
                                    bytes_written_counter
                                        .fetch_add(bytes_written, Ordering::Relaxed);
                                    total_bytes_written += bytes_written;
                                    write_chunks.clear();
                                }

                                let header_len = header_len as usize;
                                let mut header_off = 0usize;
                                let mut payload_off = 0usize;
                                let payload_len = payload.len();

                                while header_off < header_len || payload_off < payload_len {
                                    let h = &header[header_off..header_len];
                                    let p = &payload[payload_off..];
                                    let mut slices = [IoSlice::new(h), IoSlice::new(p)];
                                    let slice_count = if h.is_empty() {
                                        slices[0] = IoSlice::new(p);
                                        1
                                    } else if p.is_empty() {
                                        slices[0] = IoSlice::new(h);
                                        1
                                    } else {
                                        2
                                    };

                                    match write_vectored_all(&mut stream, &slices[..slice_count])
                                        .await
                                    {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                            if header_off < header_len {
                                                let h_rem = header_len - header_off;
                                                if n < h_rem {
                                                    header_off += n;
                                                    continue;
                                                } else {
                                                    header_off = header_len;
                                                    payload_off += n - h_rem;
                                                }
                                            } else {
                                                payload_off += n;
                                            }
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                            WritePayload::HeaderInlineAligned {
                                header,
                                header_len,
                                payload,
                            } => {
                                if !write_chunks.is_empty() {
                                    let bytes_written = match write_chunks_batched(
                                        &mut stream,
                                        &write_chunks,
                                    )
                                    .await
                                    {
                                        Ok(n) => n,
                                        Err(_) => return,
                                    };
                                    bytes_written_counter
                                        .fetch_add(bytes_written, Ordering::Relaxed);
                                    total_bytes_written += bytes_written;
                                    write_chunks.clear();
                                }

                                let header_len = header_len as usize;
                                let mut header_off = 0usize;
                                let mut payload_off = 0usize;
                                let payload_len = payload.len();
                                let payload_bytes = payload.as_ref();

                                while header_off < header_len || payload_off < payload_len {
                                    let h = &header[header_off..header_len];
                                    let p = &payload_bytes[payload_off..];
                                    let mut slices = [IoSlice::new(h), IoSlice::new(p)];
                                    let slice_count = if h.is_empty() {
                                        slices[0] = IoSlice::new(p);
                                        1
                                    } else if p.is_empty() {
                                        slices[0] = IoSlice::new(h);
                                        1
                                    } else {
                                        2
                                    };

                                    match write_vectored_all(&mut stream, &slices[..slice_count])
                                        .await
                                    {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                            if header_off < header_len {
                                                let h_rem = header_len - header_off;
                                                if n < h_rem {
                                                    header_off += n;
                                                    continue;
                                                } else {
                                                    header_off = header_len;
                                                    payload_off += n - h_rem;
                                                }
                                            } else {
                                                payload_off += n;
                                            }
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                            WritePayload::HeaderInline32 { header, payload } => {
                                if !write_chunks.is_empty() {
                                    let bytes_written = match write_chunks_batched(
                                        &mut stream,
                                        &write_chunks,
                                    )
                                    .await
                                    {
                                        Ok(n) => n,
                                        Err(_) => return,
                                    };
                                    bytes_written_counter
                                        .fetch_add(bytes_written, Ordering::Relaxed);
                                    total_bytes_written += bytes_written;
                                    write_chunks.clear();
                                }

                                let header_len = 32usize;
                                let mut header_off = 0usize;
                                let mut payload_off = 0usize;
                                let payload_len = payload.len();

                                while header_off < header_len || payload_off < payload_len {
                                    let h = &header[header_off..header_len];
                                    let p = &payload[payload_off..];
                                    let mut slices = [IoSlice::new(h), IoSlice::new(p)];
                                    let slice_count = if h.is_empty() {
                                        slices[0] = IoSlice::new(p);
                                        1
                                    } else if p.is_empty() {
                                        slices[0] = IoSlice::new(h);
                                        1
                                    } else {
                                        2
                                    };

                                    match write_vectored_all(&mut stream, &slices[..slice_count])
                                        .await
                                    {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                            if header_off < header_len {
                                                let h_rem = header_len - header_off;
                                                if n < h_rem {
                                                    header_off += n;
                                                    continue;
                                                } else {
                                                    header_off = header_len;
                                                    payload_off += n - h_rem;
                                                }
                                            } else {
                                                payload_off += n;
                                            }
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                            WritePayload::HeaderPooled {
                                header,
                                prefix,
                                mut payload,
                            } => {
                                if !write_chunks.is_empty() {
                                    const MAX_IOV: usize = 64;
                                    // Use drain to preserve buffer capacity
                                    let mut idx = 0;
                                    let mut iov: [MaybeUninit<IoSlice<'_>>; MAX_IOV] = unsafe {
                                        MaybeUninit::<[MaybeUninit<IoSlice<'_>>; MAX_IOV]>::uninit()
                                            .assume_init()
                                    };

                                    for chunk in &write_chunks {
                                        iov[idx].write(IoSlice::new(&chunk));
                                        idx += 1;
                                        if idx == MAX_IOV {
                                            let slices = unsafe {
                                                std::slice::from_raw_parts(
                                                    iov.as_ptr() as *const IoSlice<'_>,
                                                    idx,
                                                )
                                            };
                                            match write_vectored_all(&mut stream, slices).await {
                                                Ok(bytes_written) => {
                                                    bytes_written_counter.fetch_add(
                                                        bytes_written,
                                                        Ordering::Relaxed,
                                                    );
                                                    total_bytes_written += bytes_written;
                                                }
                                                Err(_) => return,
                                            }
                                            idx = 0;
                                        }
                                    }

                                    if idx > 0 {
                                        let slices = unsafe {
                                            std::slice::from_raw_parts(
                                                iov.as_ptr() as *const IoSlice<'_>,
                                                idx,
                                            )
                                        };
                                        match write_vectored_all(&mut stream, slices).await {
                                            Ok(bytes_written) => {
                                                bytes_written_counter
                                                    .fetch_add(bytes_written, Ordering::Relaxed);
                                                total_bytes_written += bytes_written;
                                            }
                                            Err(_) => return,
                                        }
                                    }
                                    write_chunks.clear();
                                }

                                if (stream.write_all(&header).await).is_err() {
                                    return;
                                }
                                bytes_written_counter.fetch_add(header.len(), Ordering::Relaxed);
                                total_bytes_written += header.len();

                                if let Some(prefix) = prefix {
                                    if (stream.write_all(&prefix).await).is_err() {
                                        return;
                                    }
                                    bytes_written_counter
                                        .fetch_add(prefix.len(), Ordering::Relaxed);
                                    total_bytes_written += prefix.len();
                                }

                                while payload.has_remaining() {
                                    match stream.write_buf(&mut payload).await {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                            WritePayload::HeaderInlinePooled {
                                header,
                                header_len,
                                prefix,
                                prefix_len,
                                mut payload,
                            } => {
                                if !write_chunks.is_empty() {
                                    // Use drain to preserve buffer capacity
                                    let mut slices = Vec::with_capacity(write_chunks.len());
                                    for chunk in &write_chunks {
                                        slices.push(IoSlice::new(&chunk));
                                    }
                                    match write_vectored_all(&mut stream, &slices).await {
                                        Ok(bytes_written) => {
                                            bytes_written_counter
                                                .fetch_add(bytes_written, Ordering::Relaxed);
                                            total_bytes_written += bytes_written;
                                        }
                                        Err(_) => return,
                                    }
                                }

                                let header_len = header_len as usize;
                                let prefix_len = prefix_len as usize;
                                let mut header_off = 0usize;
                                let mut prefix_off = 0usize;

                                if let Some(prefix) = prefix {
                                    while header_off < header_len || prefix_off < prefix_len {
                                        let h = &header[header_off..header_len];
                                        let p = &prefix[prefix_off..prefix_len];
                                        let mut slices = [IoSlice::new(h), IoSlice::new(p)];
                                        let slice_count = if h.is_empty() {
                                            slices[0] = IoSlice::new(p);
                                            1
                                        } else if p.is_empty() {
                                            slices[0] = IoSlice::new(h);
                                            1
                                        } else {
                                            2
                                        };

                                        match write_vectored_all(
                                            &mut stream,
                                            &slices[..slice_count],
                                        )
                                        .await
                                        {
                                            Ok(0) => break,
                                            Ok(n) => {
                                                bytes_written_counter
                                                    .fetch_add(n, Ordering::Relaxed);
                                                total_bytes_written += n;
                                                if header_off < header_len {
                                                    let h_rem = header_len - header_off;
                                                    if n < h_rem {
                                                        header_off += n;
                                                        continue;
                                                    } else {
                                                        header_off = header_len;
                                                        prefix_off += n - h_rem;
                                                    }
                                                } else {
                                                    prefix_off += n;
                                                }
                                            }
                                            Err(_) => return,
                                        }
                                    }
                                } else {
                                    while header_off < header_len {
                                        let h = &header[header_off..header_len];
                                        match write_vectored_all(&mut stream, &[IoSlice::new(h)])
                                            .await
                                        {
                                            Ok(0) => break,
                                            Ok(n) => {
                                                bytes_written_counter
                                                    .fetch_add(n, Ordering::Relaxed);
                                                total_bytes_written += n;
                                                header_off += n;
                                            }
                                            Err(_) => return,
                                        }
                                    }
                                }

                                while payload.has_remaining() {
                                    match stream.write_buf(&mut payload).await {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                            WritePayload::Buf(mut buf) => {
                                if !write_chunks.is_empty() {
                                    // Use drain to preserve buffer capacity
                                    let mut slices = Vec::with_capacity(write_chunks.len());
                                    for chunk in &write_chunks {
                                        slices.push(IoSlice::new(&chunk));
                                    }
                                    match write_vectored_all(&mut stream, &slices).await {
                                        Ok(bytes_written) => {
                                            bytes_written_counter
                                                .fetch_add(bytes_written, Ordering::Relaxed);
                                            total_bytes_written += bytes_written;
                                        }
                                        Err(_) => return,
                                    }
                                }

                                while buf.has_remaining() {
                                    match stream.write_buf(&mut buf).await {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                            WritePayload::DirectAskInline { header, payload } => {
                                // DirectAsk fast path: write full framed header + payload
                                const DIRECT_ASK_HEADER_LEN: usize =
                                    crate::framing::DIRECT_ASK_FRAME_HEADER_LEN;
                                if !write_chunks.is_empty() {
                                    // Use drain to preserve buffer capacity
                                    let mut slices = Vec::with_capacity(write_chunks.len());
                                    for chunk in &write_chunks {
                                        slices.push(IoSlice::new(&chunk));
                                    }
                                    match write_vectored_all(&mut stream, &slices).await {
                                        Ok(bytes_written) => {
                                            bytes_written_counter
                                                .fetch_add(bytes_written, Ordering::Relaxed);
                                            total_bytes_written += bytes_written;
                                        }
                                        Err(_) => return,
                                    }
                                }

                                let mut header_off = 0usize;
                                let mut payload_off = 0usize;
                                let payload_len = payload.len();

                                while header_off < DIRECT_ASK_HEADER_LEN
                                    || payload_off < payload_len
                                {
                                    let h = &header[header_off..DIRECT_ASK_HEADER_LEN];
                                    let p = &payload[payload_off..];
                                    let mut slices = [IoSlice::new(h), IoSlice::new(p)];
                                    let slice_count = if h.is_empty() {
                                        slices[0] = IoSlice::new(p);
                                        1
                                    } else if p.is_empty() {
                                        slices[0] = IoSlice::new(h);
                                        1
                                    } else {
                                        2
                                    };

                                    match write_vectored_all(&mut stream, &slices[..slice_count])
                                        .await
                                    {
                                        Ok(0) => break,
                                        Ok(n) => {
                                            bytes_written_counter.fetch_add(n, Ordering::Relaxed);
                                            total_bytes_written += n;
                                            if header_off < DIRECT_ASK_HEADER_LEN {
                                                let h_rem = DIRECT_ASK_HEADER_LEN - header_off;
                                                if n < h_rem {
                                                    header_off += n;
                                                    continue;
                                                } else {
                                                    header_off = DIRECT_ASK_HEADER_LEN;
                                                    payload_off += n - h_rem;
                                                }
                                            } else {
                                                payload_off += n;
                                            }
                                        }
                                        Err(_) => return,
                                    }
                                }
                            }
                        }
                    }
                }

                if !write_chunks.is_empty() {
                    let bytes_written = match write_chunks_batched(&mut stream, &write_chunks).await
                    {
                        Ok(n) => n,
                        Err(_) => return,
                    };
                    bytes_written_counter.fetch_add(bytes_written, Ordering::Relaxed);
                    total_bytes_written += bytes_written;
                    write_chunks.clear();
                }
            }

            bytes_since_flush += total_bytes_written;
            let elapsed = last_flush.elapsed();

            if should_flush(
                bytes_since_flush,
                elapsed,
                FLUSH_THRESHOLD,
                WRITER_MAX_LATENCY,
            ) {
                let _ = stream.flush().await;
                bytes_since_flush = 0;
                last_flush = std::time::Instant::now();
                flush_pending.store(false, Ordering::Release);
            }

            if let (Some(ctx), Some(state), Some(streaming_state)) = (
                read_context.as_ref(),
                read_state.as_mut(),
                streaming_state.as_mut(),
            ) {
                if last_cleanup.elapsed() >= std::time::Duration::from_secs(30) {
                    streaming_state.cleanup_stale();
                    last_cleanup = std::time::Instant::now();
                }

                if did_work {
                    let mut reads = 0usize;
                    while reads < READ_BATCH_LIMIT {
                        let read_start = perf.map(|_| Instant::now());
                        let read_result =
                            match read_message_step_nonblocking(&mut stream, state, ctx).await {
                                Ok(result) => result,
                                Err(e) => {
                                    warn!(
                                        peer = %ctx.peer_addr,
                                        error = %e,
                                        "IO task read error"
                                    );
                                    return;
                                }
                            };
                        if let (Some(perf), Some(start)) = (perf, read_start) {
                            if read_result.progressed || read_result.result.is_some() {
                                perf.read_calls.fetch_add(1, Ordering::Relaxed);
                                perf.read_ns.fetch_add(
                                    start.elapsed().as_nanos() as u64,
                                    Ordering::Relaxed,
                                );
                            }
                        }

                        if let Some(result) = read_result.result {
                            reads += 1;
                            if let Some(registry) = ctx.registry_weak.upgrade() {
                                if let Err(e) = process_read_result_io(
                                    result,
                                    streaming_state,
                                    &registry,
                                    ctx.peer_addr,
                                    ctx.response_correlation.as_ref().map(|c| c.as_ref()),
                                    &mut stream,
                                    &bytes_written_counter,
                                    &mut bytes_since_flush,
                                    &mut response_batch,
                                    perf,
                                )
                                .await
                                {
                                    warn!(
                                        peer = %ctx.peer_addr,
                                        error = %e,
                                        "Failed to process message on IO task"
                                    );
                                }
                            } else {
                                warn!(
                                    peer = %ctx.peer_addr,
                                    "Registry dropped, stopping IO task"
                                );
                                return;
                            }
                        } else if !read_result.progressed {
                            break;
                        }
                    }
                    if !response_batch.is_empty() {
                        if let Err(e) = write_response_batch(
                            &mut stream,
                            &bytes_written_counter,
                            &mut bytes_since_flush,
                            &mut response_batch,
                        )
                        .await
                        {
                            warn!(
                                peer = %ctx.peer_addr,
                                error = %e,
                                "Failed to write response batch"
                            );
                            return;
                        }
                    }
                }
            }

            if !did_work {
                if let (Some(ctx), Some(state), Some(streaming_state)) = (
                    read_context.as_ref(),
                    read_state.as_mut(),
                    streaming_state.as_mut(),
                ) {
                    tokio::select! {
                        // Idle path: block waiting for socket readability.
                        read_result = read_message_step_poll(&mut stream, state, ctx, true) => {
                            let read_start = perf.map(|_| Instant::now());
                            let read_result = match read_result {
                                Ok(result) => result,
                                Err(e) => {
                                    warn!(
                                        peer = %ctx.peer_addr,
                                        error = %e,
                                        "IO task read error"
                                    );
                                    return;
                                }
                            };

                            if let (Some(perf), Some(start)) = (perf, read_start) {
                                if read_result.progressed || read_result.result.is_some() {
                                    perf.read_calls.fetch_add(1, Ordering::Relaxed);
                                    perf.read_ns.fetch_add(
                                        start.elapsed().as_nanos() as u64,
                                        Ordering::Relaxed,
                                    );
                                }
                            }

                            if let Some(result) = read_result.result {
                                if let Some(registry) = ctx.registry_weak.upgrade() {
                                    if let Err(e) = process_read_result_io(
                                        result,
                                        streaming_state,
                                        &registry,
                                        ctx.peer_addr,
                                        ctx.response_correlation.as_ref().map(|c| c.as_ref()),
                                        &mut stream,
                                        &bytes_written_counter,
                                        &mut bytes_since_flush,
                                        &mut response_batch,
                                        perf,
                                    )
                                    .await
                                    {
                                        warn!(
                                            peer = %ctx.peer_addr,
                                            error = %e,
                                            "Failed to process message on IO task"
                                        );
                                    }

                                } else {
                                    warn!(
                                        peer = %ctx.peer_addr,
                                        "Registry dropped, stopping IO task"
                                    );
                                    return;
                                }
                            }

                            // Under load the socket can become readable with many frames queued.
                            // The old "idle path" processed only a single frame per wake-up,
                            // which inflates RTT and caps ActorAsk throughput on server-heavy links.
                            //
                            // Drain additional frames non-blocking to batch handler + response writes.
                            let mut drained = 0usize;
                            while drained < READ_BATCH_LIMIT {
                                let read_start = perf.map(|_| Instant::now());
                                let next = match read_message_step_nonblocking(&mut stream, state, ctx).await {
                                    Ok(r) => r,
                                    Err(e) => {
                                        warn!(
                                            peer = %ctx.peer_addr,
                                            error = %e,
                                            "IO task read error"
                                        );
                                        return;
                                    }
                                };
                                if let (Some(perf), Some(start)) = (perf, read_start) {
                                    if next.progressed || next.result.is_some() {
                                        perf.read_calls.fetch_add(1, Ordering::Relaxed);
                                        perf.read_ns.fetch_add(
                                            start.elapsed().as_nanos() as u64,
                                            Ordering::Relaxed,
                                        );
                                    }
                                }

                                if let Some(result) = next.result {
                                    drained += 1;
                                    if let Some(registry) = ctx.registry_weak.upgrade() {
                                        if let Err(e) = process_read_result_io(
                                            result,
                                            streaming_state,
                                            &registry,
                                            ctx.peer_addr,
                                            ctx.response_correlation.as_ref().map(|c| c.as_ref()),
                                            &mut stream,
                                            &bytes_written_counter,
                                            &mut bytes_since_flush,
                                            &mut response_batch,
                                            perf,
                                        )
                                        .await
                                        {
                                            warn!(
                                                peer = %ctx.peer_addr,
                                                error = %e,
                                                "Failed to process message on IO task"
                                            );
                                        }
                                    } else {
                                        warn!(
                                            peer = %ctx.peer_addr,
                                            "Registry dropped, stopping IO task"
                                        );
                                        return;
                                    }
                                } else if !next.progressed {
                                    break;
                                }
                            }

                            if !response_batch.is_empty() {
                                if let Err(e) = write_response_batch(
                                    &mut stream,
                                    &bytes_written_counter,
                                    &mut bytes_since_flush,
                                    &mut response_batch,
                                )
                                .await
                                {
                                    warn!(
                                        peer = %ctx.peer_addr,
                                        error = %e,
                                        "Failed to write response batch"
                                    );
                                    return;
                                }
                            }

                            // Ensure actor responses don't sit in the kernel/TLS buffers indefinitely
                            // on links that are primarily request->response (server-side).
                            if should_flush(
                                bytes_since_flush,
                                last_flush.elapsed(),
                                FLUSH_THRESHOLD,
                                WRITER_MAX_LATENCY,
                            ) {
                                let _ = stream.flush().await;
                                bytes_since_flush = 0;
                                last_flush = std::time::Instant::now();
                                flush_pending.store(false, Ordering::Release);
                            }
                        }
                        // Wake on new outbound writes even if the socket is currently idle for reads.
                        // Without this, a mostly-write workload (e.g., initial gossip propagation)
                        // can stall until an unrelated read event occurs.
                        _ = write_queue.data_notify.notified() => {
                            pending_cmd = write_queue.pop();
                        }
                        _ = write_queue.space_notify.notified() => {
                            // Producer wakeup only; no action needed.
                        }
                        _ = streaming_queue.data_notify.notified() => {
                            // Wake on streaming commands; drained at the top of the loop.
                        }
                        _ = tokio::time::sleep(WRITER_MAX_LATENCY) => {}
                    }
                } else {
                    tokio::select! {
                        _ = streaming_queue.data_notify.notified() => {
                            // Wake on streaming commands; drained at the top of the loop.
                        }
                        _ = write_queue.data_notify.notified() => {
                            pending_cmd = write_queue.pop();
                        }
                        _ = write_queue.space_notify.notified() => {
                            // Producer wakeup only; no action needed.
                        }
                        _ = tokio::time::sleep(WRITER_MAX_LATENCY) => {}
                    }
                }
            }

            if let Some(perf) = perf {
                if perf_last.elapsed() >= perf_interval {
                    let (read_calls, read_ns, handle_calls, handle_ns, write_calls, write_ns) =
                        perf.snapshot_and_reset();
                    let read_us = read_ns as f64 / 1000.0;
                    let handle_us = handle_ns as f64 / 1000.0;
                    let write_us = write_ns as f64 / 1000.0;
                    info!(
                        read_calls,
                        handle_calls,
                        write_calls,
                        read_us = read_us,
                        handle_us = handle_us,
                        write_us = write_us,
                        read_avg_us = read_us / (read_calls.max(1) as f64),
                        handle_avg_us = handle_us / (handle_calls.max(1) as f64),
                        write_avg_us = write_us / (write_calls.max(1) as f64),
                        "IO PERF"
                    );
                    perf_last = Instant::now();
                }
            }
        }
    }

    async fn acquire_ask_permit(&self) -> OwnedSemaphorePermit {
        // Owned permits allow us to move them across threads safely
        self.ask_permits
            .clone()
            .acquire_owned()
            .await
            .expect("ask permit semaphore closed")
    }

    async fn acquire_ask_permit_fast(&self) -> Result<OwnedSemaphorePermit> {
        match self.ask_permits.clone().try_acquire_owned() {
            Ok(permit) => Ok(permit),
            Err(TryAcquireError::NoPermits) => Ok(self.acquire_ask_permit().await),
            Err(TryAcquireError::Closed) => Err(GossipError::Shutdown),
        }
    }

    async fn enqueue_write(&self, payload: WritePayload) -> Result<()> {
        if self.exit_flag.load(Ordering::Acquire) {
            return Err(GossipError::ConnectionClosed(self.addr));
        }
        if self.shutdown_signal.load(Ordering::Acquire) {
            return Err(GossipError::Shutdown);
        }
        self.sequence_counter.fetch_add(1, Ordering::Relaxed);
        let command = WriteCommand::Payload(payload);
        match self.write_queue.try_push(command) {
            Ok(()) => {
                self.write_queue.data_notify.notify_one();
                Ok(())
            }
            Err(command) => self.write_queue.push(command).await,
        }
    }

    async fn enqueue_write_with_permit(
        &self,
        payload: WritePayload,
        permit: OwnedSemaphorePermit,
    ) -> Result<()> {
        if self.exit_flag.load(Ordering::Acquire) {
            return Err(GossipError::ConnectionClosed(self.addr));
        }
        if self.shutdown_signal.load(Ordering::Acquire) {
            return Err(GossipError::Shutdown);
        }
        self.sequence_counter.fetch_add(1, Ordering::Relaxed);
        let command = WriteCommand::PayloadWithPermit { payload, permit };
        match self.write_queue.try_push(command) {
            Ok(()) => {
                self.write_queue.data_notify.notify_one();
                Ok(())
            }
            Err(command) => self.write_queue.push(command).await,
        }
    }

    fn enqueue_write_nonblocking(&self, payload: WritePayload) -> Result<()> {
        if self.exit_flag.load(Ordering::Acquire) {
            return Err(GossipError::ConnectionClosed(self.addr));
        }
        if self.shutdown_signal.load(Ordering::Acquire) {
            return Err(GossipError::Shutdown);
        }
        self.sequence_counter.fetch_add(1, Ordering::Relaxed);
        match self.write_queue.try_push(WriteCommand::Payload(payload)) {
            Ok(()) => {
                self.write_queue.data_notify.notify_one();
                Ok(())
            }
            Err(_) => Err(GossipError::WriteQueueFull),
        }
    }

    pub async fn write_bytes_ask(&self, data: bytes::Bytes) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(WritePayload::Single(data), permit)
            .await
    }

    pub async fn write_bytes_control(&self, data: bytes::Bytes) -> Result<()> {
        self.enqueue_write(WritePayload::Single(data)).await
    }

    pub async fn write_header_and_payload_control(
        &self,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.enqueue_write(WritePayload::HeaderPayload { header, payload })
            .await
    }

    /// Write header + payload inline without permit allocation for tell messages.
    /// This is a fast path for high-throughput tell operations.
    pub async fn write_header_and_payload_control_inline(
        &self,
        header: [u8; 16],
        header_len: u8,
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.enqueue_write(WritePayload::HeaderInline {
            header,
            header_len,
            payload,
        })
        .await
    }

    pub async fn write_header_and_payload_control_inline_aligned(
        &self,
        header: [u8; 16],
        header_len: u8,
        payload: crate::AlignedBytes,
    ) -> Result<()> {
        self.enqueue_write(WritePayload::HeaderInlineAligned {
            header,
            header_len,
            payload,
        })
        .await
    }

    pub async fn write_header_and_payload_control_inline32(
        &self,
        header: [u8; 32],
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.enqueue_write(WritePayload::HeaderInline32 { header, payload })
            .await
    }

    /// Non-blocking variant of `write_header_and_payload_control_inline32`.
    ///
    /// This avoids awaiting on the write queue and is useful for building sync `tell` APIs
    /// on top of the background writer task.
    pub fn write_header_and_payload_control_inline32_nonblocking(
        &self,
        header: [u8; 32],
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.enqueue_write_nonblocking(WritePayload::HeaderInline32 { header, payload })
    }

    pub async fn write_header_and_payload_ask(
        &self,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(WritePayload::HeaderPayload { header, payload }, permit)
            .await
    }

    /// Write header + payload inline for ask messages.
    pub async fn write_header_and_payload_ask_inline(
        &self,
        header: [u8; 16],
        header_len: u8,
        payload: bytes::Bytes,
    ) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(
            WritePayload::HeaderInline {
                header,
                header_len,
                payload,
            },
            permit,
        )
        .await
    }

    pub async fn write_header_and_payload_ask_inline32(
        &self,
        header: [u8; 32],
        payload: bytes::Bytes,
    ) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(WritePayload::HeaderInline32 { header, payload }, permit)
            .await
    }

    /// Write DirectAsk header + payload inline (fast path for direct ask)
    /// Wire format: [length:4][type:1][correlation_id:2][payload_len:4][payload:N]
    pub async fn write_direct_ask_inline(
        &self,
        header: [u8; 16], // DIRECT_ASK_FRAME_HEADER_LEN
        payload: bytes::Bytes,
    ) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(WritePayload::DirectAskInline { header, payload }, permit)
            .await
    }

    /// Write DirectResponse inline (same format as DirectAsk)
    /// Wire format: [length:4][type:1][correlation_id:2][payload_len:4][payload:N]
    pub async fn write_direct_response_inline(
        &self,
        header: [u8; 16], // DIRECT_RESPONSE_FRAME_HEADER_LEN
        payload: bytes::Bytes,
    ) -> Result<()> {
        // DirectResponse has same wire format as DirectAsk, so reuse the implementation
        self.write_direct_ask_inline(header, payload).await
    }

    pub async fn write_pooled_control(
        &self,
        header: bytes::Bytes,
        prefix: Option<bytes::Bytes>,
        payload: crate::typed::PooledPayload,
    ) -> Result<()> {
        self.enqueue_write(WritePayload::HeaderPooled {
            header,
            prefix,
            payload,
        })
        .await
    }

    pub async fn write_pooled_control_inline(
        &self,
        header: [u8; 16],
        header_len: u8,
        prefix: Option<[u8; 16]>,
        prefix_len: u8,
        payload: crate::typed::PooledPayload,
    ) -> Result<()> {
        self.enqueue_write(WritePayload::HeaderInlinePooled {
            header,
            header_len,
            prefix,
            prefix_len,
            payload,
        })
        .await
    }

    pub async fn write_pooled_ask(
        &self,
        header: bytes::Bytes,
        prefix: Option<bytes::Bytes>,
        payload: crate::typed::PooledPayload,
    ) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(
            WritePayload::HeaderPooled {
                header,
                prefix,
                payload,
            },
            permit,
        )
        .await
    }

    pub async fn write_pooled_ask_inline(
        &self,
        header: [u8; 16],
        header_len: u8,
        prefix: Option<[u8; 16]>,
        prefix_len: u8,
        payload: crate::typed::PooledPayload,
    ) -> Result<()> {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(
            WritePayload::HeaderInlinePooled {
                header,
                header_len,
                prefix,
                prefix_len,
                payload,
            },
            permit,
        )
        .await
    }

    pub async fn write_buf_control<B>(&self, buf: B) -> Result<()>
    where
        B: Buf + Send + 'static,
    {
        self.enqueue_write(WritePayload::Buf(Box::new(buf))).await
    }

    pub async fn write_buf_ask<B>(&self, buf: B) -> Result<()>
    where
        B: Buf + Send + 'static,
    {
        let permit = self.acquire_ask_permit_fast().await?;
        self.enqueue_write_with_permit(WritePayload::Buf(Box::new(buf)), permit)
            .await
    }

    /// Enqueue bytes for the background writer (non-blocking).
    pub fn write_bytes_nonblocking(&self, data: bytes::Bytes) -> Result<()> {
        self.enqueue_write_nonblocking(WritePayload::Single(data))
    }

    /// Enqueue header + payload without concatenating.
    pub fn write_header_and_payload_nonblocking(
        &self,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.enqueue_write_nonblocking(WritePayload::HeaderPayload { header, payload })
    }

    /// Enqueue bytes (legacy name kept for compatibility).
    pub fn write_bytes_nonblocking_checked(&self, data: bytes::Bytes) -> Result<()> {
        self.enqueue_write_nonblocking(WritePayload::Single(data))
    }

    /// Enqueue header + payload (legacy name kept for compatibility).
    pub fn write_header_and_payload_nonblocking_checked(
        &self,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.enqueue_write_nonblocking(WritePayload::HeaderPayload { header, payload })
    }

    /// Flush the writer immediately - used for low-latency ask operations
    pub fn flush_immediately(&self) -> Result<()> {
        // Coalesce flush requests to avoid flooding the writer task.
        if self
            .flush_pending
            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)
            .is_ok()
        {
            match self.streaming_queue.try_push(StreamingCommand::Flush) {
                Ok(()) => Ok(()),
                Err(_) => {
                    // Queue is full (backpressure). Drop the explicit flush request and allow
                    // the writer's periodic/threshold flush to handle it.
                    self.flush_pending.store(false, Ordering::Release);
                    Ok(())
                }
            }
        } else {
            Ok(())
        }
    }

    /// Write data with vectored batching - still no blocking
    pub fn write_vectored_nonblocking(&self, data_chunks: &[&[u8]]) -> Result<()> {
        if data_chunks.is_empty() {
            return Ok(());
        }

        // Use BytesMut for efficient concatenation
        let total_len: usize = data_chunks.iter().map(|chunk| chunk.len()).sum();
        let mut combined_buffer = bytes::BytesMut::with_capacity(total_len);

        for chunk in data_chunks {
            combined_buffer.extend_from_slice(chunk); // ALLOW_COPY
        }

        self.write_bytes_nonblocking(combined_buffer.freeze())
    }

    /// Write large data in chunks to avoid blocking
    pub fn write_chunked_nonblocking(&self, data: &[u8], chunk_size: usize) -> Result<()> {
        if data.is_empty() {
            return Ok(());
        }

        for chunk in data.chunks(chunk_size) {
            let _ = self.write_bytes_nonblocking(
                bytes::Bytes::copy_from_slice(chunk), /* ALLOW_COPY */
            );
        }

        Ok(())
    }

    /// Get channel ID
    pub fn channel_id(&self) -> ChannelId {
        self.channel_id
    }

    /// Get total bytes written
    pub fn bytes_written(&self) -> usize {
        self.bytes_written.load(Ordering::Relaxed)
    }

    /// True when a streaming send/response is in progress and the IO task will avoid
    /// draining the normal payload queue (streaming takes exclusive priority).
    pub fn is_streaming_active(&self) -> bool {
        self.streaming_active.load(Ordering::Acquire)
    }

    /// Get sequence counter
    pub fn sequence_number(&self) -> usize {
        self.sequence_counter.load(Ordering::Relaxed)
    }

    /// Get next stream frame sequence ID (wraps at u16::MAX)
    fn next_frame_sequence_id(&self) -> u16 {
        (self.frame_sequence.fetch_add(1, Ordering::Relaxed) & 0xFFFF) as u16
    }

    /// Get socket address
    pub fn addr(&self) -> SocketAddr {
        self.addr
    }

    /// Shutdown the background writer task
    pub fn shutdown(&self) {
        self.shutdown_signal.store(true, Ordering::Relaxed);
    }

    /// Wait until the IO task exits.
    pub async fn wait_for_exit(&self) {
        while !self.exit_flag.load(Ordering::Acquire) {
            self.exit_notify.notified().await;
        }
    }

    /// Get the streaming threshold for this connection
    ///
    /// Returns the threshold above which messages should be streamed rather than
    /// queued through the background writer. This is always derived from the buffer size.
    pub fn streaming_threshold(&self) -> usize {
        self.buffer_config.streaming_threshold()
    }

    pub fn schema_hash(&self) -> Option<u64> {
        self.schema_hash
    }

    fn max_stream_chunk_size(&self) -> Result<usize> {
        // Streaming frame wire format excludes the 4-byte length prefix from `msg_len`.
        // `msg_len` for stream data frames is: type(1) + corr(2) + reserved(9) + header(36) + chunk(N).
        const STREAM_FRAME_OVERHEAD: usize = 12 + crate::StreamHeader::SERIALIZED_SIZE;

        let max_chunk = self.max_message_size.saturating_sub(STREAM_FRAME_OVERHEAD);
        if max_chunk == 0 {
            return Err(GossipError::InvalidConfig(format!(
                "max_message_size={} too small for streaming (overhead={})",
                self.max_message_size, STREAM_FRAME_OVERHEAD
            )));
        }
        Ok(std::cmp::min(STREAM_CHUNK_SIZE, max_chunk))
    }

    /// Stream a large message directly to the socket, bypassing the write queue
    /// This provides maximum performance for large messages like PreBacktest
    pub async fn stream_large_message(
        &self,
        msg: &[u8],
        type_hash: u32,
        actor_id: u64,
    ) -> Result<()> {
        use crate::{MessageType, StreamHeader, current_timestamp_nanos};

        let chunk_size = self.max_stream_chunk_size()?;

        // Acquire streaming mode atomically
        while self
            .streaming_active
            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)
            .is_err()
        {
            tokio::task::yield_now().await;
        }

        // Ensure we release streaming mode on exit
        let _guard = StreamingGuard {
            flag: self.streaming_active.clone(),
        };

        // Generate unique stream ID using nanoseconds to avoid collisions
        let stream_id = current_timestamp_nanos();

        let schema_hash = self.schema_hash();

        // Helper to serialize message with type and header
        fn serialize_stream_message(
            msg_type: MessageType,
            header: &StreamHeader,
            schema_hash: Option<u64>,
        ) -> Vec<u8> {
            // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36]
            let inner_size = 12 + StreamHeader::SERIALIZED_SIZE; // type(1) + corr(2) + reserved(9) + header
            let mut message = Vec::with_capacity(4 + inner_size);

            // Length prefix (required by protocol)
            message.extend_from_slice(&(inner_size as u32).to_be_bytes()); // ALLOW_COPY

            // Header
            message.push(msg_type as u8);
            message.extend_from_slice(&[0, 0]); // ALLOW_COPY correlation_id (not used for streaming)
            let mut reserved = [0u8; 9];
            crate::framing::write_schema_hash(&mut reserved, schema_hash);
            message.extend_from_slice(&reserved); // ALLOW_COPY 9 reserved bytes for 32-byte alignment
            message.extend_from_slice(&header.to_bytes()); // ALLOW_COPY
            message
        }

        // Send StreamStart header
        let start_header = StreamHeader {
            stream_id,
            total_size: msg.len() as u64,
            chunk_size: 0,
            chunk_index: 0,
            type_hash,
            actor_id,
        };

        let start_msg =
            serialize_stream_message(MessageType::StreamStart, &start_header, schema_hash);
        self.streaming_queue
            .push(StreamingCommand::WriteBytes(start_msg.into()))
            .await?;

        // Stream chunks directly
        for (idx, chunk) in msg.chunks(chunk_size).enumerate() {
            let data_header = StreamHeader {
                stream_id,
                total_size: msg.len() as u64,
                chunk_size: chunk.len() as u32,
                chunk_index: idx as u32,
                type_hash,
                actor_id,
            };

            // Create combined message with proper length prefix
            // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36][chunk_data:N]
            let inner_size = 12 + StreamHeader::SERIALIZED_SIZE + chunk.len(); // type(1) + corr(2) + reserved(9) + header + data
            let mut chunk_msg = Vec::with_capacity(4 + inner_size);

            // Length prefix (includes header + chunk data)
            chunk_msg.extend_from_slice(&(inner_size as u32).to_be_bytes()); // ALLOW_COPY

            // Header
            chunk_msg.push(MessageType::StreamData as u8);
            chunk_msg.extend_from_slice(&[0, 0]); // ALLOW_COPY correlation_id
            let mut reserved = [0u8; 9];
            crate::framing::write_schema_hash(&mut reserved, schema_hash);
            chunk_msg.extend_from_slice(&reserved); // ALLOW_COPY 9 reserved bytes for 32-byte alignment
            chunk_msg.extend_from_slice(&data_header.to_bytes()); // ALLOW_COPY

            // Chunk data
            chunk_msg.extend_from_slice(chunk); // ALLOW_COPY

            self.streaming_queue
                .push(StreamingCommand::WriteBytes(chunk_msg.into()))
                .await?;

            // Yield periodically to prevent blocking
            if idx % 10 == 0 {
                self.streaming_queue.push(StreamingCommand::Flush).await?;
                tokio::task::yield_now().await;
            }
        }

        // Send StreamEnd
        let end_msg = serialize_stream_message(MessageType::StreamEnd, &start_header, schema_hash);
        self.streaming_queue
            .push(StreamingCommand::WriteBytes(end_msg.into()))
            .await?;
        self.streaming_queue.push(StreamingCommand::Flush).await?;

        debug!(
            " STREAMING: Successfully streamed {} MB in {} chunks",
            msg.len() as f64 / 1_048_576.0,
            msg.len().div_ceil(chunk_size)
        );

        Ok(())
    }

    /// Stream a response back to the caller, using streaming protocol for large payloads.
    /// This is used when a response exceeds the streaming threshold.
    ///
    /// NOTE: This method copies the payload. For zero-copy streaming responses,
    /// use `stream_response_bytes` with pre-owned `Bytes` instead.
    ///
    /// # Arguments
    /// * `payload` - The response payload bytes
    /// * `correlation_id` - The correlation ID from the original request (for response matching)
    pub async fn stream_response(&self, payload: &[u8], correlation_id: u16) -> Result<()> {
        // Convert to Bytes and use zero-copy implementation
        self.stream_response_bytes(bytes::Bytes::copy_from_slice(payload), correlation_id) // ALLOW_COPY
            .await
    }

    /// Zero-copy stream a response back to the caller.
    /// Uses vectored writes to avoid copying the payload data.
    ///
    /// # Arguments
    /// * `payload` - The response payload as owned Bytes
    /// * `correlation_id` - The correlation ID from the original request
    pub async fn stream_response_bytes(
        &self,
        payload: bytes::Bytes,
        correlation_id: u16,
    ) -> Result<()> {
        use crate::{MessageType, StreamHeader, current_timestamp_nanos};
        use bytes::BufMut;

        let chunk_size = self.max_stream_chunk_size()?;

        // Acquire streaming mode atomically
        while self
            .streaming_active
            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)
            .is_err()
        {
            tokio::task::yield_now().await;
        }

        // Ensure we release streaming mode on exit
        let _guard = StreamingGuard {
            flag: self.streaming_active.clone(),
        };

        // Generate unique stream ID for this response stream
        let stream_id = current_timestamp_nanos();

        let schema_hash = self.schema_hash();

        // Helper to build stream response header bytes (zero-copy friendly)
        fn build_stream_response_header(
            msg_type: MessageType,
            header: &StreamHeader,
            correlation_id: u16,
            chunk_len: usize,
            schema_hash: Option<u64>,
        ) -> bytes::Bytes {
            // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36]
            let inner_size = 12 + StreamHeader::SERIALIZED_SIZE + chunk_len;
            let mut message =
                bytes::BytesMut::with_capacity(4 + 12 + StreamHeader::SERIALIZED_SIZE);

            // Length prefix
            message.put_u32(inner_size as u32);

            // Header with correlation ID for response matching
            message.put_u8(msg_type as u8);
            message.put_u16(correlation_id);
            let mut reserved = [0u8; 9];
            crate::framing::write_schema_hash(&mut reserved, schema_hash);
            message.put_slice(&reserved); // 9 reserved bytes
            message.put_slice(&header.to_bytes());
            message.freeze()
        }

        // Use StreamResponseStart to indicate this is a streaming response
        let start_header = StreamHeader {
            stream_id,
            total_size: payload.len() as u64,
            chunk_size: 0,
            chunk_index: 0,
            type_hash: 0, // Not needed for responses
            actor_id: 0,  // Actor ID doesn't matter - message type distinguishes responses
        };

        // Send StreamResponseStart
        let start_msg = build_stream_response_header(
            MessageType::StreamResponseStart,
            &start_header,
            correlation_id,
            0,
            schema_hash,
        );
        self.streaming_queue
            .push(StreamingCommand::WriteBytes(start_msg))
            .await?;

        // Stream response chunks using zero-copy slices
        let total_len = payload.len();
        let num_chunks = total_len.div_ceil(chunk_size);

        for idx in 0..num_chunks {
            let start = idx * chunk_size;
            let end = std::cmp::min(start + chunk_size, total_len);
            let chunk_len = end - start;

            // Zero-copy slice of the payload
            let chunk_data = payload.slice(start..end);

            let data_header = StreamHeader {
                stream_id,
                total_size: total_len as u64,
                chunk_size: chunk_len as u32,
                chunk_index: idx as u32,
                type_hash: 0,
                actor_id: 0,
            };

            // Build header bytes (small, okay to allocate)
            let header_bytes = build_stream_response_header(
                MessageType::StreamResponseData,
                &data_header,
                correlation_id,
                chunk_len,
                schema_hash,
            );

            // Use vectored write: header + chunk_data (zero-copy)
            self.streaming_queue
                .push(StreamingCommand::VectoredWrite(VectoredSendItem {
                    header: header_bytes,
                    payload: chunk_data,
                }))
                .await?;

            // Yield periodically to prevent blocking
            if idx % 10 == 0 {
                self.streaming_queue.push(StreamingCommand::Flush).await?;
                tokio::task::yield_now().await;
            }
        }

        // Send StreamResponseEnd
        let end_msg = build_stream_response_header(
            MessageType::StreamResponseEnd,
            &start_header,
            correlation_id,
            0,
            schema_hash,
        );
        self.streaming_queue
            .push(StreamingCommand::WriteBytes(end_msg))
            .await?;
        self.streaming_queue.push(StreamingCommand::Flush).await?;

        debug!(
            " STREAMING RESPONSE: Successfully streamed {} bytes in {} chunks (correlation_id: {})",
            total_len, num_chunks, correlation_id
        );

        Ok(())
    }

    /// Send a response using the inline write queue (never streaming).
    ///
    /// # Arguments
    /// * `payload` - The response payload bytes
    /// * `correlation_id` - The correlation ID from the original request
    ///
    /// # Returns
    /// Ok(()) on success, or an error if sending failed
    pub async fn send_response_auto(&self, payload: &[u8], correlation_id: u16) -> Result<()> {
        let header = framing::write_ask_response_header(
            crate::MessageType::Response,
            correlation_id,
            payload.len(),
        );
        self.write_header_and_payload_control_inline(
            header,
            16,
            bytes::Bytes::copy_from_slice(payload), /* ALLOW_COPY */
        )
        .await
    }

    /// Send a response with owned Bytes using the inline write queue (never streaming).
    ///
    /// # Arguments
    /// * `correlation_id` - The correlation ID from the original request
    /// * `payload` - The response payload as owned Bytes
    ///
    /// # Returns
    /// Ok(()) on success, or an error if sending failed
    pub async fn send_response_auto_bytes(
        &self,
        correlation_id: u16,
        payload: bytes::Bytes,
    ) -> Result<()> {
        let header = framing::write_ask_response_header(
            crate::MessageType::Response,
            correlation_id,
            payload.len(),
        );
        self.write_header_and_payload_control_inline(header, 16, payload)
            .await
    }

    /// Zero-copy vectored write for header + payload in single operation
    /// This eliminates copying payload data into frame buffer - optimal for streaming
    pub async fn write_bytes_vectored(
        &self,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        // Create vectored command that preserves both header and payload as separate Bytes
        let command = VectoredSendItem { header, payload };

        // Prefer the streaming queue for vectored operations; if it's full, fall back
        // to the normal payload queue (still zero-copy: header + payload remain `Bytes`).
        match self
            .streaming_queue
            .try_push(StreamingCommand::VectoredWrite(command))
        {
            Ok(()) => Ok(()),
            Err(StreamingCommand::VectoredWrite(vectored_cmd)) => {
                self.enqueue_write(WritePayload::HeaderPayload {
                    header: vectored_cmd.header,
                    payload: vectored_cmd.payload,
                })
                .await
            }
            Err(_) => Err(GossipError::Shutdown),
        }
    }

    /// Send owned chunks without copying - optimal for streaming large messages
    pub fn write_owned_chunks(&self, chunks: Vec<bytes::Bytes>) -> Result<()> {
        if chunks.is_empty() {
            return Ok(());
        }

        // Send chunks as a batch via the bounded streaming queue for optimal vectored I/O.
        let command = StreamingCommand::OwnedChunks(chunks);
        self.streaming_queue
            .try_push(command)
            .map_err(|_| GossipError::WriteQueueFull)?;

        Ok(())
    }
}

/// Guard to ensure streaming_active is released on drop
struct StreamingGuard {
    flag: Arc<AtomicBool>,
}

impl Drop for StreamingGuard {
    fn drop(&mut self) {
        self.flag.store(false, Ordering::Release);
    }
}

impl Debug for LockFreeStreamHandle {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("LockFreeStreamHandle")
            .field("addr", &self.addr)
            .field("channel_id", &self.channel_id)
            .field("bytes_written", &self.bytes_written.load(Ordering::Relaxed))
            .field("sequence", &self.sequence_counter.load(Ordering::Relaxed))
            .finish()
    }
}

#[derive(Debug, PartialEq, Eq)]
#[cfg(any(test, feature = "test-helpers", debug_assertions))]
#[allow(dead_code)]
enum DirectPayloadError {
    HeaderTooShort,
    PayloadTruncated { expected: usize, available: usize },
}

#[cfg(any(test, feature = "test-helpers", debug_assertions))]
#[allow(dead_code)]
fn parse_direct_message_payload<'a>(
    msg_data: &'a [u8],
) -> std::result::Result<&'a [u8], DirectPayloadError> {
    if msg_data.len() < crate::framing::DIRECT_ASK_HEADER_LEN {
        return Err(DirectPayloadError::HeaderTooShort);
    }

    let payload_len =
        u32::from_be_bytes([msg_data[3], msg_data[4], msg_data[5], msg_data[6]]) as usize;
    let payload_start = crate::framing::DIRECT_ASK_HEADER_LEN;
    let payload_end = payload_start + payload_len;

    if msg_data.len() < payload_end {
        return Err(DirectPayloadError::PayloadTruncated {
            expected: payload_len,
            available: msg_data.len().saturating_sub(payload_start),
        });
    }

    Ok(&msg_data[payload_start..payload_end])
}

/// Connection pool for maintaining persistent TCP connections to peers
/// All connections are persistent - there is no checkout/checkin
/// Lock-free connection pool using atomic operations and lock-free data structures
pub struct ConnectionPool {
    /// PRIMARY: Mapping Peer ID -> LockFreeConnection
    /// This is the main storage - we identify connections by peer ID, not address
    pub connections_by_peer: SccHashMap<crate::PeerId, Arc<LockFreeConnection>>,
    /// SECONDARY: Mapping SocketAddr -> Peer ID (for incoming connection identification)
    pub addr_to_peer_id: SccHashMap<SocketAddr, crate::PeerId>,
    /// Configuration: Peer ID -> Expected SocketAddr (where to connect)
    pub peer_id_to_addr: SccHashMap<crate::PeerId, SocketAddr>,
    /// Address-based connection index for fast lookup by SocketAddr
    pub connections_by_addr: SccHashMap<SocketAddr, Arc<LockFreeConnection>>,
    /// Shared correlation trackers by peer ID - ensures ask/response works across bidirectional connections
    correlation_trackers: SccHashMap<crate::PeerId, Arc<CorrelationTracker>>,
    max_connections: usize,
    connection_timeout: Duration,
    /// Registry reference for handling incoming messages
    registry: ArcSwapWeak<GossipRegistry>,
    /// Shared message buffer pool for zero-allocation processing
    message_buffer_pool: Arc<MessageBufferPool>,
    /// Shared aligned bytes pool for zero-copy receive buffers
    aligned_bytes_pool: Arc<crate::AlignedBytesPool>,
    /// Connection counter for load balancing
    connection_counter: AtomicUsize,
}

/// Maximum number of pending responses (must be power of 2 for fast modulo)
const PENDING_RESPONSES_SIZE: usize = 8192;
const SLOT_EMPTY: u8 = 0;
const SLOT_WAITING: u8 = 1;
const SLOT_WRITING: u8 = 2;
const SLOT_READY: u8 = 3;

/// Pending response slot
struct PendingResponseSlot {
    state: AtomicU8,
    response: UnsafeCell<MaybeUninit<crate::AlignedBytes>>,
    waker: AtomicWaker,
}

// Safety: access is synchronized via atomics and the correlation protocol.
unsafe impl Send for PendingResponseSlot {}
unsafe impl Sync for PendingResponseSlot {}

/// Shared state for correlation tracking
pub(crate) struct CorrelationTracker {
    /// Next correlation ID to use
    next_id: AtomicU16,
    /// Fixed-size pending responses (boxed to avoid large stack allocations)
    pending: Box<[PendingResponseSlot]>,
}

impl std::fmt::Debug for CorrelationTracker {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("CorrelationTracker")
            .field("next_id", &self.next_id.load(Ordering::Relaxed))
            .finish()
    }
}

impl CorrelationTracker {
    fn new() -> Arc<Self> {
        let mut pending = Vec::with_capacity(PENDING_RESPONSES_SIZE);
        pending.resize_with(PENDING_RESPONSES_SIZE, || PendingResponseSlot {
            state: AtomicU8::new(SLOT_EMPTY),
            response: UnsafeCell::new(MaybeUninit::uninit()),
            waker: AtomicWaker::new(),
        });
        Arc::new(Self {
            next_id: AtomicU16::new(1),
            pending: pending.into_boxed_slice(),
        })
    }

    /// Allocate a correlation ID and reserve the response slot.
    fn allocate(&self) -> u16 {
        loop {
            let id = self.next_id.fetch_add(1, Ordering::Relaxed);
            if id == 0 {
                continue; // Skip 0 as it's reserved
            }

            let slot = (id as usize) % PENDING_RESPONSES_SIZE;
            let slot_ref = &self.pending[slot];
            if slot_ref
                .state
                .compare_exchange(
                    SLOT_EMPTY,
                    SLOT_WAITING,
                    Ordering::AcqRel,
                    Ordering::Acquire,
                )
                .is_ok()
            {
                trace!(
                    "CorrelationTracker: Allocated correlation_id {} in slot {}",
                    id, slot
                );
                return id;
            }

            // Slot is occupied, try next ID (trace level - fires frequently under load)
            trace!("CorrelationTracker: Slot {} occupied, trying next ID", slot);
        }
    }

    /// Check if a correlation ID has a pending request
    pub(crate) fn has_pending(&self, correlation_id: u16) -> bool {
        let slot = (correlation_id as usize) % PENDING_RESPONSES_SIZE;
        matches!(
            self.pending[slot].state.load(Ordering::Acquire),
            SLOT_WAITING | SLOT_WRITING
        )
    }

    /// Complete a pending request with a response
    pub(crate) fn complete(&self, correlation_id: u16, response: crate::AlignedBytes) {
        let slot = (correlation_id as usize) % PENDING_RESPONSES_SIZE;
        let slot_ref = &self.pending[slot];
        if slot_ref
            .state
            .compare_exchange(
                SLOT_WAITING,
                SLOT_WRITING,
                Ordering::AcqRel,
                Ordering::Acquire,
            )
            .is_err()
        {
            return;
        }

        // Store response, then publish READY
        unsafe {
            (*slot_ref.response.get()).write(response);
        }
        slot_ref.state.store(SLOT_READY, Ordering::Release);
        slot_ref.waker.wake();
    }

    /// Cancel a pending request (used when send fails).
    pub(crate) fn cancel(&self, correlation_id: u16) {
        let slot = (correlation_id as usize) % PENDING_RESPONSES_SIZE;
        let slot_ref = &self.pending[slot];
        loop {
            let state = slot_ref.state.load(Ordering::Acquire);
            match state {
                SLOT_WAITING => {
                    if slot_ref
                        .state
                        .compare_exchange(
                            SLOT_WAITING,
                            SLOT_EMPTY,
                            Ordering::AcqRel,
                            Ordering::Acquire,
                        )
                        .is_ok()
                    {
                        break;
                    }
                }
                SLOT_READY => {
                    if slot_ref
                        .state
                        .compare_exchange(
                            SLOT_READY,
                            SLOT_EMPTY,
                            Ordering::AcqRel,
                            Ordering::Acquire,
                        )
                        .is_ok()
                    {
                        unsafe {
                            (*slot_ref.response.get()).assume_init_drop();
                        }
                        break;
                    }
                }
                SLOT_WRITING => {
                    std::hint::spin_loop();
                }
                _ => break,
            }
        }
        slot_ref.waker.wake();
    }

    /// Cancel all pending requests (used when a connection drops).
    pub(crate) fn cancel_all(&self) {
        for slot_ref in self.pending.iter() {
            loop {
                let state = slot_ref.state.load(Ordering::Acquire);
                match state {
                    SLOT_WAITING => {
                        if slot_ref
                            .state
                            .compare_exchange(
                                SLOT_WAITING,
                                SLOT_EMPTY,
                                Ordering::AcqRel,
                                Ordering::Acquire,
                            )
                            .is_ok()
                        {
                            slot_ref.waker.wake();
                            break;
                        }
                    }
                    SLOT_READY => {
                        if slot_ref
                            .state
                            .compare_exchange(
                                SLOT_READY,
                                SLOT_EMPTY,
                                Ordering::AcqRel,
                                Ordering::Acquire,
                            )
                            .is_ok()
                        {
                            unsafe {
                                (*slot_ref.response.get()).assume_init_drop();
                            }
                            slot_ref.waker.wake();
                            break;
                        }
                    }
                    SLOT_WRITING => {
                        std::hint::spin_loop();
                    }
                    _ => break,
                }
            }
        }
    }

    async fn wait_for_response(
        &self,
        correlation_id: u16,
        timeout: Duration,
    ) -> Result<crate::AlignedBytes> {
        if timeout.is_zero() {
            return self.wait_for_response_no_timeout(correlation_id).await;
        }
        let slot = (correlation_id as usize) % PENDING_RESPONSES_SIZE;
        let slot_ref = &self.pending[slot];

        let wait_fut = futures::future::poll_fn(|cx| {
            // If the slot was cancelled (e.g. connection dropped and cancel_all() ran),
            // return a concrete error instead of waiting forever.
            let state = slot_ref.state.load(Ordering::Acquire);
            if state == SLOT_READY {
                slot_ref.state.store(SLOT_EMPTY, Ordering::Release);
                let response = unsafe { (*slot_ref.response.get()).assume_init_read() };
                return std::task::Poll::Ready(Ok(response));
            }
            if state == SLOT_EMPTY {
                return std::task::Poll::Ready(Err(crate::GossipError::Timeout));
            }

            slot_ref.waker.register(cx.waker());

            let state = slot_ref.state.load(Ordering::Acquire);
            if state == SLOT_READY {
                slot_ref.state.store(SLOT_EMPTY, Ordering::Release);
                let response = unsafe { (*slot_ref.response.get()).assume_init_read() };
                return std::task::Poll::Ready(Ok(response));
            }
            if state == SLOT_EMPTY {
                return std::task::Poll::Ready(Err(crate::GossipError::Timeout));
            }

            std::task::Poll::Pending
        });

        match tokio::time::timeout(timeout, wait_fut).await {
            Ok(result) => result,
            Err(_) => {
                if slot_ref
                    .state
                    .compare_exchange(
                        SLOT_WAITING,
                        SLOT_EMPTY,
                        Ordering::AcqRel,
                        Ordering::Acquire,
                    )
                    .is_ok()
                {
                    return Err(crate::GossipError::Timeout);
                }
                // The slot isn't WAITING anymore. It might be READY/WRITING/EMPTY (cancelled).
                // READY: return the response.
                // WRITING: wait without timeout to avoid dropping an in-progress write.
                // EMPTY: treat as timeout/cancel to avoid hanging forever.
                match slot_ref.state.load(Ordering::Acquire) {
                    SLOT_READY => self.wait_for_response_no_timeout(correlation_id).await,
                    SLOT_WRITING => self.wait_for_response_no_timeout(correlation_id).await,
                    SLOT_EMPTY => Err(crate::GossipError::Timeout),
                    _ => Err(crate::GossipError::Timeout),
                }
            }
        }
    }

    async fn wait_for_response_no_timeout(
        &self,
        correlation_id: u16,
    ) -> Result<crate::AlignedBytes> {
        let slot = (correlation_id as usize) % PENDING_RESPONSES_SIZE;
        let slot_ref = &self.pending[slot];

        futures::future::poll_fn(|cx| {
            let state = slot_ref.state.load(Ordering::Acquire);
            if state == SLOT_READY {
                slot_ref.state.store(SLOT_EMPTY, Ordering::Release);
                let response = unsafe { (*slot_ref.response.get()).assume_init_read() };
                return std::task::Poll::Ready(Ok(response));
            }
            if state == SLOT_EMPTY {
                return std::task::Poll::Ready(Err(crate::GossipError::Timeout));
            }
            slot_ref.waker.register(cx.waker());
            let state = slot_ref.state.load(Ordering::Acquire);
            if state == SLOT_READY {
                slot_ref.state.store(SLOT_EMPTY, Ordering::Release);
                let response = unsafe { (*slot_ref.response.get()).assume_init_read() };
                return std::task::Poll::Ready(Ok(response));
            }
            if state == SLOT_EMPTY {
                return std::task::Poll::Ready(Err(crate::GossipError::Timeout));
            }
            std::task::Poll::Pending
        })
        .await
    }
}

/// Handle to send messages through a persistent connection - LOCK-FREE
#[derive(Clone)]
pub struct ConnectionHandle {
    pub addr: SocketAddr,
    // Direct lock-free stream handle - NO MUTEX!
    stream_handle: Arc<LockFreeStreamHandle>,
    // Correlation tracker for ask/response
    correlation: Arc<CorrelationTracker>,
}

impl std::fmt::Debug for ConnectionHandle {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("ConnectionHandle")
            .field("addr", &self.addr)
            .field("stream_handle", &self.stream_handle)
            .finish()
    }
}

/// Deferred ask handle backed by the per-connection correlation tracker.
///
/// This is the correct way to "delegate" awaiting a response:
/// - the request is sent immediately,
/// - the returned handle can be moved to another task and awaited later,
/// - dropping the handle cancels the pending slot to keep resources bounded.
pub struct PendingAsk {
    correlation_id: u16,
    correlation: Arc<CorrelationTracker>,
    timeout: Duration,
}

impl PendingAsk {
    pub fn correlation_id(&self) -> u16 {
        self.correlation_id
    }

    pub async fn wait(self) -> Result<bytes::Bytes> {
        let response = self
            .correlation
            .wait_for_response(self.correlation_id, self.timeout)
            .await?;
        Ok(response.into_bytes())
    }
}

impl Drop for PendingAsk {
    fn drop(&mut self) {
        self.correlation.cancel(self.correlation_id);
    }
}

impl std::fmt::Debug for PendingAsk {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("PendingAsk")
            .field("correlation_id", &self.correlation_id)
            .field("timeout", &self.timeout)
            .finish()
    }
}

impl ConnectionHandle {
    /// Returns true if the underlying IO task has exited and the connection is closed.
    pub fn is_closed(&self) -> bool {
        self.stream_handle.exit_flag.load(Ordering::Acquire)
    }

    /// Observability hook for tests/diagnostics: total bytes written to the socket by the IO task.
    pub fn bytes_written(&self) -> usize {
        self.stream_handle.bytes_written()
    }

    /// Observability hook for tests/diagnostics: is the connection currently in streaming mode.
    pub fn is_streaming_active(&self) -> bool {
        self.stream_handle.is_streaming_active()
    }

    /// Observability hook for tests/diagnostics: number of queued write operations attempted.
    pub fn sequence_number(&self) -> usize {
        self.stream_handle.sequence_number()
    }

    /// Send pre-serialized data through this connection - LOCK-FREE
    pub async fn send_data(&self, data: Vec<u8>) -> Result<()> {
        self.stream_handle
            .write_bytes_control(bytes::Bytes::from(data))
            .await
    }

    /// Send raw bytes without any framing - used by ReplyTo
    pub async fn send_raw_bytes(&self, data: &[u8]) -> Result<()> {
        // Must copy here since we don't own the data
        self.stream_handle
            .write_bytes_control(bytes::Bytes::copy_from_slice(data)) // ALLOW_COPY
            .await
    }

    /// Send a response payload with framing, without copying the payload.
    pub async fn send_response_bytes(
        &self,
        correlation_id: u16,
        payload: bytes::Bytes,
    ) -> Result<()> {
        let header = framing::write_ask_response_header(
            crate::MessageType::Response,
            correlation_id,
            payload.len(),
        );

        self.stream_handle
            .write_header_and_payload_control_inline(header, 16, payload)
            .await
    }

    /// Send a response using the inline write queue (never streaming).
    ///
    /// # Arguments
    /// * `correlation_id` - The correlation ID from the original request
    /// * `payload` - The response payload bytes
    pub async fn send_response_auto(&self, correlation_id: u16, payload: &[u8]) -> Result<()> {
        self.stream_handle
            .send_response_auto(payload, correlation_id)
            .await
    }

    /// Send a response with owned Bytes using the inline write queue (never streaming).
    ///
    /// # Arguments
    /// * `correlation_id` - The correlation ID from the original request
    /// * `payload` - The response payload as owned Bytes
    pub async fn send_response_auto_bytes(
        &self,
        correlation_id: u16,
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.stream_handle
            .send_response_auto_bytes(correlation_id, payload)
            .await
    }

    /// Send a response payload using a Buf without copying.
    pub async fn send_response_buf<B>(
        &self,
        correlation_id: u16,
        payload: B,
        payload_len: usize,
    ) -> Result<()>
    where
        B: Buf + Send + 'static,
    {
        let header = framing::write_ask_response_header(
            crate::MessageType::Response,
            correlation_id,
            payload_len,
        );
        let buf = bytes::Bytes::copy_from_slice(&header).chain(payload); // ALLOW_COPY
        self.stream_handle.write_buf_control(buf).await
    }

    /// Send a response payload using a pooled payload without dynamic dispatch.
    pub async fn send_response_pooled(
        &self,
        correlation_id: u16,
        payload: crate::typed::PooledPayload,
        prefix: Option<[u8; 16]>,
        payload_len: usize,
    ) -> Result<()> {
        let header = framing::write_ask_response_header(
            crate::MessageType::Response,
            correlation_id,
            payload_len,
        );
        let prefix_len = prefix.as_ref().map(|p| p.len()).unwrap_or(0) as u8;
        self.stream_handle
            .write_pooled_control_inline(header, 16, prefix, prefix_len, payload)
            .await
    }

    /// Send bytes without copying - TRUE ZERO-COPY
    pub async fn send_bytes_zero_copy(&self, data: bytes::Bytes) -> Result<()> {
        self.stream_handle.write_bytes_control(data).await
    }

    /// Stream a large message directly - MAXIMUM PERFORMANCE
    pub async fn stream_large_message(
        &self,
        msg: &[u8],
        type_hash: u32,
        actor_id: u64,
    ) -> Result<()> {
        self.stream_handle
            .stream_large_message(msg, type_hash, actor_id)
            .await
    }

    /// Get the streaming threshold for this connection
    ///
    /// Messages larger than this threshold should be sent via streaming
    /// rather than through the write queue to prevent message loss.
    pub fn streaming_threshold(&self) -> usize {
        self.stream_handle.streaming_threshold()
    }

    /// Tell using owned bytes to avoid payload copies.
    pub async fn tell_bytes(&self, data: bytes::Bytes) -> Result<()> {
        let mut header = [0u8; 16];
        header[..4].copy_from_slice(&(data.len() as u32).to_be_bytes());

        self.stream_handle
            .write_header_and_payload_control_inline(header, 4, data)
            .await
    }

    /// Tell an actor using the direct actor frame envelope (MessageType::ActorTell).
    pub async fn tell_actor_frame(
        &self,
        actor_id: u64,
        type_hash: u32,
        payload: bytes::Bytes,
    ) -> Result<()> {
        let schema_hash = self.stream_handle.schema_hash();
        let header = crate::framing::write_actor_frame_header(
            crate::MessageType::ActorTell,
            0,
            actor_id,
            type_hash,
            schema_hash,
            payload.len(),
        );
        self.stream_handle
            .write_header_and_payload_control_inline32(header, payload)
            .await
    }

    /// Try to send an ActorTell frame without awaiting on the write queue.
    ///
    /// Returns `GossipError::WriteQueueFull` if the per-connection write queue is full.
    pub fn try_tell_actor_frame(
        &self,
        actor_id: u64,
        type_hash: u32,
        payload: bytes::Bytes,
    ) -> Result<()> {
        let schema_hash = self.stream_handle.schema_hash();
        let header = crate::framing::write_actor_frame_header(
            crate::MessageType::ActorTell,
            0,
            actor_id,
            type_hash,
            schema_hash,
            payload.len(),
        );
        self.stream_handle
            .write_header_and_payload_control_inline32_nonblocking(header, payload)
    }

    /// Ask an actor using the direct actor frame envelope (MessageType::ActorAsk).
    pub async fn ask_actor_frame(
        &self,
        actor_id: u64,
        type_hash: u32,
        payload: bytes::Bytes,
        timeout: Duration,
    ) -> Result<bytes::Bytes> {
        let response = self
            .ask_actor_frame_aligned(actor_id, type_hash, payload, timeout)
            .await?;
        Ok(response.into_bytes())
    }

    /// Ask an actor using the direct actor frame envelope (MessageType::ActorAsk), aligned response.
    pub async fn ask_actor_frame_aligned(
        &self,
        actor_id: u64,
        type_hash: u32,
        payload: bytes::Bytes,
        timeout: Duration,
    ) -> Result<crate::AlignedBytes> {
        let correlation_id = self.correlation.allocate();
        let schema_hash = self.stream_handle.schema_hash();
        let header = crate::framing::write_actor_frame_header(
            crate::MessageType::ActorAsk,
            correlation_id,
            actor_id,
            type_hash,
            schema_hash,
            payload.len(),
        );
        if let Err(e) = self
            .stream_handle
            .write_header_and_payload_ask_inline32(header, payload)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        self.correlation
            .wait_for_response(correlation_id, timeout)
            .await
    }

    /// Ask an actor using the direct actor frame envelope without timeout allocation.
    pub async fn ask_actor_frame_no_timeout(
        &self,
        actor_id: u64,
        type_hash: u32,
        payload: bytes::Bytes,
    ) -> Result<bytes::Bytes> {
        let response = self
            .ask_actor_frame_no_timeout_aligned(actor_id, type_hash, payload)
            .await?;
        Ok(response.into_bytes())
    }

    /// Ask an actor using the direct actor frame envelope without timeout allocation, aligned response.
    pub async fn ask_actor_frame_no_timeout_aligned(
        &self,
        actor_id: u64,
        type_hash: u32,
        payload: bytes::Bytes,
    ) -> Result<crate::AlignedBytes> {
        let correlation_id = self.correlation.allocate();
        let schema_hash = self.stream_handle.schema_hash();
        let header = crate::framing::write_actor_frame_header(
            crate::MessageType::ActorAsk,
            correlation_id,
            actor_id,
            type_hash,
            schema_hash,
            payload.len(),
        );

        if let Err(e) = self
            .stream_handle
            // Ask path: hold an ask permit until written to avoid unbounded write queueing.
            .write_header_and_payload_ask_inline32(header, payload)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        self.correlation
            .wait_for_response_no_timeout(correlation_id)
            .await
    }

    /// Tell with typed payload (rkyv) and debug-only type hash verification.
    pub async fn tell_typed<T>(&self, value: &T) -> Result<()>
    where
        T: crate::typed::WireEncode,
    {
        let payload = crate::typed::encode_typed_pooled(value)?;
        let (payload, prefix, payload_len) = crate::typed::typed_payload_parts::<T>(payload);
        let mut header = [0u8; 16];
        header[..4].copy_from_slice(&(payload_len as u32).to_be_bytes());
        let prefix_len = prefix.as_ref().map(|p| p.len()).unwrap_or(0) as u8;
        self.stream_handle
            .write_pooled_control_inline(header, 4, prefix, prefix_len, payload)
            .await
    }

    /// Send a pre-formatted binary message (already has length prefix)
    pub async fn send_binary_message(&self, message: &[u8]) -> Result<()> {
        // Message already has length prefix, send as-is
        self.stream_handle
            .write_bytes_control(bytes::Bytes::copy_from_slice(message)) // ALLOW_COPY
            .await
    }

    /// Send a single tell payload.
    pub async fn tell<T: AsRef<[u8]>>(&self, message: T) -> Result<()> {
        self.tell_bytes(
            bytes::Bytes::copy_from_slice(message.as_ref()), /* ALLOW_COPY */
        )
        .await
    }

    /// Direct access to try_send for maximum performance testing
    pub fn try_send_direct(&self, _data: &[u8]) -> Result<()> {
        // Direct TCP doesn't support try_send - would need try_lock
        Err(GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::WouldBlock,
            "use tell() for direct TCP writes",
        )))
    }

    /// Send raw bytes through existing connection (zero-copy where possible)
    pub async fn send_raw(&self, data: &[u8]) -> Result<()> {
        // Direct TCP write
        self.tell_bytes(bytes::Bytes::copy_from_slice(data) /* ALLOW_COPY */)
            .await
    }

    /// Ask method for request-response.
    /// Returns response as Bytes to avoid allocation.
    pub async fn ask(&self, request: &[u8]) -> Result<bytes::Bytes> {
        // Use default timeout of 30 seconds
        self.ask_with_timeout_bytes(
            bytes::Bytes::copy_from_slice(request), /* ALLOW_COPY */
            Duration::from_secs(30),
        )
        .await
    }

    /// Ask using owned bytes to avoid payload copies. Returns response as Bytes.
    pub async fn ask_bytes(&self, request: bytes::Bytes) -> Result<bytes::Bytes> {
        self.ask_with_timeout_bytes(request, Duration::from_secs(30))
            .await
    }

    /// Ask with typed request/response (rkyv) and debug-only type hash verification.
    pub async fn ask_typed<Req, Resp>(&self, request: &Req) -> Result<Resp>
    where
        Req: crate::typed::WireEncode,
        Resp: crate::typed::WireType + rkyv::Archive,
        for<'a> Resp::Archived: rkyv::bytecheck::CheckBytes<
                rkyv::rancor::Strategy<
                    rkyv::validation::Validator<
                        rkyv::validation::archive::ArchiveValidator<'a>,
                        rkyv::validation::shared::SharedValidator,
                    >,
                    rkyv::rancor::Error,
                >,
            > + rkyv::Deserialize<Resp, rkyv::rancor::Strategy<rkyv::de::Pool, rkyv::rancor::Error>>,
    {
        let payload = crate::typed::encode_typed_pooled(request)?;
        let (payload, prefix, payload_len) = crate::typed::typed_payload_parts::<Req>(payload);
        let response = self
            .ask_with_timeout_pooled(payload, prefix, payload_len, Duration::from_secs(30))
            .await?;
        crate::typed::decode_typed::<Resp>(response.as_ref())
    }

    /// Ask with typed request/response and custom timeout.
    pub async fn ask_typed_with_timeout<Req, Resp>(
        &self,
        request: &Req,
        timeout: Duration,
    ) -> Result<Resp>
    where
        Req: crate::typed::WireEncode,
        Resp: crate::typed::WireType + rkyv::Archive,
        for<'a> Resp::Archived: rkyv::bytecheck::CheckBytes<
                rkyv::rancor::Strategy<
                    rkyv::validation::Validator<
                        rkyv::validation::archive::ArchiveValidator<'a>,
                        rkyv::validation::shared::SharedValidator,
                    >,
                    rkyv::rancor::Error,
                >,
            > + rkyv::Deserialize<Resp, rkyv::rancor::Strategy<rkyv::de::Pool, rkyv::rancor::Error>>,
    {
        let payload = crate::typed::encode_typed_pooled(request)?;
        let (payload, prefix, payload_len) = crate::typed::typed_payload_parts::<Req>(payload);
        let response = self
            .ask_with_timeout_pooled(payload, prefix, payload_len, timeout)
            .await?;
        crate::typed::decode_typed::<Resp>(response.as_ref())
    }

    /// Ask with typed request and return a zero-copy archived response.
    pub async fn ask_typed_archived<Req, Resp>(
        &self,
        request: &Req,
    ) -> Result<crate::typed::ArchivedBytes<Resp>>
    where
        Req: crate::typed::WireEncode,
        Resp: crate::typed::WireType + rkyv::Archive,
        for<'a> Resp::Archived: rkyv::Portable
            + rkyv::bytecheck::CheckBytes<
                rkyv::rancor::Strategy<
                    rkyv::validation::Validator<
                        rkyv::validation::archive::ArchiveValidator<'a>,
                        rkyv::validation::shared::SharedValidator,
                    >,
                    rkyv::rancor::Error,
                >,
            >,
    {
        let payload = crate::typed::encode_typed_pooled(request)?;
        let (payload, prefix, payload_len) = crate::typed::typed_payload_parts::<Req>(payload);
        let response = self
            .ask_with_timeout_pooled(payload, prefix, payload_len, Duration::from_secs(30))
            .await?;
        crate::typed::decode_typed_archived::<Resp>(response)
    }

    /// Ask with typed request and custom timeout, returning a zero-copy archived response.
    pub async fn ask_typed_archived_with_timeout<Req, Resp>(
        &self,
        request: &Req,
        timeout: Duration,
    ) -> Result<crate::typed::ArchivedBytes<Resp>>
    where
        Req: crate::typed::WireEncode,
        Resp: crate::typed::WireType + rkyv::Archive,
        for<'a> Resp::Archived: rkyv::Portable
            + rkyv::bytecheck::CheckBytes<
                rkyv::rancor::Strategy<
                    rkyv::validation::Validator<
                        rkyv::validation::archive::ArchiveValidator<'a>,
                        rkyv::validation::shared::SharedValidator,
                    >,
                    rkyv::rancor::Error,
                >,
            >,
    {
        let payload = crate::typed::encode_typed_pooled(request)?;
        let (payload, prefix, payload_len) = crate::typed::typed_payload_parts::<Req>(payload);
        let response = self
            .ask_with_timeout_pooled(payload, prefix, payload_len, timeout)
            .await?;
        crate::typed::decode_typed_archived::<Resp>(response)
    }

    async fn ask_with_timeout_pooled(
        &self,
        payload: crate::typed::PooledPayload,
        prefix: Option<[u8; 16]>,
        payload_len: usize,
        timeout: Duration,
    ) -> Result<bytes::Bytes> {
        let correlation_id = self.correlation.allocate();
        let header = framing::write_ask_response_header(
            crate::MessageType::Ask,
            correlation_id,
            payload_len,
        );
        let prefix_len = prefix.as_ref().map(|p| p.len()).unwrap_or(0) as u8;

        if let Err(e) = self
            .stream_handle
            .write_pooled_ask_inline(header, 16, prefix, prefix_len, payload)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        let response = self
            .correlation
            .wait_for_response(correlation_id, timeout)
            .await?;
        Ok(response.into_bytes())
    }

    /// Ask using owned bytes and custom timeout. Returns response as Bytes.
    pub async fn ask_with_timeout_bytes(
        &self,
        request: bytes::Bytes,
        timeout: Duration,
    ) -> Result<bytes::Bytes> {
        let correlation_id = self.correlation.allocate();

        let header = framing::write_ask_response_header(
            crate::MessageType::Ask,
            correlation_id,
            request.len(),
        );

        if let Err(e) = self
            .stream_handle
            .write_header_and_payload_ask_inline(header, 16, request)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        let response = self
            .correlation
            .wait_for_response(correlation_id, timeout)
            .await?;
        Ok(response.into_bytes())
    }

    /// Fast-path direct ask that bypasses the actor message handler.
    ///
    /// This is optimized for high-throughput request-response scenarios where
    /// the server can generate responses directly without spawning actor tasks.
    ///
    /// Wire format: [length:4][type:1][correlation_id:2][payload_len:4][payload:N]
    pub async fn ask_direct(
        &self,
        request: bytes::Bytes,
        timeout: Duration,
    ) -> Result<bytes::Bytes> {
        let correlation_id = self.correlation.allocate();

        // Build DirectAsk header
        let header = framing::write_direct_ask_header(correlation_id, request.len());

        // Write header + payload inline (fast path)
        if let Err(e) = self
            .stream_handle
            .write_direct_ask_inline(header, request)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        let response = self
            .correlation
            .wait_for_response(correlation_id, timeout)
            .await?;
        Ok(response.into_bytes())
    }

    /// Fast-path direct ask without timeout allocation (benchmarking/hot path).
    pub async fn ask_direct_no_timeout(&self, request: bytes::Bytes) -> Result<bytes::Bytes> {
        let correlation_id = self.correlation.allocate();

        // Build DirectAsk header
        let header = framing::write_direct_ask_header(correlation_id, request.len());

        // Write header + payload inline (fast path)
        if let Err(e) = self
            .stream_handle
            .write_direct_ask_inline(header, request)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        let response = self
            .correlation
            .wait_for_response_no_timeout(correlation_id)
            .await?;
        Ok(response.into_bytes())
    }

    /// Zero-copy streaming ask that takes Bytes directly.
    ///
    /// This version avoids copying the payload data by using Bytes::slice()
    /// to create views into the original buffer for each chunk. Only the
    /// small headers (52 bytes each) are copied.
    ///
    /// Use this when you already have a Bytes buffer to avoid an extra copy.
    ///
    /// # Arguments
    /// * `payload` - The message payload as Bytes (will be sliced, not copied)
    /// * `type_hash` - The type hash for the message
    /// * `actor_id` - The target actor ID
    /// * `timeout` - How long to wait for a response
    ///
    /// # Returns
    /// The response bytes from the actor
    pub async fn ask_streaming_bytes(
        &self,
        payload: bytes::Bytes,
        type_hash: u32,
        actor_id: u64,
        timeout: Duration,
    ) -> Result<bytes::Bytes> {
        use crate::{MessageType, StreamHeader, current_timestamp_nanos};

        let chunk_size = self.stream_handle.max_stream_chunk_size()?;

        // Allocate correlation ID for the response
        let correlation_id = self.correlation.allocate();

        // Acquire streaming mode atomically
        while self
            .stream_handle
            .streaming_active
            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)
            .is_err()
        {
            tokio::task::yield_now().await;
        }

        // Ensure we release streaming mode on exit
        let _guard = StreamingGuard {
            flag: self.stream_handle.streaming_active.clone(),
        };

        // Generate unique stream ID
        let stream_id = current_timestamp_nanos();

        let schema_hash = self.stream_handle.schema_hash();

        // Helper to build stream header bytes (52 bytes total for header-only messages)
        fn build_stream_header_bytes(
            msg_type: MessageType,
            header: &StreamHeader,
            correlation_id: u16,
            schema_hash: Option<u64>,
        ) -> bytes::Bytes {
            // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36]
            let inner_size = 12 + StreamHeader::SERIALIZED_SIZE;
            let mut message = bytes::BytesMut::with_capacity(4 + inner_size);

            message.extend_from_slice(&(inner_size as u32).to_be_bytes()); // ALLOW_COPY
            message.put_u8(msg_type as u8);
            message.extend_from_slice(&correlation_id.to_be_bytes()); // ALLOW_COPY
            let mut reserved = [0u8; 9];
            crate::framing::write_schema_hash(&mut reserved, schema_hash);
            message.extend_from_slice(&reserved); // ALLOW_COPY 9 reserved bytes
            message.extend_from_slice(&header.to_bytes()); // ALLOW_COPY
            message.freeze()
        }

        // Helper to build chunk header bytes (for use with vectored write)
        fn build_chunk_header_bytes(
            header: &StreamHeader,
            correlation_id: u16,
            chunk_len: usize,
            schema_hash: Option<u64>,
        ) -> bytes::Bytes {
            // Message format: [length:4][type:1][correlation_id:2][reserved:9][header:36]
            // (chunk data follows separately via vectored write)
            let inner_size = 12 + StreamHeader::SERIALIZED_SIZE + chunk_len;
            let mut message =
                bytes::BytesMut::with_capacity(4 + 12 + StreamHeader::SERIALIZED_SIZE);

            message.extend_from_slice(&(inner_size as u32).to_be_bytes()); // ALLOW_COPY
            message.put_u8(MessageType::StreamData as u8);
            message.extend_from_slice(&correlation_id.to_be_bytes()); // ALLOW_COPY
            let mut reserved = [0u8; 9];
            crate::framing::write_schema_hash(&mut reserved, schema_hash);
            message.extend_from_slice(&reserved); // ALLOW_COPY 9 reserved bytes
            message.extend_from_slice(&header.to_bytes()); // ALLOW_COPY
            message.freeze()
        }

        let total_size = payload.len();

        // Send StreamStart header
        let start_header = StreamHeader {
            stream_id,
            total_size: total_size as u64,
            chunk_size: 0,
            chunk_index: 0,
            type_hash,
            actor_id,
        };

        let start_msg = build_stream_header_bytes(
            MessageType::StreamStart,
            &start_header,
            correlation_id,
            schema_hash,
        );
        if self
            .stream_handle
            .streaming_queue
            .try_push(StreamingCommand::WriteBytes(start_msg))
            .is_err()
        {
            self.correlation.cancel(correlation_id);
            return Err(GossipError::WriteQueueFull);
        }

        // Stream chunks using zero-copy slicing
        let mut offset = 0;
        let mut chunk_index = 0;

        while offset < total_size {
            let chunk_end = std::cmp::min(offset + chunk_size, total_size);
            let chunk_len = chunk_end - offset;

            // Zero-copy slice of the original Bytes buffer
            let chunk_data = payload.slice(offset..chunk_end);

            let data_header = StreamHeader {
                stream_id,
                total_size: total_size as u64,
                chunk_size: chunk_len as u32,
                chunk_index,
                type_hash,
                actor_id,
            };

            // Build header (52 bytes - small, ok to copy)
            let header_bytes =
                build_chunk_header_bytes(&data_header, correlation_id, chunk_len, schema_hash);

            // Send header + chunk data via OwnedChunks for vectored I/O
            // This avoids copying the chunk data into the header buffer
            if self
                .stream_handle
                .streaming_queue
                .try_push(StreamingCommand::OwnedChunks(vec![
                    header_bytes,
                    chunk_data,
                ]))
                .is_err()
            {
                self.correlation.cancel(correlation_id);
                return Err(GossipError::WriteQueueFull);
            }

            // Yield periodically to prevent blocking
            if chunk_index % 10 == 0 {
                let _ = self
                    .stream_handle
                    .streaming_queue
                    .try_push(StreamingCommand::Flush);
                tokio::task::yield_now().await;
            }

            offset = chunk_end;
            chunk_index += 1;
        }

        // Send StreamEnd
        let end_msg = build_stream_header_bytes(
            MessageType::StreamEnd,
            &start_header,
            correlation_id,
            schema_hash,
        );
        if self
            .stream_handle
            .streaming_queue
            .try_push(StreamingCommand::WriteBytes(end_msg))
            .is_err()
        {
            self.correlation.cancel(correlation_id);
            return Err(GossipError::WriteQueueFull);
        }
        let _ = self
            .stream_handle
            .streaming_queue
            .try_push(StreamingCommand::Flush);

        debug!(
            " STREAMING ASK (zero-copy): Streamed {} bytes in {} chunks, waiting for response",
            total_size, chunk_index
        );

        // Wait for response
        let response = self
            .correlation
            .wait_for_response(correlation_id, timeout)
            .await?;
        Ok(response.into_bytes())
    }

    /// Send an ask and return a handle that can be awaited later.
    ///
    /// This is useful for:
    /// - delegating the "wait for response" to another task,
    /// - checking correlation IDs for diagnostics/tests without misusing `ReplyTo`,
    /// - keeping resources bounded (dropping cancels the pending slot).
    pub async fn ask_deferred(&self, request: &[u8]) -> Result<PendingAsk> {
        self.ask_deferred_with_timeout_bytes(
            bytes::Bytes::copy_from_slice(request), /* ALLOW_COPY */
            Duration::from_secs(30),
        )
        .await
    }

    /// Deferred ask using owned bytes and a custom timeout.
    pub async fn ask_deferred_with_timeout_bytes(
        &self,
        request: bytes::Bytes,
        timeout: Duration,
    ) -> Result<PendingAsk> {
        let correlation_id = self.correlation.allocate();

        let header = framing::write_ask_response_header(
            crate::MessageType::Ask,
            correlation_id,
            request.len(),
        );

        if let Err(e) = self
            .stream_handle
            .write_header_and_payload_ask_inline(header, 16, request)
            .await
        {
            self.correlation.cancel(correlation_id);
            return Err(e);
        }

        Ok(PendingAsk {
            correlation_id,
            correlation: self.correlation.clone(),
            timeout,
        })
    }

    /// Batch ask in a single write, returning deferred handles for each request.
    pub async fn ask_batch_deferred(
        &self,
        requests: &[&[u8]],
        timeout: Duration,
    ) -> Result<Vec<PendingAsk>> {
        if requests.is_empty() {
            return Ok(Vec::new());
        }

        let mut correlation_ids = Vec::with_capacity(requests.len());

        // Pre-calculate total message size to avoid growth reallocations.
        let total_size: usize = requests
            .iter()
            .map(|req| framing::ASK_RESPONSE_FRAME_HEADER_LEN + req.len())
            .sum();
        let mut batch_message = bytes::BytesMut::with_capacity(total_size);

        for request in requests {
            let correlation_id = self.correlation.allocate();
            correlation_ids.push(correlation_id);

            let header = framing::write_ask_response_header(
                crate::MessageType::Ask,
                correlation_id,
                request.len(),
            );
            batch_message.extend_from_slice(&header); // ALLOW_COPY
            batch_message.extend_from_slice(request); // ALLOW_COPY
        }

        if let Err(e) = self
            .stream_handle
            .write_bytes_ask(batch_message.freeze())
            .await
        {
            for correlation_id in correlation_ids {
                self.correlation.cancel(correlation_id);
            }
            return Err(e);
        }

        let handles = correlation_ids
            .into_iter()
            .map(|correlation_id| PendingAsk {
                correlation_id,
                correlation: self.correlation.clone(),
                timeout,
            })
            .collect();
        Ok(handles)
    }

    /// High-performance streaming API - send structured data with custom framing - LOCK-FREE
    pub async fn stream_send<T>(&self, data: &T) -> Result<()>
    where
        T: for<'a> rkyv::Serialize<
                rkyv::rancor::Strategy<
                    rkyv::ser::Serializer<
                        rkyv::util::AlignedVec,
                        rkyv::ser::allocator::ArenaHandle<'a>,
                        rkyv::ser::sharing::Share,
                    >,
                    rkyv::rancor::Error,
                >,
            >,
    {
        // Serialize the data using rkyv for maximum performance
        let payload = rkyv::to_bytes::<rkyv::rancor::Error>(data)
            .map_err(crate::GossipError::Serialization)?;

        // Create stream frame: [frame_type, channel_id, flags, seq_id[2], payload_len[4]]
        let frame_header = StreamFrameHeader {
            frame_type: StreamFrameType::Data as u8,
            channel_id: ChannelId::TellAsk as u8,
            flags: 0,
            sequence_id: self.stream_handle.next_frame_sequence_id(),
            payload_len: payload.len() as u32,
        };

        let header_bytes = rkyv::to_bytes::<rkyv::rancor::Error>(&frame_header)
            .map_err(crate::GossipError::Serialization)?;

        // Combine header and payload for single write
        let mut combined = bytes::BytesMut::with_capacity(header_bytes.len() + payload.len());
        combined.extend_from_slice(&header_bytes); // ALLOW_COPY
        combined.extend_from_slice(&payload); // ALLOW_COPY

        // Enqueue into the background writer - NO MUTEX!
        self.stream_handle
            .write_bytes_nonblocking(combined.freeze())
    }

    /// High-performance streaming API - send batch of structured data - LOCK-FREE
    pub async fn stream_send_batch<T>(&self, batch: &[T]) -> Result<()>
    where
        T: for<'a> rkyv::Serialize<
                rkyv::rancor::Strategy<
                    rkyv::ser::Serializer<
                        rkyv::util::AlignedVec,
                        rkyv::ser::allocator::ArenaHandle<'a>,
                        rkyv::ser::sharing::Share,
                    >,
                    rkyv::rancor::Error,
                >,
            >,
    {
        if batch.is_empty() {
            return Ok(());
        }

        // Pre-allocate buffer for entire batch
        let mut total_payload = Vec::new();

        for item in batch {
            let payload = rkyv::to_bytes::<rkyv::rancor::Error>(item)
                .map_err(crate::GossipError::Serialization)?;

            let frame_header = StreamFrameHeader {
                frame_type: StreamFrameType::Data as u8,
                channel_id: ChannelId::TellAsk as u8,
                flags: if std::ptr::eq(item, batch.last().unwrap()) {
                    0
                } else {
                    StreamFrameFlags::More as u8
                },
                sequence_id: self.stream_handle.next_frame_sequence_id(),
                payload_len: payload.len() as u32,
            };

            let header_bytes = rkyv::to_bytes::<rkyv::rancor::Error>(&frame_header)
                .map_err(crate::GossipError::Serialization)?;
            total_payload.extend_from_slice(&header_bytes); // ALLOW_COPY
            total_payload.extend_from_slice(&payload); // ALLOW_COPY
        }

        // Enqueue into the background writer - NO MUTEX!
        self.stream_handle
            .write_bytes_nonblocking(bytes::Bytes::from(total_payload))
    }

    /// Get truly lock-free streaming handle - direct access to the internal handle
    pub fn get_lock_free_stream(&self) -> &Arc<LockFreeStreamHandle> {
        &self.stream_handle
    }

    /// Zero-copy vectored write for header + payload in single syscall
    /// This eliminates the need to copy payload data into frame buffer
    pub async fn write_bytes_vectored(
        &self,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        self.stream_handle
            .write_bytes_vectored(header, payload)
            .await
    }

    /// Send owned chunks without copying - optimal for streaming large messages
    pub fn write_owned_chunks(&self, chunks: Vec<bytes::Bytes>) -> Result<()> {
        self.stream_handle.write_owned_chunks(chunks)
    }
}

impl ConnectionPool {
    pub fn new(max_connections: usize, connection_timeout: Duration) -> Self {
        Self::new_with_aligned_pool_size(
            max_connections,
            connection_timeout,
            crate::aligned::DEFAULT_ALIGNED_POOL_SIZE,
        )
    }

    pub fn new_with_aligned_pool_size(
        max_connections: usize,
        connection_timeout: Duration,
        aligned_pool_size: usize,
    ) -> Self {
        const POOL_SIZE: usize = 256;
        const BUFFER_SIZE: usize = TCP_BUFFER_SIZE / 128; // Smaller pool buffers (8KB default)
        let pool = Self {
            connections_by_peer: SccHashMap::default(),
            addr_to_peer_id: SccHashMap::default(),
            peer_id_to_addr: SccHashMap::default(),
            connections_by_addr: SccHashMap::default(),
            correlation_trackers: SccHashMap::default(),
            max_connections,
            connection_timeout,
            registry: ArcSwapWeak::new(std::sync::Weak::new()),
            message_buffer_pool: Arc::new(MessageBufferPool::new(POOL_SIZE, BUFFER_SIZE)),
            aligned_bytes_pool: Arc::new(crate::AlignedBytesPool::new(
                aligned_pool_size.max(crate::aligned::DEFAULT_ALIGNED_POOL_SIZE),
            )),
            connection_counter: AtomicUsize::new(0),
        };

        // Log the pool's address for debugging
        debug!(
            "CONNECTION POOL: Created new pool at {:p}",
            &pool as *const _
        );
        pool
    }

    /// Set the registry reference for handling incoming messages
    pub fn set_registry(&self, registry: std::sync::Arc<GossipRegistry>) {
        self.registry.store(std::sync::Arc::downgrade(&registry));
    }

    /// Shared pool for aligned receive buffers.
    pub fn aligned_bytes_pool(&self) -> Arc<crate::AlignedBytesPool> {
        self.aligned_bytes_pool.clone()
    }

    /// Allocate a pooled aligned buffer for streaming assembly.
    pub fn make_pooled_aligned_buffer(&self, len: usize) -> crate::PooledAlignedBuffer {
        crate::PooledAlignedBuffer::with_len(len, self.aligned_bytes_pool.clone())
    }

    fn clear_capabilities_for_addr(&self, addr: &SocketAddr) {
        if let Some(registry) = self.registry.load().upgrade() {
            registry.clear_peer_capabilities(addr);
        }
    }

    /// Store or update the address for a peer
    /// Only updates if no address is already configured for this peer
    pub fn update_node_address(&self, peer_id: &crate::PeerId, addr: SocketAddr) {
        // Check if we already have a configured address for this node
        if let Some(existing_addr) = self.peer_id_to_addr.read_sync(peer_id, |_, v| *v) {
            debug!(
                "CONNECTION POOL: Node {} already has configured address {}, not updating to ephemeral port {}",
                peer_id, existing_addr, addr
            );
            return;
        }

        // Only update if no address is configured
        if self
            .peer_id_to_addr
            .insert_sync(peer_id.clone(), addr)
            .is_ok()
        {
            let _ = self.addr_to_peer_id.upsert_sync(addr, peer_id.clone());
        }
        debug!(
            "CONNECTION POOL: Set initial address for node {} to {}",
            peer_id, addr
        );
    }

    /// Reindex an existing connection under a new logical address for the peer.
    ///
    /// This is needed when a peer connects FROM an ephemeral TCP port but advertises
    /// a different bind address in gossip. We need to update `connections_by_addr` so
    /// that lookups by the advertised address find the connection.
    pub fn reindex_connection_addr(&self, peer_id: &crate::PeerId, new_addr: SocketAddr) {
        // First, check if this peer still has an active connection
        // This guards against race conditions where disconnect happens between checks
        let Some(connection) = self
            .connections_by_peer
            .read_sync(peer_id, |_, v| v.clone())
        else {
            // Peer was disconnected, nothing to reindex.
            return;
        };

        // Check if new_addr is already indexed
        if let Some(existing_peer_id) = self.addr_to_peer_id.read_sync(&new_addr, |_, v| v.clone())
        {
            if existing_peer_id == *peer_id {
                // Already indexed under the advertised address for this peer.
                // But we still need to ensure the OLD (ephemeral) address is indexed too!
                // Without this, lookups by ephemeral address fail after gossip rounds.
                let old_addr = connection.addr;
                if old_addr != new_addr && !self.connections_by_addr.contains_sync(&old_addr) {
                    let _ = self
                        .connections_by_addr
                        .upsert_sync(old_addr, connection.clone());
                    let _ = self.addr_to_peer_id.upsert_sync(old_addr, peer_id.clone());
                    debug!(
                        old_addr = %old_addr,
                        new_addr = %new_addr,
                        peer_id = %peer_id,
                        " Added missing ephemeral address mapping"
                    );
                }
                return;
            } else {
                // Stale entry from different peer - remove it before reindexing
                // This can happen if an old connection wasn't fully cleaned up
                warn!(
                    "CONNECTION POOL: Removing stale address mapping {} (was peer {}, now peer {})",
                    new_addr, existing_peer_id, peer_id
                );
                let _ = self.connections_by_addr.remove_sync(&new_addr);
                let _ = self.addr_to_peer_id.remove_sync(&new_addr);
            }
        }

        let old_addr = connection.addr;

        // Double-check peer still exists (guard against concurrent disconnect)
        if !self.connections_by_peer.contains_sync(peer_id) {
            return;
        }

        // Insert the connection under the new (advertised) address
        let _ = self
            .connections_by_addr
            .upsert_sync(new_addr, connection.clone());
        let _ = self.addr_to_peer_id.upsert_sync(new_addr, peer_id.clone());
        // Also update peer_id_to_addr so disconnect uses the correct address
        let _ = self.peer_id_to_addr.upsert_sync(peer_id.clone(), new_addr);

        // IMPORTANT: Keep the old (ephemeral) address entry as well!
        // Inbound messages still arrive with the TCP source address (old_addr),
        // so we need both addresses to point to the same connection.
        // The old entry is NOT removed - both addresses are valid for this peer.
        if old_addr != new_addr {
            // Re-insert connection under old addr to ensure both addresses work
            let _ = self.connections_by_addr.upsert_sync(old_addr, connection);
            // Keep addr_to_peer_id for old_addr so lookups work
            let _ = self.addr_to_peer_id.upsert_sync(old_addr, peer_id.clone());
        }

        info!(
            old_addr = %old_addr,
            new_addr = %new_addr,
            peer_id = %peer_id,
            " Reindexed connection from ephemeral port to bind address"
        );
    }

    /// Get a connection by peer ID
    pub(crate) fn get_connection_by_peer_id(
        &self,
        peer_id: &crate::PeerId,
    ) -> Option<Arc<LockFreeConnection>> {
        // PRIMARY: Look up connection directly by peer ID
        if let Some(conn) = self
            .connections_by_peer
            .read_sync(peer_id, |_, v| v.clone())
        {
            if conn.is_connected() {
                debug!("CONNECTION POOL: Found connection for peer '{}'", peer_id);
                return Some(conn);
            }
            warn!(
                "CONNECTION POOL: Connection for peer '{}' is disconnected",
                peer_id
            );
        }

        // FALLBACK: Outbound connections may only be indexed by address.
        // Look up the address via peer_id_to_addr, then get the connection by address.
        if let Some(addr) = self.peer_id_to_addr.read_sync(peer_id, |_, v| *v) {
            if let Some(conn) = self.connections_by_addr.read_sync(&addr, |_, v| v.clone()) {
                if conn.is_connected() {
                    debug!(
                        "CONNECTION POOL: Found connection for peer '{}' via address fallback ({})",
                        peer_id, addr
                    );
                    // Index by peer_id for future lookups
                    let _ = self
                        .connections_by_peer
                        .upsert_sync(peer_id.clone(), conn.clone());
                    return Some(conn);
                }
            }
        }

        warn!(
            "CONNECTION POOL: No connection found for peer '{}'",
            peer_id
        );
        // Debug: show what nodes we do have connections for
        let mut connected_nodes: Vec<String> = Vec::new();
        self.connections_by_peer.iter_sync(|peer_id, _| {
            connected_nodes.push(peer_id.to_hex());
            true
        });
        warn!(
            "CONNECTION POOL: Available node connections: {:?}",
            connected_nodes
        );
        None
    }

    /// Get a connection by socket address
    pub(crate) fn get_connection_by_addr(
        &self,
        addr: &SocketAddr,
    ) -> Option<Arc<LockFreeConnection>> {
        let conn = self.connections_by_addr.read_sync(addr, |_, v| v.clone())?;
        conn.is_connected().then_some(conn)
    }

    /// Get the peer ID for a given socket address
    pub fn get_peer_id_by_addr(&self, addr: &SocketAddr) -> Option<crate::PeerId> {
        self.addr_to_peer_id.read_sync(addr, |_, v| v.clone())
    }

    /// Add an additional address mapping for a peer ID.
    /// Used when a peer connects from an ephemeral port that differs from their bind address.
    pub fn add_addr_to_peer_id(&self, addr: SocketAddr, peer_id: crate::PeerId) {
        debug!(
            "CONNECTION POOL: Adding additional address {} -> peer_id {}",
            addr, peer_id
        );
        let _ = self.addr_to_peer_id.upsert_sync(addr, peer_id);
    }

    /// Get the shared correlation tracker for a peer ID
    pub(crate) fn get_shared_correlation_tracker(
        &self,
        peer_id: &crate::PeerId,
    ) -> Option<Arc<CorrelationTracker>> {
        self.correlation_trackers
            .read_sync(peer_id, |_, v| v.clone())
    }

    /// Get or create a correlation tracker for a peer
    pub(crate) fn get_or_create_correlation_tracker(
        &self,
        peer_id: &crate::PeerId,
    ) -> Arc<CorrelationTracker> {
        let tracker = self
            .correlation_trackers
            .entry_sync(peer_id.clone())
            .or_insert_with(|| {
                debug!(
                    "CONNECTION POOL: Creating new correlation tracker for peer {}",
                    peer_id
                );
                CorrelationTracker::new()
            })
            .get()
            .clone();
        debug!(
            "CONNECTION POOL: Got correlation tracker for peer {} (total trackers: {})",
            peer_id,
            self.correlation_trackers.len()
        );
        tracker
    }

    /// Add a connection indexed by peer ID
    pub fn add_connection_by_peer_id(
        &self,
        peer_id: crate::PeerId,
        addr: SocketAddr,
        mut connection: Arc<LockFreeConnection>,
    ) -> bool {
        // Only set correlation tracker if the connection doesn't already have one
        if connection.correlation.is_none() {
            // Get or create shared correlation tracker for this peer
            let correlation_tracker = self.get_or_create_correlation_tracker(&peer_id);

            // Set the correlation tracker on the connection
            // We need to make the connection mutable
            if let Some(conn_mut) = Arc::get_mut(&mut connection) {
                conn_mut.correlation = Some(correlation_tracker);
            } else {
                warn!(
                    "CONNECTION POOL: Cannot set correlation tracker - Arc has multiple references"
                );
            }
        } else {
            // Connection already has a correlation tracker - ensure it's registered
            if let Some(ref correlation) = connection.correlation {
                let _ = self
                    .correlation_trackers
                    .upsert_sync(peer_id.clone(), correlation.clone());
                debug!(
                    "CONNECTION POOL: Registered existing correlation tracker for peer '{}'",
                    peer_id
                );
            }
        }

        // Update the address mappings
        let _ = self.addr_to_peer_id.upsert_sync(addr, peer_id.clone());

        debug!(
            "CONNECTION POOL: Added connection for peer '{}' (address: {})",
            peer_id, addr
        );

        // PRIMARY: Store the connection by peer ID
        let _ = self
            .connections_by_peer
            .upsert_sync(peer_id, connection.clone());

        // Also index by address for direct lookups
        let _ = self.connections_by_addr.upsert_sync(addr, connection);

        self.connection_counter.fetch_add(1, Ordering::AcqRel);
        true
    }

    /// Index an existing connection by an additional address.
    ///
    /// This is useful for incoming connections where the ephemeral TCP address
    /// differs from the peer's configured bind address. By indexing both addresses,
    /// response delivery can find the connection by the ephemeral address.
    pub fn index_connection_by_addr(&self, addr: SocketAddr, connection: Arc<LockFreeConnection>) {
        debug!(
            "CONNECTION POOL: Indexing connection by additional address {}",
            addr
        );
        let _ = self.connections_by_addr.upsert_sync(addr, connection);
    }

    /// Send data to a peer by ID
    pub fn send_to_peer_id(&self, peer_id: &crate::PeerId, data: &[u8]) -> Result<()> {
        debug!(
            "CONNECTION POOL: send_to_peer_id called for peer '{}', pool has {} peer connections",
            peer_id,
            self.connections_by_peer.len()
        );
        if let Some(connection) = self.get_connection_by_peer_id(peer_id) {
            if let Some(ref stream_handle) = connection.stream_handle {
                debug!(
                    "CONNECTION POOL: Sending {} bytes to peer '{}'",
                    data.len(),
                    peer_id
                );
                return stream_handle.write_bytes_nonblocking(
                    bytes::Bytes::copy_from_slice(data), /* ALLOW_COPY */
                );
            } else {
                warn!(peer_id = %peer_id, "Connection found but no stream handle");
            }
        } else {
            warn!(peer_id = %peer_id, "No connection found for peer");
        }
        Err(crate::GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            format!("Connection not found for peer {}", peer_id),
        )))
    }

    /// Send bytes to a peer by its ID (zero-copy version)
    pub fn send_bytes_to_peer_id(&self, peer_id: &crate::PeerId, data: bytes::Bytes) -> Result<()> {
        debug!(
            "CONNECTION POOL: send_bytes_to_peer_id called for peer '{}', pool has {} peer connections",
            peer_id,
            self.connections_by_peer.len()
        );
        if let Some(connection) = self.get_connection_by_peer_id(peer_id) {
            if let Some(ref stream_handle) = connection.stream_handle {
                debug!(
                    "CONNECTION POOL: Sending {} bytes to peer '{}'",
                    data.len(),
                    peer_id
                );
                return stream_handle.write_bytes_nonblocking(data);
            } else {
                warn!(peer_id = %peer_id, "Connection found but no stream handle");
            }
        } else {
            warn!(peer_id = %peer_id, "No connection found for peer");
        }
        Err(crate::GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            format!("Connection not found for peer {}", peer_id),
        )))
    }

    /// Get or create a lock-free connection - NO MUTEX NEEDED
    pub fn get_lock_free_connection(&self, addr: SocketAddr) -> Option<Arc<LockFreeConnection>> {
        self.connections_by_addr.read_sync(&addr, |_, v| v.clone())
    }

    /// Add a new lock-free connection - completely lock-free operation
    pub fn add_lock_free_connection(
        &self,
        addr: SocketAddr,
        tcp_stream: TcpStream,
    ) -> Result<Arc<LockFreeConnection>> {
        let connection_count = self.connection_counter.fetch_add(1, Ordering::AcqRel);

        if connection_count >= self.max_connections {
            self.connection_counter.fetch_sub(1, Ordering::AcqRel);
            return Err(crate::GossipError::Network(std::io::Error::other(format!(
                "Max connections ({}) reached",
                self.max_connections
            ))));
        }

        // Create lock-free streaming handle with exclusive socket ownership.
        let (buffer_config, schema_hash, read_context) = self
            .registry
            .load()
            .upgrade()
            .map(|registry| {
                let read_context = ReadContext {
                    registry_weak: Arc::downgrade(&registry),
                    peer_addr: addr,
                    peer_id: None,
                    max_message_size: registry.config.max_message_size,
                    aligned_pool: registry.connection_pool.aligned_bytes_pool(),
                    response_correlation: None,
                };
                (
                    BufferConfig::default()
                        .with_ask_inflight_limit(registry.config.ask_inflight_limit),
                    registry.config.schema_hash,
                    Some(read_context),
                )
            })
            .unwrap_or((BufferConfig::default(), None, None));

        let (stream_handle, writer_task_handle) = LockFreeStreamHandle::new(
            tcp_stream,
            addr,
            ChannelId::Global,
            buffer_config,
            schema_hash,
            read_context,
        );
        let stream_handle = Arc::new(stream_handle);

        let mut connection = LockFreeConnection::new(addr, ConnectionDirection::Outbound);
        connection.stream_handle = Some(stream_handle);
        connection.set_state(ConnectionState::Connected);
        connection.update_last_used();

        // Track the writer task handle (H-004).
        connection
            .task_tracker
            .set_writer(writer_task_handle.abort_handle());

        let connection_arc = Arc::new(connection);

        // Insert into lock-free hash map.
        let _ = self
            .connections_by_addr
            .upsert_sync(addr, connection_arc.clone());
        debug!(
            "CONNECTION POOL: Added lock-free connection to {} - pool now has {} connections",
            addr,
            self.connections_by_addr.len()
        );

        Ok(connection_arc)
    }

    /// Send data through lock-free connection - NO BLOCKING
    pub fn send_lock_free(&self, addr: SocketAddr, data: &[u8]) -> Result<()> {
        if let Some(connection) = self.get_lock_free_connection(addr) {
            if let Some(ref stream_handle) = connection.stream_handle {
                return stream_handle.write_bytes_nonblocking(
                    bytes::Bytes::copy_from_slice(data), /* ALLOW_COPY */
                );
            } else {
                warn!(addr = %addr, "Connection found but no stream handle");
            }
        } else {
            warn!(addr = %addr, "No connection found for address");
            let mut addrs: Vec<SocketAddr> = Vec::new();
            self.connections_by_addr.iter_sync(|addr, _| {
                addrs.push(*addr);
                true
            });
            warn!("Available connections: {:?}", addrs);
        }
        Err(crate::GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Connection not found",
        )))
    }

    /// Send header + payload without copying the payload.
    pub fn send_lock_free_parts(
        &self,
        addr: SocketAddr,
        header: bytes::Bytes,
        payload: bytes::Bytes,
    ) -> Result<()> {
        if let Some(connection) = self.get_lock_free_connection(addr) {
            if let Some(ref stream_handle) = connection.stream_handle {
                stream_handle.write_header_and_payload_nonblocking_checked(header, payload)?;
                return Ok(());
            } else {
                warn!(addr = %addr, "Connection found but no stream handle");
            }
        } else {
            warn!(addr = %addr, "No connection found for address");
        }
        Err(crate::GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Connection not found",
        )))
    }

    /// Try to send data through any available connection for a node
    /// This handles cases where we might have multiple connections (incoming/outgoing)
    pub fn send_to_node(
        &self,
        node_addr: SocketAddr,
        data: &[u8],
        _registry: &GossipRegistry,
    ) -> Result<()> {
        // First try direct lookup
        if let Ok(()) = self.send_lock_free(node_addr, data) {
            return Ok(());
        }

        // If that fails, look for any connection that could reach this node
        // This could be enhanced with a node ID -> connections mapping
        debug!(node_addr = %node_addr, "Direct send failed, looking for alternative connections");

        // For now, we'll rely on the caller to handle fallback strategies
        Err(crate::GossipError::Network(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            format!("No connection found for node {}", node_addr),
        )))
    }

    /// Remove a connection from the pool - lock-free operation
    pub fn remove_connection(&self, addr: SocketAddr) -> Option<Arc<LockFreeConnection>> {
        // First remove from address-based map
        if let Some((_, connection)) = self.connections_by_addr.remove_sync(&addr) {
            debug!(
                "CONNECTION POOL: Removed connection to {} - pool now has {} connections",
                addr,
                self.connections_by_addr.len()
            );

            // Also remove from node ID mapping
            if let Some((_, node_id)) = self.addr_to_peer_id.remove_sync(&addr) {
                if self.connections_by_peer.remove_sync(&node_id).is_some() {
                    debug!(
                        "CONNECTION POOL: Also removed connection by node ID '{}'",
                        node_id
                    );
                }
            }

            self.connection_counter.fetch_sub(1, Ordering::AcqRel);
            self.clear_capabilities_for_addr(&addr);

            // H-004: Abort background tasks (writer, reader) to prevent resource leaks
            connection.abort_tasks();

            Some(connection)
        } else {
            None
        }
    }

    /// Disconnect and remove a connection by peer ID
    pub fn disconnect_connection_by_peer_id(
        &self,
        peer_id: &crate::PeerId,
    ) -> Option<Arc<LockFreeConnection>> {
        if let Some((_, connection)) = self.connections_by_peer.remove_sync(peer_id) {
            // Remove peer_id_to_addr entry
            let _ = self.peer_id_to_addr.remove_sync(peer_id);

            // Remove correlation tracker to prevent memory leak (LEAK-001 fix)
            if self.correlation_trackers.remove_sync(peer_id).is_some() {
                debug!(
                    peer_id = %peer_id,
                    "Cleaned up correlation tracker for disconnected peer"
                );
            }

            // Remove ALL addr_to_peer_id entries for this peer
            // (may have multiple due to reindexing keeping both ephemeral and bind addresses)
            let mut addrs_to_remove: Vec<SocketAddr> = Vec::new();
            self.addr_to_peer_id.iter_sync(|addr, pid| {
                if pid == peer_id {
                    addrs_to_remove.push(*addr);
                }
                true
            });

            for addr in &addrs_to_remove {
                let _ = self.addr_to_peer_id.remove_sync(addr);
                let _ = self.connections_by_addr.remove_sync(addr);
                self.clear_capabilities_for_addr(addr);
            }

            self.connection_counter.fetch_sub(1, Ordering::AcqRel);

            // H-004: Abort background tasks (writer, reader) to prevent resource leaks
            connection.abort_tasks();

            Some(connection)
        } else {
            None
        }
    }

    /// Get connection count - lock-free operation
    pub fn connection_count(&self) -> usize {
        let mut count = 0usize;
        self.connections_by_peer.iter_sync(|_, conn| {
            if conn.is_connected() {
                count += 1;
            }
            true
        });
        count
    }

    /// Get all connected peers - lock-free operation
    pub fn get_connected_peers(&self) -> Vec<SocketAddr> {
        let mut peers: Vec<SocketAddr> = Vec::new();
        self.connections_by_addr.iter_sync(|addr, conn| {
            if conn.is_connected() {
                peers.push(*addr);
            }
            true
        });
        peers
    }

    /// Get all connections (including disconnected) - for debugging
    pub fn get_all_connections(&self) -> Vec<SocketAddr> {
        let mut peers: Vec<SocketAddr> = Vec::new();
        self.connections_by_addr.iter_sync(|addr, _| {
            peers.push(*addr);
            true
        });
        peers
    }

    /// Get a buffer from the pool or create a new one
    pub fn get_buffer(&mut self, min_capacity: usize) -> Vec<u8> {
        // Use the message buffer pool for lock-free buffer management
        if let Some(buffer) = self.message_buffer_pool.get_buffer() {
            if buffer.capacity() >= min_capacity {
                return buffer;
            }
            // Buffer too small, return it and create new one
            self.message_buffer_pool.return_buffer(buffer);
        }
        Vec::with_capacity(min_capacity.max(1024)) // Minimum 1KB buffers
    }

    /// Return a buffer to the pool for reuse
    pub fn return_buffer(&mut self, buffer: Vec<u8>) {
        if buffer.capacity() >= 1024 && buffer.capacity() <= TCP_BUFFER_SIZE {
            // Return to the lock-free message buffer pool (up to TCP_BUFFER_SIZE)
            self.message_buffer_pool.return_buffer(buffer);
        }
        // Otherwise let the buffer drop
    }

    /// Get a message buffer from the pool for zero-copy processing
    pub fn get_message_buffer(&self) -> Vec<u8> {
        self.message_buffer_pool
            .get_buffer()
            .unwrap_or_else(|| Vec::with_capacity(TCP_BUFFER_SIZE / 256)) // Default small buffer
    }

    /// Return a message buffer to the pool
    pub fn return_message_buffer(&self, buffer: Vec<u8>) {
        if buffer.capacity() >= 1024 && buffer.capacity() <= TCP_BUFFER_SIZE {
            // Keep buffers with reasonable size (up to TCP_BUFFER_SIZE)
            self.message_buffer_pool.return_buffer(buffer);
        }
        // Otherwise let the buffer drop
    }

    /// Create a gossip message buffer with length header (optimized for reuse)
    pub fn create_message_buffer(&self, data: &[u8]) -> Vec<u8> {
        let header = framing::write_gossip_frame_prefix(data.len());
        let mut buffer = self
            .message_buffer_pool
            .get_buffer()
            .unwrap_or_else(|| Vec::with_capacity(header.len() + data.len()));
        buffer.extend_from_slice(&header); // ALLOW_COPY
        buffer.extend_from_slice(data); // ALLOW_COPY
        buffer
    }

    /// Get or create a persistent connection to a peer
    /// Fast path: Check for existing connection without creating new ones
    pub fn get_existing_connection(&self, addr: SocketAddr) -> Option<ConnectionHandle> {
        let _current_time = current_timestamp();

        let conn = self
            .connections_by_addr
            .read_sync(&addr, |_, v| v.clone())?;
        if !conn.is_connected() {
            debug!(addr = %addr, "removing disconnected connection");
            let _ = self.connections_by_addr.remove_sync(&addr);
            return None;
        }

        conn.update_last_used();
        debug!(addr = %addr, "using existing persistent connection (fast path)");

        let stream_handle = conn.stream_handle.as_ref()?.clone();

        // Look up peer_id to get shared correlation tracker.
        let peer_id = self.addr_to_peer_id.read_sync(&addr, |_, v| v.clone());

        // Use shared correlation tracker if we have a peer_id, otherwise use connection's tracker.
        let correlation = if let Some(pid) = peer_id {
            self.get_or_create_correlation_tracker(&pid)
        } else {
            conn.correlation
                .clone()
                .unwrap_or_else(CorrelationTracker::new)
        };

        Some(ConnectionHandle {
            addr,
            stream_handle,
            correlation,
        })
    }

    /// Get or create a connection to a peer by its ID
    pub(crate) async fn get_connection_to_peer(
        &self,
        peer_id: &crate::PeerId,
    ) -> Result<ConnectionHandle> {
        debug!(
            "CONNECTION POOL: get_connection_to_peer called for peer '{}'",
            peer_id
        );

        // First check if we already have a connection to this node
        if let Some(conn) = self
            .connections_by_peer
            .read_sync(peer_id, |_, v| v.clone())
        {
            if conn.is_connected() {
                conn.update_last_used();
                debug!(
                    "CONNECTION POOL: Found existing connection to peer '{}'",
                    peer_id
                );

                if let Some(ref stream_handle) = conn.stream_handle {
                    // If the IO task already exited, treat this entry as stale even if the
                    // connection state wasn't updated yet (race with failure handling).
                    if stream_handle.exit_flag.load(Ordering::Acquire) {
                        let addr = conn.addr;
                        debug!(
                            peer_id = %peer_id,
                            addr = %addr,
                            "CONNECTION POOL: existing peer entry has exited IO task; evicting"
                        );
                        let _ = self.remove_connection(addr);
                    } else {
                        // Need to get the address for ConnectionHandle
                        let addr = conn.addr;
                        // Use shared correlation tracker
                        let correlation = self.get_or_create_correlation_tracker(peer_id);
                        return Ok(ConnectionHandle {
                            addr,
                            stream_handle: stream_handle.clone(),
                            correlation,
                        });
                    }
                } else {
                    return Err(crate::GossipError::Network(std::io::Error::other(
                        "Connection exists but no stream handle",
                    )));
                }
            } else {
                // Remove disconnected connection
                debug!(
                    "CONNECTION POOL: Removing disconnected connection to peer '{}'",
                    peer_id
                );
                let _ = self.connections_by_peer.remove_sync(peer_id);
            }
        }

        // Look up the address for this node
        let addr = if let Some(addr) = self.peer_id_to_addr.read_sync(peer_id, |_, v| *v) {
            addr
        } else {
            return Err(crate::GossipError::Network(std::io::Error::new(
                std::io::ErrorKind::NotFound,
                format!("No address configured for peer '{}'", peer_id),
            )));
        };

        debug!(
            "CONNECTION POOL: Creating new connection to peer '{}' at {}",
            peer_id, addr
        );

        // Convert PeerId to NodeId for TLS
        let node_id_for_tls = Some(peer_id.to_node_id());

        // Create the connection and store it by node ID
        // Pass the NodeId so TLS can work even if gossip state doesn't have it yet
        let handle = self
            .get_connection_with_node_id(addr, node_id_for_tls)
            .await?;

        // After successful connection, ensure it's indexed by node ID
        if let Some(conn) = self.connections_by_addr.read_sync(&addr, |_, v| v.clone()) {
            let _ = self.connections_by_peer.upsert_sync(peer_id.clone(), conn);
            let _ = self.addr_to_peer_id.upsert_sync(addr, peer_id.clone());
            debug!(
                "CONNECTION POOL: Indexed new connection under peer ID '{}'",
                peer_id
            );
        }

        Ok(handle)
    }

    pub(crate) async fn get_connection(&self, addr: SocketAddr) -> Result<ConnectionHandle> {
        self.get_connection_with_node_id(addr, None).await
    }

    pub(crate) async fn get_connection_with_node_id(
        &self,
        addr: SocketAddr,
        node_id: Option<crate::NodeId>,
    ) -> Result<ConnectionHandle> {
        let _current_time = current_timestamp();
        let mut addr = addr;
        // Debug logging removed for performance - these logs were too verbose
        // debug!("CONNECTION POOL: get_connection called on pool at {:p} for {}", self as *const _, addr);
        // debug!("CONNECTION POOL: This pool instance has {} connections stored", self.connections_by_addr.len());

        // Use address index to reuse existing connections when available

        // Check if we already have a lock-free connection (fast path).
        if let Some(conn) = self.connections_by_addr.read_sync(&addr, |_, v| v.clone()) {
            if conn.is_connected() {
                if let Some(ref stream_handle) = conn.stream_handle {
                    // If IO task exited, treat as disconnected and force a fresh dial.
                    if stream_handle.exit_flag.load(Ordering::Acquire) {
                        debug!(addr = %addr, "found closed stream handle, removing stale connection");
                        conn.set_state(ConnectionState::Disconnected);
                        let _ = self.connections_by_addr.remove_sync(&addr);
                    } else {
                        conn.update_last_used();
                        debug!(addr = %addr, "found existing lock-free connection, reusing handle");

                        // Return the existing lock-free connection handle.
                        // Look up peer_id to get shared correlation tracker.
                        let peer_id = self.addr_to_peer_id.read_sync(&addr, |_, v| v.clone());

                        // Use shared correlation tracker if we have a peer_id, otherwise use connection's tracker.
                        let correlation = if let Some(pid) = peer_id {
                            self.get_or_create_correlation_tracker(&pid)
                        } else {
                            conn.correlation
                                .clone()
                                .unwrap_or_else(CorrelationTracker::new)
                        };

                        return Ok(ConnectionHandle {
                            addr,
                            stream_handle: stream_handle.clone(),
                            correlation,
                        });
                    }
                } else {
                    // Connection exists but no stream handle - this shouldn't happen.
                    return Err(crate::GossipError::Network(std::io::Error::other(
                        "Connection exists but no stream handle",
                    )));
                }
            } else {
                // Remove disconnected connection.
                debug!(addr = %addr, "removing disconnected connection");
                let _ = self.connections_by_addr.remove_sync(&addr);
            }
        }

        // Extract what we need before any await points to avoid Send issues
        let max_connections = self.max_connections;
        let connection_timeout = self.connection_timeout;
        let registry_weak = self.registry.load_full();
        let resolved_node_id = match node_id {
            Some(node_id) => Some(node_id),
            None => {
                if let Some(registry_arc) = registry_weak.upgrade() {
                    registry_arc.lookup_node_id(&addr).await.or_else(|| {
                        registry_arc
                            .peer_capability_addr_to_node
                            .read_sync(&addr, |_, v| *v)
                    })
                } else {
                    None
                }
            }
        };

        // Make room if necessary - evict the least-recently-used connection.
        if self.connections_by_addr.len() >= max_connections {
            let mut oldest: Option<(SocketAddr, usize)> = None;
            self.connections_by_addr.iter_sync(|addr, conn| {
                let last_used = conn.last_used.load(Ordering::Acquire);
                match oldest {
                    None => oldest = Some((*addr, last_used)),
                    Some((_, best_last_used)) => {
                        if last_used < best_last_used {
                            oldest = Some((*addr, last_used));
                        }
                    }
                }
                true
            });

            if let Some((oldest_addr, _)) = oldest {
                let _ = self.remove_connection(oldest_addr);
                warn!(addr = %oldest_addr, "removed oldest connection to make room");
            }
        }

        // Duplicate connection tie-breaker: decide whether to reuse an existing link
        if let (Some(registry_arc), Some(node_id_value)) =
            (registry_weak.upgrade(), resolved_node_id.as_ref())
        {
            let remote_peer_id = crate::PeerId::from(node_id_value);
            if let Some(existing_conn) = self.get_connection_by_peer_id(&remote_peer_id) {
                let alive = existing_conn.is_connected()
                    && existing_conn
                        .stream_handle
                        .as_ref()
                        .map(|h| !h.exit_flag.load(Ordering::Acquire))
                        .unwrap_or(false);
                if !alive {
                    debug!(
                        remote = %remote_peer_id,
                        addr = %existing_conn.addr,
                        "tie-breaker: evicting stale existing connection before dialing"
                    );
                    let _ = self.remove_connection(existing_conn.addr);
                } else if !registry_arc.should_keep_connection(&remote_peer_id, true) {
                    debug!(
                        remote = %remote_peer_id,
                        "tie-breaker: reusing existing connection instead of dialing outbound"
                    );
                    if let Some(ref stream_handle) = existing_conn.stream_handle {
                        // Use shared correlation tracker
                        let correlation = self.get_or_create_correlation_tracker(&remote_peer_id);
                        return Ok(ConnectionHandle {
                            addr: existing_conn.addr,
                            stream_handle: stream_handle.clone(),
                            correlation,
                        });
                    } else {
                        return Err(GossipError::Network(std::io::Error::other(
                            "Existing connection missing stream handle",
                        )));
                    }
                } else {
                    debug!(
                        remote = %remote_peer_id,
                        "tie-breaker: replacing existing connection with outbound dial"
                    );
                    if let Some(removed) = self.disconnect_connection_by_peer_id(&remote_peer_id) {
                        if let Some(handle) = removed.stream_handle.as_ref() {
                            handle.shutdown();
                        }
                    }
                }
            }
        }

        let mut dns_refreshed = false;
        let tls_stream = loop {
            let attempt: Result<_> = async {
                debug!("CONNECTION POOL: Attempting to connect to {}", addr);
                let stream = tokio::time::timeout(connection_timeout, TcpStream::connect(addr))
                    .await
                    .map_err(|_| {
                        debug!(
                            "CONNECTION POOL: Connection to {} timed out after {:?}",
                            addr, connection_timeout
                        );
                        GossipError::Timeout
                    })?
                    .map_err(|e| {
                        debug!(
                            "CONNECTION POOL: Connection to {} failed: {} (will retry in {}s if this is a gossip peer)",
                            addr, e, 5
                        ); // 5s is the default retry interval
                        GossipError::Network(e)
                    })?;
                debug!("CONNECTION POOL: Successfully connected to {}", addr);

                // Configure socket
                stream.set_nodelay(true).map_err(GossipError::Network)?;
                if let Some(registry_arc) = registry_weak.upgrade() {
                    crate::net::apply_tcp_keepalive(&stream, &registry_arc.config);
                }

                // Check if TLS is enabled
                let tls_stream = if let Some(registry_arc) = registry_weak.upgrade() {
                    if let Some(tls_config) = &registry_arc.tls_config {
                        // TLS is enabled - perform handshake as client
                        debug!("CONNECTION POOL: TLS enabled, checking for peer NodeId");

                        // Use provided NodeId if available, otherwise look it up from gossip state
                        let peer_node_id = resolved_node_id.or_else(|| {
                            registry_arc
                                .peer_capability_addr_to_node
                                .read_sync(&addr, |_, v| *v)
                        });

                        let mut discovered_node_id = peer_node_id;
                        let (server_name, server_name_label) =
                            if let Some(node_id) = discovered_node_id {
                                debug!(
                                    "CONNECTION POOL: Found NodeId for peer {}, performing TLS handshake",
                                    addr
                                );
                                let dns_name = crate::tls::name::encode(&node_id);
                                let server_name =
                                    rustls::pki_types::ServerName::try_from(dns_name).map_err(
                                        |e| GossipError::TlsError(format!("Invalid DNS name: {}", e)),
                                    )?;
                                (server_name, format!("NodeId {}", node_id.fmt_short()))
                            } else {
                                // Use placeholder DNS name so TLS can still negotiate; verifier will extract NodeId from cert
                                let placeholder = format!("peer-{}.kameo.invalid", addr.port());
                                let server_name =
                                    rustls::pki_types::ServerName::try_from(placeholder.clone())
                                        .map_err(|e| {
                                            GossipError::TlsError(format!(
                                                "Invalid fallback DNS name: {}",
                                                e
                                            ))
                                        })?;
                                (
                                    server_name,
                                    format!("placeholder SNI {} (NodeId unknown)", placeholder),
                                )
                            };

                        info!(
                            " TLS ENABLED: Initiating TLS connection to {} using {}",
                            addr, server_name_label
                        );
                        let connector =
                            tokio_rustls::TlsConnector::from(tls_config.client_config.clone());

                        match tokio::time::timeout(
                            Duration::from_secs(10),
                            connector.connect(server_name, stream),
                        )
                        .await
                        {
                            Ok(Ok(mut tls_stream)) => {
                                if discovered_node_id.is_none() {
                                    if let Some(certs) =
                                        tls_stream.get_ref().1.peer_certificates()
                                    {
                                        if let Some(cert) = certs.first() {
                                            match crate::tls::extract_node_id_from_cert(cert) {
                                                Ok(node_id) => {
                                                    debug!(
                                                        addr = %addr,
                                                        "Extracted NodeId {} from peer certificate",
                                                        node_id.fmt_short()
                                                    );
                                                    // Spawn to avoid deadlock: get_connection_with_node_id holds pool lock,
                                                    // and add_peer_with_node_id tries to acquire it.
                                                    let registry_clone = registry_arc.clone();
                                                    if registry_arc.lookup_node_id(&addr).await.is_none()
                                                    {
                                                        tokio::spawn(async move {
                                                            registry_clone
                                                                .add_peer_with_node_id(
                                                                    addr,
                                                                    Some(node_id),
                                                                )
                                                                .await;
                                                        });
                                                    }
                                                    discovered_node_id = Some(node_id);
                                                }
                                                Err(err) => {
                                                    warn!(
                                                        addr = %addr,
                                                        error = %err,
                                                        "Failed to extract NodeId from peer certificate"
                                                    );
                                                }
                                            }
                                        }
                                    }
                                }

                                if let Some(node_id) = discovered_node_id {
                                    info!(
                                        " TLS handshake successful with {} (NodeId: {})",
                                        addr,
                                        node_id.fmt_short()
                                    );
                                } else {
                                    info!(
                                        " TLS handshake successful with {} (NodeId unknown)",
                                        addr
                                    );
                                }

                                let negotiated_alpn = tls_stream
                                    .get_ref()
                                    .1
                                    .alpn_protocol()
                                    .map(|proto| proto.to_vec());

                                let enable_peer_discovery = registry_arc.config.enable_peer_discovery;
                                let peer_caps = crate::handshake::perform_hello_handshake(
                                    &mut tls_stream,
                                    negotiated_alpn.as_deref(),
                                    enable_peer_discovery,
                                )
                                .await?;
                                registry_arc.set_peer_capabilities(addr, peer_caps);

                                if let Some(node_id) = discovered_node_id.or_else(|| {
                                    registry_arc
                                        .lookup_node_id(&addr)
                                        .now_or_never()
                                        .flatten()
                                }) {
                                    registry_arc
                                        .associate_peer_capabilities_with_node(addr, node_id)
                                        .await;

                                    // Notify peer discovery that a connection is established (outgoing)
                                    // H-004: Spawn to avoid potential deadlock as we currently hold the Pool lock
                                    // and mark_peer_connected locks GossipState (safe) but potentially interacts back
                                    let registry_clone_for_mark = registry_arc.clone();
                                    tokio::spawn(async move {
                                        registry_clone_for_mark.mark_peer_connected(addr).await;
                                    });
                                }

                                Ok::<_, GossipError>(tls_stream)
                            }
                            Ok(Err(e)) => {
                                error!(addr = %addr, error = %e, " TLS handshake failed");
                                Err(GossipError::TlsError(format!(
                                    "TLS handshake failed: {}",
                                    e
                                )))
                            }
                            Err(_) => {
                                error!(
                                    addr = %addr,
                                    " TLS handshake timed out (NodeId hint: {:?})",
                                    discovered_node_id.as_ref().map(|id| id.fmt_short())
                                );
                                Err(GossipError::Timeout)
                            }
                        }?
                    } else {
                        // TLS is required for kameo_remote connections. Don't panic in background IO tasks;
                        // return a typed error so callers can mark the peer as failed / retry.
                        return Err(GossipError::TlsConfigError(format!(
                            "TLS is required but not configured (addr={})",
                            addr
                        )));
                    }
                } else {
                    // This can happen during shutdown or mis-wiring (pool used before registry sets the weak ref).
                    // Returning Shutdown avoids panics from background connection attempts.
                    return Err(GossipError::Shutdown);
                };

                Ok(tls_stream)
            }
            .await;

            match attempt {
                Ok(s) => break s,
                Err(e) => {
                    if !dns_refreshed {
                        if let Some(registry_arc) = registry_weak.upgrade() {
                            if let Some(new_addr) = registry_arc.refresh_peer_dns(addr).await {
                                addr = new_addr;
                                dns_refreshed = true;
                                continue;
                            }
                        }
                    }
                    return Err(e);
                }
            }
        };

        // Determine peer ID (if known) before creating the stream handle.
        let peer_id_opt = self
            .addr_to_peer_id
            .read_sync(&addr, |_, v| v.clone())
            .or_else(|| {
                // Try reverse lookup: find peer ID that maps to this address.
                let mut found: Option<crate::PeerId> = None;
                self.peer_id_to_addr.iter_sync(|peer_id, peer_addr| {
                    if peer_addr == &addr {
                        found = Some(peer_id.clone());
                        return false;
                    }
                    true
                });
                found
            });

        let correlation_tracker = peer_id_opt
            .as_ref()
            .map(|peer_id| self.get_or_create_correlation_tracker(peer_id))
            .unwrap_or_else(CorrelationTracker::new);

        // Create lock-free connection for receiving
        let (buffer_config, schema_hash, read_context) = registry_weak
            .upgrade()
            .map(|registry| {
                let read_context = ReadContext {
                    registry_weak: Arc::downgrade(&registry),
                    peer_addr: addr,
                    peer_id: peer_id_opt.clone(),
                    max_message_size: registry.config.max_message_size,
                    aligned_pool: registry.connection_pool.aligned_bytes_pool(),
                    response_correlation: Some(correlation_tracker.clone()),
                };
                (
                    BufferConfig::default()
                        .with_ask_inflight_limit(registry.config.ask_inflight_limit),
                    registry.config.schema_hash,
                    Some(read_context),
                )
            })
            .unwrap_or((BufferConfig::default(), None, None));
        let (stream_handle, writer_task_handle) = LockFreeStreamHandle::new(
            tls_stream,
            addr,
            ChannelId::Global,
            buffer_config,
            schema_hash,
            read_context,
        );
        let stream_handle = Arc::new(stream_handle);

        let mut conn = LockFreeConnection::new(addr, ConnectionDirection::Outbound);
        conn.stream_handle = Some(stream_handle.clone());
        conn.set_state(ConnectionState::Connected);
        conn.update_last_used();

        // Track the writer task handle (H-004).
        conn.task_tracker
            .set_writer(writer_task_handle.abort_handle());

        // For outgoing connections, we might know the peer ID from configuration
        if let Some(peer_id) = peer_id_opt.clone() {
            // Use shared correlation tracker for this peer
            conn.correlation = Some(correlation_tracker.clone());
            conn.embedded_peer_id = Some(peer_id.clone());
            debug!(
                "CONNECTION POOL: Using shared correlation tracker for peer {:?} at {}",
                peer_id, addr
            );
        } else {
            // No peer ID yet, create a new correlation tracker
            // This will be replaced when we learn the peer ID from their FullSync message
            conn.correlation = Some(correlation_tracker.clone());
            debug!(
                "CONNECTION POOL: Created new correlation tracker for unknown peer at {}",
                addr
            );
        }

        let connection_arc = Arc::new(conn);

        // Insert into lock-free map before spawning.
        let _ = self
            .connections_by_addr
            .upsert_sync(addr, connection_arc.clone());
        debug!(
            "CONNECTION POOL: Added connection via get_connection to {} - pool now has {} connections",
            addr,
            self.connections_by_addr.len()
        );
        // Double check it's really there
        assert!(
            self.connections_by_addr.contains_sync(&addr),
            "Connection was not added to pool!"
        );
        debug!("CONNECTION POOL: Verified connection exists for {}", addr);

        // Send initial FullSync message to identify ourselves
        if let Some(registry_arc) = registry_weak.upgrade() {
            let initial_msg = {
                let (local_actors, known_actors) = registry_arc.snapshot_actor_pairs();
                let gossip_state = registry_arc.gossip_state.lock().await;

                RegistryMessage::FullSync {
                    local_actors,
                    known_actors,
                    sender_peer_id: registry_arc.peer_id.clone(),
                    sender_bind_addr: Some(registry_arc.bind_addr.to_string()), // Use our listening address, not ephemeral port
                    sequence: gossip_state.gossip_sequence,
                    wall_clock_time: crate::current_timestamp(),
                }
            };

            // Serialize and send the initial message with Gossip type prefix
            match rkyv::to_bytes::<rkyv::rancor::Error>(&initial_msg) {
                Ok(data) => {
                    let header = framing::write_gossip_frame_prefix(data.len());
                    let mut msg_buffer = Vec::with_capacity(header.len() + data.len());
                    msg_buffer.extend_from_slice(&header); // ALLOW_COPY
                    msg_buffer.extend_from_slice(&data); // ALLOW_COPY

                    // Create a connection handle to send the message
                    let conn_handle = ConnectionHandle {
                        addr,
                        stream_handle: stream_handle.clone(),
                        correlation: connection_arc
                            .correlation
                            .clone()
                            .unwrap_or_else(CorrelationTracker::new),
                    };
                    if let Err(e) = conn_handle.send_data(msg_buffer).await {
                        warn!(peer = %addr, error = %e, "Failed to send initial FullSync message");
                    } else {
                        info!(peer = %addr, "Sent initial FullSync message to identify ourselves");
                    }
                }
                Err(e) => {
                    warn!(peer = %addr, error = %e, "Failed to serialize initial FullSync message");
                }
            }
        }

        // Note: actor_message_handler is fetched from registry on each message to handle
        // cases where the handler is registered after connection establishment

        // Reset failure state for this peer since we successfully connected.
        if let Some(registry) = registry_weak.upgrade() {
            let registry_clone = registry.clone();
            let peer_addr = addr;
            tokio::spawn(async move {
                let mut gossip_state = registry_clone.gossip_state.lock().await;

                // Check if we need to reset failures and clear pending
                let need_to_clear_pending = if let Some(peer_info) =
                    gossip_state.peers.get_mut(&peer_addr)
                {
                    let had_failures = peer_info.failures > 0;
                    if had_failures {
                        info!(peer = %peer_addr,
                                  prev_failures = peer_info.failures,
                                  " Successfully established outgoing connection - resetting failure state");
                        peer_info.failures = 0;
                        peer_info.last_failure_time = None;
                    }
                    peer_info.last_success = crate::current_timestamp();
                    had_failures
                } else {
                    false
                };

                // Clear pending failure record if needed
                if need_to_clear_pending {
                    gossip_state.pending_peer_failures.remove(&peer_addr);
                }
            });
        }

        info!(peer = %addr, "successfully created new persistent connection");

        // Verify the connection is in the pool
        debug!(
            "CONNECTION POOL: After get_connection, pool has {} connections",
            self.connections_by_addr.len()
        );
        debug!(
            "CONNECTION POOL: Pool contains connection to {}? {}",
            addr,
            self.connections_by_addr.contains_sync(&addr)
        );

        // Return a lock-free ConnectionHandle
        Ok(ConnectionHandle {
            addr,
            stream_handle,
            correlation: connection_arc
                .correlation
                .clone()
                .unwrap_or_else(CorrelationTracker::new),
        })
    }

    /// Mark a connection as disconnected
    pub fn mark_disconnected(&self, addr: SocketAddr) {
        if let Some(conn) = self.connections_by_addr.read_sync(&addr, |_, v| v.clone()) {
            conn.set_state(ConnectionState::Disconnected);
            info!(peer = %addr, "marked connection as disconnected");
        }
    }

    /// Remove a connection from the pool by address
    pub fn remove_connection_mut(&mut self, addr: SocketAddr) {
        if let Some((_, conn)) = self.connections_by_addr.remove_sync(&addr) {
            // H-004: Abort background tasks (writer, reader) to prevent resource leaks
            conn.abort_tasks();

            info!(addr = %addr, "removed connection from pool");
            // Dropping the sender will cause the receiver to return None,
            // signaling the connection handler to shut down
            // No need to drop writer
            self.clear_capabilities_for_addr(&addr);
        }
    }

    /// Check if we have a connection to a peer by address
    pub fn has_connection(&self, addr: &SocketAddr) -> bool {
        self.connections_by_addr
            .read_sync(addr, |_, v| v.is_connected())
            .unwrap_or(false)
    }

    /// Check if we have a connection to a peer by peer ID
    pub fn has_connection_by_peer_id(&self, peer_id: &crate::PeerId) -> bool {
        self.connections_by_peer
            .read_sync(peer_id, |_, v| v.is_connected())
            .unwrap_or(false)
    }

    /// Check health of all connections
    pub async fn check_connection_health(&self) -> Vec<SocketAddr> {
        // Health checking is now done by the persistent connection handlers
        Vec::new()
    }

    /// Clean up stale connections
    pub fn cleanup_stale_connections(&self) {
        // Find disconnected peers and use peer-id-based removal to clean up all maps
        let mut stale_peer_ids: Vec<crate::PeerId> = Vec::new();
        self.connections_by_peer.iter_sync(|peer_id, conn| {
            if !conn.is_connected() {
                stale_peer_ids.push(peer_id.clone());
            }
            true
        });

        for peer_id in stale_peer_ids {
            if let Some(_conn) = self.disconnect_connection_by_peer_id(&peer_id) {
                debug!(peer_id = %peer_id, "cleaned up disconnected connection (all aliases)");
            }
        }
    }

    /// Close all connections (for shutdown)
    pub fn close_all_connections(&self) {
        // Use peer-id-based removal to properly clean up all address aliases
        // This avoids double-decrement of connection_counter when a connection
        // has both ephemeral and bind addresses after reindexing
        let mut peer_ids: Vec<crate::PeerId> = Vec::new();
        self.connections_by_peer.iter_sync(|peer_id, _| {
            peer_ids.push(peer_id.clone());
            true
        });
        let count = peer_ids.len();
        for peer_id in peer_ids {
            self.disconnect_connection_by_peer_id(&peer_id);
        }
        // Also remove any remaining connections that were not indexed by peer_id
        // (e.g., outbound connections established before peer_id mapping exists).
        let mut addrs: Vec<SocketAddr> = Vec::new();
        self.connections_by_addr.iter_sync(|addr, _| {
            addrs.push(*addr);
            true
        });
        let mut addr_count = 0usize;
        for addr in addrs {
            if self.remove_connection(addr).is_some() {
                addr_count += 1;
            }
        }
        info!(
            "closed all {} connections ({} by peer_id, {} by addr-only)",
            count + addr_count,
            count,
            addr_count
        );
    }
    /// Handle persistent connection reader - only reads messages, no channels
    pub(crate) async fn handle_persistent_connection_reader(
        mut reader: tokio::net::tcp::OwnedReadHalf,
        _writer: Option<tokio::net::tcp::OwnedWriteHalf>,
        peer_addr: SocketAddr,
        registry_weak: Option<std::sync::Weak<GossipRegistry>>,
    ) {
        let max_message_size = registry_weak
            .as_ref()
            .and_then(|weak| weak.upgrade())
            .map(|registry| registry.config.max_message_size)
            .unwrap_or(10 * 1024 * 1024);
        let aligned_pool = registry_weak
            .as_ref()
            .and_then(|weak| weak.upgrade())
            .map(|registry| registry.connection_pool.aligned_bytes_pool());

        let mut streaming_state = crate::protocol::StreamingState::new();
        let mut cleanup_interval = tokio::time::interval(std::time::Duration::from_secs(30));
        cleanup_interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Delay);

        loop {
            let msg_result = tokio::select! {
                result = crate::handle::read_message_from_tls_reader(&mut reader, max_message_size, aligned_pool.as_ref()) => result,
                _ = cleanup_interval.tick() => {
                    streaming_state.cleanup_stale();
                    continue;
                }
            };

            match msg_result {
                Ok(result) => {
                    if let Some(registry) = registry_weak.as_ref().and_then(|w| w.upgrade()) {
                        if let Err(e) = crate::protocol::process_read_result(
                            result,
                            &mut streaming_state,
                            &registry,
                            peer_addr,
                            None,
                            None,
                        )
                        .await
                        {
                            warn!(peer = %peer_addr, error = %e, "Failed to process message on persistent connection");
                        }
                    } else {
                        warn!(peer = %peer_addr, "Registry dropped, stopping persistent connection reader");
                        break;
                    }
                }
                Err(e) => {
                    warn!(peer = %peer_addr, error = %e, "Persistent connection reader error");
                    break;
                }
            }
        }

        info!(peer = %peer_addr, "CONNECTION_POOL: Triggering peer failure handling");
        if let Some(ref registry_weak) = registry_weak {
            if let Some(registry) = registry_weak.upgrade() {
                if let Err(e) = registry.handle_peer_connection_failure(peer_addr).await {
                    warn!(error = %e, peer = %peer_addr, "CONNECTION_POOL: Failed to handle peer connection failure");
                }
            }
        }
    }
}

/// Resolve the peer state address for a sender.
async fn resolve_peer_state_addr(
    registry: &GossipRegistry,
    sender_peer_id: Option<&crate::PeerId>,
    socket_addr: SocketAddr,
) -> SocketAddr {
    if let Some(peer_id) = sender_peer_id {
        if let Some(addr) = {
            let pool = &registry.connection_pool;
            pool.peer_id_to_addr
                .read_sync(peer_id, |_, v| *v)
                .filter(|addr| addr.port() != 0)
        } {
            return addr;
        }

        if let Some(addr) = registry.lookup_advertised_addr(&peer_id.to_node_id()).await {
            return addr;
        }
    }

    if let Some(node_id) = registry
        .peer_capability_addr_to_node
        .read_sync(&socket_addr, |_, v| *v)
    {
        if let Some(addr) = registry.lookup_advertised_addr(&node_id).await {
            return addr;
        }
    }

    socket_addr
}

/// Handle an incoming message on a bidirectional connection
pub(crate) fn handle_incoming_message(
    registry: Arc<GossipRegistry>,
    _peer_addr: SocketAddr,
    msg: RegistryMessage,
) -> Pin<Box<dyn Future<Output = Result<()>> + Send>> {
    Box::pin(async move {
        match msg {
            RegistryMessage::DeltaGossip { delta } => {
                debug!(
                    sender = %delta.sender_peer_id,
                    since_sequence = delta.since_sequence,
                    changes = delta.changes.len(),
                    "received delta gossip message on bidirectional connection"
                );

                let sender_socket_addr =
                    resolve_peer_state_addr(&registry, Some(&delta.sender_peer_id), _peer_addr)
                        .await;

                // OPTIMIZATION: Do all peer management in one lock acquisition
                {
                    let mut gossip_state = registry.gossip_state.lock().await;

                    // Add the sender as a peer (inlined to avoid separate lock)
                    if delta.sender_peer_id != registry.peer_id {
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            gossip_state.peers.entry(sender_socket_addr)
                        {
                            let current_time = crate::current_timestamp();
                            e.insert(crate::registry::PeerInfo {
                                address: sender_socket_addr,
                                peer_address: None,
                                node_id: None,
                                dns_name: None,
                                failures: 0,
                                last_attempt: current_time,
                                last_success: current_time,
                                last_sequence: 0,
                                last_sent_sequence: 0,
                                consecutive_deltas: 0,
                                last_failure_time: None,
                                last_dns_refresh_attempt: None,
                            });
                        }
                    }

                    // Check if this is a previously failed peer
                    let was_failed = gossip_state
                        .peers
                        .get(&sender_socket_addr)
                        .map(|info| info.failures >= registry.config.max_peer_failures)
                        .unwrap_or(false);

                    if was_failed {
                        info!(
                            peer = %delta.sender_peer_id,
                            " Received delta from previously failed peer - connection restored!"
                        );

                        // Clear the pending failure record
                        gossip_state
                            .pending_peer_failures
                            .remove(&sender_socket_addr);
                    }

                    // Update peer info and check if we need to clear pending failures
                    let need_to_clear_pending =
                        if let Some(peer_info) = gossip_state.peers.get_mut(&sender_socket_addr) {
                            // Always reset failure state when we receive messages from the peer
                            // This proves the peer is alive and communicating
                            let had_failures = peer_info.failures > 0;
                            if had_failures {
                                info!(peer = %delta.sender_peer_id,
                              prev_failures = peer_info.failures,
                              " Resetting failure state after receiving DeltaGossip");
                                peer_info.failures = 0;
                                peer_info.last_failure_time = None;
                            }
                            peer_info.last_success = crate::current_timestamp();

                            peer_info.last_sequence =
                                std::cmp::max(peer_info.last_sequence, delta.current_sequence);
                            peer_info.consecutive_deltas += 1;

                            had_failures
                        } else {
                            false
                        };

                    // Clear pending failure record if needed
                    if need_to_clear_pending {
                        gossip_state
                            .pending_peer_failures
                            .remove(&sender_socket_addr);
                    }
                    gossip_state.delta_exchanges += 1;
                }

                // Collect immediate actors for ACK before consuming delta.
                let mut immediate_actors = Vec::new();
                for change in &delta.changes {
                    if let crate::registry::RegistryChange::ActorAdded { name, priority, .. } =
                        change
                    {
                        if priority.should_trigger_immediate_gossip() {
                            immediate_actors.push(name.clone());
                        }
                    }
                }

                // Apply the delta using the canonical registry logic (vector clocks +
                // deterministic tiebreakers). The previous "inline apply" fast-path had
                // multiple conflict-resolution implementations depending on lock contention,
                // which could cause nodes to diverge.
                registry.apply_delta(delta).await?;

                // NEW: Send ACK back for immediate registrations
                if !immediate_actors.is_empty() {
                    // Send ACKs for immediate priority actor additions
                    // Use lock-free send since we're responding on the same connection
                    for actor_name in immediate_actors {
                        // Send lightweight ACK immediately
                        let ack = crate::registry::RegistryMessage::ImmediateAck {
                            actor_name: actor_name.clone(),
                            success: true,
                        };

                        // Serialize and send
                        if let Ok(serialized) = rkyv::to_bytes::<rkyv::rancor::Error>(&ack) {
                            let pool = &registry.connection_pool;
                            let buffer = pool.create_message_buffer(&serialized);
                            // Use send_lock_free to send directly without needing a connection handle
                            if let Err(e) = pool.send_lock_free(sender_socket_addr, &buffer) {
                                warn!("Failed to send ImmediateAck: {}", e);
                            } else {
                                info!("Sent ImmediateAck for actor '{}'", actor_name);
                            }
                        }
                    }
                }

                // Note: Response will be sent during regular gossip rounds
                Ok(())
            }
            RegistryMessage::FullSync {
                local_actors,
                known_actors,
                sender_peer_id,
                sender_bind_addr,
                sequence,
                wall_clock_time,
            } => {
                // Use resolve_peer_addr for safe address resolution with validation
                // This handles: None, invalid addresses, 0.0.0.0, and falls back to TCP source
                let sender_socket_addr = resolve_peer_addr(sender_bind_addr.as_deref(), _peer_addr);

                // Note: sender_peer_id is now a PeerId (e.g., "node_a"), not an address
                debug!(
                    "Received FullSync from node '{}' at bind_addr {} (tcp_source={})",
                    sender_peer_id, sender_socket_addr, _peer_addr
                );

                // OPTIMIZATION: Do all peer management in one lock acquisition
                {
                    let mut gossip_state = registry.gossip_state.lock().await;

                    // FIX: If the resolved bind address differs from the TCP source address,
                    // migrate the PeerInfo from the ephemeral port entry to the bind address.
                    // This preserves node_id, sequence, and failure state learned during TLS handshake.
                    if sender_socket_addr != _peer_addr && _peer_addr != registry.bind_addr {
                        if let Some(mut old_peer_info) = gossip_state.peers.remove(&_peer_addr) {
                            info!(
                                old_addr = %_peer_addr,
                                new_addr = %sender_socket_addr,
                                node_id = ?old_peer_info.node_id,
                                " Migrating peer info from ephemeral TCP source to bind address from FullSync"
                            );
                            // Update the address field and preserve the connection address
                            old_peer_info.address = sender_socket_addr;
                            old_peer_info.peer_address = Some(_peer_addr);
                            // Insert with new key (bind address), preserving all state
                            gossip_state.peers.insert(sender_socket_addr, old_peer_info);
                            // Also clean up pending failures for the old address
                            gossip_state.pending_peer_failures.remove(&_peer_addr);
                        }
                    }

                    // Add the sender as a peer if not already present (inlined to avoid separate lock)
                    if sender_socket_addr != registry.bind_addr {
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            gossip_state.peers.entry(sender_socket_addr)
                        {
                            info!(peer = %sender_socket_addr, "Adding new peer from FullSync");
                            let current_time = crate::current_timestamp();
                            e.insert(crate::registry::PeerInfo {
                                address: sender_socket_addr,
                                peer_address: Some(_peer_addr), // Remember the actual connection address
                                node_id: None,
                                dns_name: None,
                                failures: 0,
                                last_attempt: current_time,
                                last_success: current_time,
                                last_sequence: 0,
                                last_sent_sequence: 0,
                                consecutive_deltas: 0,
                                last_failure_time: None,
                                last_dns_refresh_attempt: None,
                            });
                        }
                    }

                    // Update peer info and reset failure state
                    let had_failures = gossip_state
                        .peers
                        .get(&sender_socket_addr)
                        .map(|info| info.failures > 0)
                        .unwrap_or(false);

                    if had_failures {
                        // Clear the pending failure record
                        gossip_state
                            .pending_peer_failures
                            .remove(&sender_socket_addr);
                    }

                    if let Some(peer_info) = gossip_state.peers.get_mut(&sender_socket_addr) {
                        let prev_failures = peer_info.failures;
                        // Always reset failure state when we receive a FullSync from the peer
                        // This proves the peer is alive and communicating
                        if peer_info.failures > 0 {
                            info!(peer = %sender_socket_addr,
                              prev_failures = prev_failures,
                              " Resetting failure state after receiving FullSync");
                            peer_info.failures = 0;
                            peer_info.last_failure_time = None;
                        }
                        peer_info.last_success = crate::current_timestamp();
                        peer_info.consecutive_deltas = 0;
                    } else {
                        warn!(peer = %sender_socket_addr, "Peer not found in peer list when trying to reset failure state");
                    }
                    gossip_state.full_sync_exchanges += 1;
                }

                debug!(
                    sender = %sender_peer_id,
                    sequence = sequence,
                    local_actors = local_actors.len(),
                    known_actors = known_actors.len(),
                    " INCOMING: Received full sync message on bidirectional connection"
                );

                // IMPORTANT: Register the incoming connection with the peer_id mapping
                // This allows bidirectional communication to work properly
                {
                    let pool = &registry.connection_pool;

                    // NOTE: Do NOT remove addr_to_peer_id for the ephemeral address here.
                    // The reindex_connection_addr function preserves both addresses,
                    // and disconnect_connection_by_peer_id needs both entries to clean up properly.

                    let _ = pool
                        .peer_id_to_addr
                        .upsert_sync(sender_peer_id.clone(), sender_socket_addr);
                    let _ = pool
                        .addr_to_peer_id
                        .upsert_sync(sender_socket_addr, sender_peer_id.clone());

                    // CRITICAL FIX: Reindex the connection from ephemeral TCP port to bind address
                    // Without this, get_connection(bind_addr) fails because the connection is
                    // still indexed under the ephemeral port the peer connected FROM.
                    // This allows messages to be sent back to the peer using their advertised address.
                    // Note: reindex_connection_addr already has early-return if already indexed,
                    // and logs internally when it actually does work.
                    if sender_socket_addr != _peer_addr {
                        pool.reindex_connection_addr(&sender_peer_id, sender_socket_addr);
                    }

                    debug!(
                        "BIDIRECTIONAL: Registered incoming connection - peer_id={} addr={}",
                        sender_peer_id, sender_socket_addr
                    );
                }

                // Only remaining async operation
                registry
                    .merge_full_sync(
                        local_actors.into_iter().collect(),
                        known_actors.into_iter().collect(),
                        sender_peer_id.clone(),
                        sender_socket_addr,
                        sequence,
                        wall_clock_time,
                    )
                    .await;

                // Send back our state as a response so the sender can receive our actors
                // This is critical for late-joining nodes (like Node C) to get existing state
                {
                    // Get our current state
                    let (our_local_actors, our_known_actors, our_sequence) = {
                        let (local_actors, known_actors) = registry.snapshot_actor_pairs();
                        let gossip_state = registry.gossip_state.lock().await;
                        (local_actors, known_actors, gossip_state.gossip_sequence)
                    };

                    // Calculate sizes before moving
                    let local_actors_count = our_local_actors.len();
                    let known_actors_count = our_known_actors.len();

                    // Create a FullSyncResponse message
                    let response = RegistryMessage::FullSyncResponse {
                        local_actors: our_local_actors,
                        known_actors: our_known_actors,
                        sender_peer_id: registry.peer_id.clone(), // Use peer ID
                        sender_bind_addr: Some(registry.bind_addr.to_string()), // Our listening address
                        sequence: our_sequence,
                        wall_clock_time: crate::current_timestamp(),
                    };

                    // Send the response back through existing connection
                    // We'll use send_lock_free which doesn't create new connections
                    let response_data = match rkyv::to_bytes::<rkyv::rancor::Error>(&response) {
                        Ok(data) => data,
                        Err(e) => {
                            warn!(error = %e, "Failed to serialize FullSync response");
                            return Ok(());
                        }
                    };

                    // Try to send immediately on existing connection
                    {
                        debug!(
                            "FULLSYNC RESPONSE: Node {} is about to acquire connection pool lock",
                            registry.bind_addr
                        );
                        let pool = &registry.connection_pool;
                        debug!(
                            "FULLSYNC RESPONSE: Node {} got pool lock, pool has {} total entries",
                            registry.bind_addr,
                            pool.connection_count()
                        );
                        debug!("FULLSYNC RESPONSE: Pool instance address: {:p}", &*pool);

                        // Log details about each connection.
                        pool.connections_by_addr.iter_sync(|addr, conn| {
                            debug!(
                                "FULLSYNC RESPONSE: Connection to {} - state={:?}",
                                addr,
                                conn.get_state()
                            );
                            true
                        });

                        // Create message with length + type prefix
                        let buffer = pool.create_message_buffer(&response_data);

                        // Debug: Log what connections we have
                        let mut available_addrs: Vec<SocketAddr> = Vec::new();
                        pool.connections_by_addr.iter_sync(|addr, _| {
                            available_addrs.push(*addr);
                            true
                        });
                        debug!(
                            "FULLSYNC RESPONSE DEBUG: Available connections by addr: {:?}",
                            available_addrs
                        );
                        debug!("FULLSYNC RESPONSE DEBUG: Available node mappings: {:?}", {
                            let mut mappings: Vec<(crate::PeerId, SocketAddr)> = Vec::new();
                            pool.peer_id_to_addr.iter_sync(|peer_id, addr| {
                                mappings.push((peer_id.clone(), *addr));
                                true
                            });
                            mappings
                        });
                        debug!(
                            "FULLSYNC RESPONSE DEBUG: Looking for connection to sender_peer_id: {}",
                            sender_peer_id
                        );
                        debug!(
                            "FULLSYNC RESPONSE DEBUG: sender_socket_addr={}",
                            sender_socket_addr
                        );

                        // Try to send using peer ID
                        let frozen_buffer = bytes::Bytes::from(buffer);
                        let send_result = match pool
                            .send_bytes_to_peer_id(&sender_peer_id, frozen_buffer.clone())
                        {
                            Ok(()) => Ok(()),
                            Err(e) => {
                                warn!("Failed to send via peer ID {}: {}", sender_peer_id, e);
                                // Fall back to socket address
                                pool.send_lock_free(sender_socket_addr, &frozen_buffer)
                            }
                        };

                        if send_result.is_err() {
                            warn!(
                                "Primary send failed for peer {}, no fallback available",
                                sender_peer_id
                            );
                        }

                        match send_result {
                            Ok(()) => {
                                debug!(peer = %sender_socket_addr,
                                  peer_id = %sender_peer_id,
                                  local_actors = local_actors_count,
                                  known_actors = known_actors_count,
                                  bind_addr = %registry.bind_addr,
                                  " RESPONSE: Successfully sent FullSync response with our state");
                            }
                            Err(e) => {
                                // If we can't send immediately, queue it for the next gossip round
                                warn!(peer = %sender_socket_addr,
                                  peer_id = %sender_peer_id,
                                  error = %e,
                                  "Could not send FullSync response immediately - will be sent in next gossip round");

                                // Store in gossip state to be sent during next gossip round
                                let mut gossip_state = registry.gossip_state.lock().await;

                                // Mark that we need to send a full sync to this peer
                                if let Some(peer_info) =
                                    gossip_state.peers.get_mut(&sender_socket_addr)
                                {
                                    // Force a full sync on the next gossip round
                                    peer_info.consecutive_deltas =
                                        registry.config.max_delta_history as u64;
                                    info!(peer = %sender_socket_addr,
                                      "Marked peer for full sync in next gossip round");
                                }
                            }
                        }
                    }
                }

                Ok(())
            }
            RegistryMessage::FullSyncRequest {
                sender_peer_id,
                sender_bind_addr: _, // Not used for requests, but must be present
                sequence: _,
                wall_clock_time: _,
            } => {
                debug!(
                    sender = %sender_peer_id,
                    "received full sync request on bidirectional connection"
                );

                {
                    let mut gossip_state = registry.gossip_state.lock().await;
                    gossip_state.full_sync_exchanges += 1;
                }

                // Note: Response will be sent during regular gossip rounds
                Ok(())
            }
            // Handle response messages (these can arrive on incoming connections too)
            RegistryMessage::DeltaGossipResponse { delta } => {
                debug!(
                    sender = %delta.sender_peer_id,
                    changes = delta.changes.len(),
                    "received delta gossip response on bidirectional connection"
                );

                if let Err(err) = registry.apply_delta(delta).await {
                    warn!(error = %err, "failed to apply delta from response");
                } else {
                    let mut gossip_state = registry.gossip_state.lock().await;
                    gossip_state.delta_exchanges += 1;
                }
                Ok(())
            }
            RegistryMessage::FullSyncResponse {
                local_actors,
                known_actors,
                sender_peer_id,
                sender_bind_addr,
                sequence,
                wall_clock_time,
            } => {
                // Use resolve_peer_addr for safe address resolution with validation
                let sender_socket_addr = resolve_peer_addr(sender_bind_addr.as_deref(), _peer_addr);

                debug!(
                    sender = %sender_peer_id,
                    bind_addr = %sender_socket_addr,
                    tcp_source = %_peer_addr,
                    local_actors = local_actors.len(),
                    known_actors = known_actors.len(),
                    "RECEIVED: FullSyncResponse from peer (using bind_addr)"
                );

                registry
                    .merge_full_sync(
                        local_actors.into_iter().collect(),
                        known_actors.into_iter().collect(),
                        sender_peer_id.clone(),
                        sender_socket_addr,
                        sequence,
                        wall_clock_time,
                    )
                    .await;

                // FIX: Update peer_id mappings (mirror the FullSync handler logic)
                // This prevents stale ephemeral addresses from being reintroduced via resolve_peer_state_addr
                {
                    let pool = &registry.connection_pool;

                    // NOTE: Do NOT remove addr_to_peer_id for the ephemeral address here.
                    // The reindex_connection_addr function preserves both addresses,
                    // and disconnect_connection_by_peer_id needs both entries to clean up properly.

                    let _ = pool
                        .peer_id_to_addr
                        .upsert_sync(sender_peer_id.clone(), sender_socket_addr);
                    let _ = pool
                        .addr_to_peer_id
                        .upsert_sync(sender_socket_addr, sender_peer_id.clone());

                    // CRITICAL FIX: Reindex the connection from ephemeral TCP port to bind address
                    // Mirror the FullSync handler fix - allows sending to advertised address
                    // Note: reindex_connection_addr already has early-return if already indexed,
                    // and logs internally when it actually does work.
                    if sender_socket_addr != _peer_addr {
                        pool.reindex_connection_addr(&sender_peer_id, sender_socket_addr);
                    }

                    debug!(
                        "BIDIRECTIONAL: Updated connection mapping from FullSyncResponse - peer_id={} addr={}",
                        sender_peer_id, sender_socket_addr
                    );
                }

                // Reset failure state when receiving response
                let mut gossip_state = registry.gossip_state.lock().await;

                // FIX: If the resolved bind address differs from the TCP source address,
                // migrate the PeerInfo from the ephemeral port entry to the bind address.
                // This preserves node_id, sequence, and failure state learned during TLS handshake.
                if sender_socket_addr != _peer_addr && _peer_addr != registry.bind_addr {
                    if let Some(mut old_peer_info) = gossip_state.peers.remove(&_peer_addr) {
                        info!(
                            old_addr = %_peer_addr,
                            new_addr = %sender_socket_addr,
                            node_id = ?old_peer_info.node_id,
                            " Migrating peer info from ephemeral TCP source to bind address from FullSyncResponse"
                        );
                        // Update the address field and preserve the connection address
                        old_peer_info.address = sender_socket_addr;
                        old_peer_info.peer_address = Some(_peer_addr);
                        // Insert with new key (bind address), preserving all state
                        gossip_state.peers.insert(sender_socket_addr, old_peer_info);
                        // Also clean up pending failures for the old address
                        gossip_state.pending_peer_failures.remove(&_peer_addr);
                    }
                }

                // Reset failure state for responding peer
                let need_to_clear_pending =
                    if let Some(peer_info) = gossip_state.peers.get_mut(&sender_socket_addr) {
                        let had_failures = peer_info.failures > 0;
                        if had_failures {
                            info!(peer = %sender_socket_addr,
                          prev_failures = peer_info.failures,
                          " Resetting failure state after receiving FullSyncResponse");
                            peer_info.failures = 0;
                            peer_info.last_failure_time = None;
                        }
                        peer_info.last_success = crate::current_timestamp();
                        had_failures
                    } else {
                        false
                    };

                // Clear pending failure record if needed
                if need_to_clear_pending {
                    gossip_state
                        .pending_peer_failures
                        .remove(&sender_socket_addr);
                }

                gossip_state.full_sync_exchanges += 1;
                Ok(())
            }
            RegistryMessage::PeerHealthQuery {
                sender,
                target_peer,
                timestamp: _,
            } => {
                let sender_socket_addr =
                    resolve_peer_state_addr(&registry, Some(&sender), _peer_addr).await;
                debug!(
                    sender = %sender,
                    target = %target_peer,
                    "received peer health query"
                );

                // Check our connection status to the target peer
                let target_addr = match target_peer.parse::<SocketAddr>() {
                    Ok(addr) => addr,
                    Err(_) => {
                        warn!(
                            "Invalid target peer address in health query: {}",
                            target_peer
                        );
                        return Ok(());
                    }
                };

                let is_alive = {
                    let pool = &registry.connection_pool;
                    pool.has_connection(&target_addr)
                };

                let last_contact = if is_alive {
                    crate::current_timestamp()
                } else {
                    // Check when we last had successful contact
                    let gossip_state = registry.gossip_state.lock().await;
                    gossip_state
                        .peers
                        .get(&target_addr)
                        .map(|info| info.last_success)
                        .unwrap_or(0)
                };

                // Send our health report back
                let mut peer_statuses = HashMap::new();

                // Get actual failure count from gossip state
                let failure_count = {
                    let gossip_state = registry.gossip_state.lock().await;
                    gossip_state
                        .peers
                        .get(&target_addr)
                        .map(|info| info.failures as u32)
                        .unwrap_or(0)
                };

                peer_statuses.insert(
                    target_peer,
                    crate::registry::PeerHealthStatus {
                        is_alive,
                        last_contact,
                        failure_count,
                    },
                );

                let report = RegistryMessage::PeerHealthReport {
                    reporter: registry.peer_id.clone(),
                    peer_statuses: peer_statuses.into_iter().collect(),
                    timestamp: crate::current_timestamp(),
                };

                // Send report back to the querying peer
                if let Ok(data) = rkyv::to_bytes::<rkyv::rancor::Error>(&report) {
                    // Use the actual peer address we received from
                    let sender_addr = sender_socket_addr;

                    // Create message with length + type prefix
                    let pool = &registry.connection_pool;
                    let buffer = pool.create_message_buffer(&data);

                    // Use send_lock_free which doesn't create new connections
                    if let Err(e) = pool.send_lock_free(sender_addr, &buffer) {
                        warn!(peer = %sender_addr, error = %e, "Failed to send peer health report");
                    }
                }

                Ok(())
            }
            RegistryMessage::PeerHealthReport {
                reporter,
                peer_statuses,
                timestamp: _,
            } => {
                let reporter_addr =
                    resolve_peer_state_addr(&registry, Some(&reporter), _peer_addr).await;
                debug!(
                    reporter = %reporter,
                    peers = peer_statuses.len(),
                    "received peer health report"
                );

                // Store the health reports
                {
                    let mut gossip_state = registry.gossip_state.lock().await;
                    for (peer, status) in peer_statuses {
                        if let Ok(peer_addr) = peer.parse::<SocketAddr>() {
                            // For now, use the reporter's peer address from the connection
                            gossip_state
                                .peer_health_reports
                                .entry(peer_addr)
                                .or_insert_with(HashMap::new)
                                .insert(reporter_addr, status);
                        }
                    }
                }

                // Check if we have enough reports to make a decision
                registry.check_peer_consensus().await;

                Ok(())
            }
            RegistryMessage::ActorMessage { .. } => {
                warn!(
                    peer = %_peer_addr,
                    "Registry ActorMessage is no longer supported in v3; use ActorTell/ActorAsk frames"
                );
                Ok(())
            }

            RegistryMessage::ImmediateAck {
                actor_name,
                success,
            } => {
                debug!(
                    actor_name = %actor_name,
                    success = success,
                    "received immediate ACK for synchronous registration"
                );

                // Look up and complete the pending ACK waiter for this actor.
                if let Some((_, pending)) = registry.pending_acks.remove_sync(&actor_name) {
                    pending.complete(success);
                    info!(
                        actor_name = %actor_name,
                        success = success,
                        " Completed ACK for waiting synchronous registration"
                    );
                } else {
                    debug!(
                        actor_name = %actor_name,
                        "Received ACK but no pending registration found (may have timed out)"
                    );
                }

                Ok(())
            }

            RegistryMessage::PeerListGossip {
                peers,
                timestamp,
                sender_addr,
            } => {
                let peer_state_addr = resolve_peer_state_addr(&registry, None, _peer_addr).await;
                debug!(
                    peer_count = peers.len(),
                    timestamp = timestamp,
                    sender = %sender_addr,
                    "received peer list gossip message"
                );

                // Accept peer list only from connected peers
                if !registry.has_active_connection(&peer_state_addr).await {
                    debug!(
                        peer = %peer_state_addr,
                        "ignoring peer list gossip from non-connected peer"
                    );
                    return Ok(());
                }

                if !registry.peer_supports_peer_list(&peer_state_addr).await {
                    debug!(
                        peer = %peer_state_addr,
                        "ignoring peer list gossip from peer without capability"
                    );
                    return Ok(());
                }

                let candidates = registry
                    .on_peer_list_gossip(peers, &sender_addr, timestamp)
                    .await;

                if candidates.is_empty() {
                    return Ok(());
                }

                let registry_clone = registry.clone();
                let discovery_handle = tokio::spawn(async move {
                    for addr in candidates {
                        let node_id = registry_clone.lookup_node_id(&addr).await;
                        registry_clone.add_peer_with_node_id(addr, node_id).await;

                        match registry_clone.get_connection(addr).await {
                            Ok(_) => {
                                registry_clone.mark_peer_connected(addr).await;
                                debug!(peer = %addr, "connected to discovered peer");
                            }
                            Err(e) => {
                                registry_clone.mark_peer_failed(addr).await;
                                warn!(peer = %addr, error = %e, "failed to connect to discovered peer");
                            }
                        }
                    }
                });

                // Track the discovery task (H-004): keep at most one dial task alive.
                registry.discovery_task.set(discovery_handle.abort_handle());

                Ok(())
            }
        }
    })
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::io::{Error, ErrorKind};
    use std::pin::Pin;
    use std::sync::{Arc, Mutex};
    use std::task::{Context, Poll};
    use tokio::runtime::Builder;
    use tokio::time::sleep;

    const TEST_THREAD_STACK_SIZE: usize = 32 * 1024 * 1024; // Prevent stack overflow during large test runs
    const TEST_WORKER_STACK_SIZE: usize = 8 * 1024 * 1024;
    const TEST_WORKER_THREADS: usize = 4;

    fn run_multi_thread_test<F>(future: F)
    where
        F: Future<Output = ()> + Send + 'static,
    {
        let handle = std::thread::Builder::new()
            .name("kameo-conn-test".into())
            .stack_size(TEST_THREAD_STACK_SIZE)
            .spawn(move || {
                let rt = Builder::new_multi_thread()
                    .worker_threads(TEST_WORKER_THREADS)
                    .thread_stack_size(TEST_WORKER_STACK_SIZE)
                    .enable_all()
                    .build()
                    .expect("failed to build test runtime");
                rt.block_on(future);
            })
            .expect("failed to spawn test thread");
        handle.join().expect("test thread panicked unexpectedly");
    }

    /// Simple in-memory writer that records bytes for verification without
    /// requiring a TCP socket. Used to keep the send_data tests fully
    /// deterministic and stack-friendly.
    #[derive(Clone, Default)]
    struct RecordingWriter {
        buffer: Arc<Mutex<Vec<u8>>>,
    }

    impl RecordingWriter {
        fn new() -> (Self, Arc<Mutex<Vec<u8>>>) {
            let writer = Self::default();
            (writer.clone(), writer.buffer.clone())
        }
    }

    impl Unpin for RecordingWriter {}

    impl tokio::io::AsyncRead for RecordingWriter {
        fn poll_read(
            self: Pin<&mut Self>,
            _cx: &mut Context<'_>,
            _buf: &mut tokio::io::ReadBuf<'_>,
        ) -> Poll<std::io::Result<()>> {
            // No readable bytes. The IO task doesn't use reads in these tests (read_context=None),
            // but LockFreeStreamHandle requires AsyncRead + AsyncWrite.
            Poll::Ready(Ok(()))
        }
    }

    impl AsyncWrite for RecordingWriter {
        fn poll_write(
            self: Pin<&mut Self>,
            _cx: &mut Context<'_>,
            buf: &[u8],
        ) -> Poll<std::io::Result<usize>> {
            if let Ok(mut guard) = self.buffer.lock() {
                guard.extend_from_slice(buf); // ALLOW_COPY
            }
            Poll::Ready(Ok(buf.len()))
        }

        fn poll_flush(self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<std::io::Result<()>> {
            Poll::Ready(Ok(()))
        }

        fn poll_shutdown(self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<std::io::Result<()>> {
            Poll::Ready(Ok(()))
        }
    }

    #[derive(Clone, Copy, Default)]
    struct ClosedWriter;

    impl Unpin for ClosedWriter {}

    impl tokio::io::AsyncRead for ClosedWriter {
        fn poll_read(
            self: Pin<&mut Self>,
            _cx: &mut Context<'_>,
            _buf: &mut tokio::io::ReadBuf<'_>,
        ) -> Poll<std::io::Result<()>> {
            Poll::Ready(Ok(()))
        }
    }

    impl AsyncWrite for ClosedWriter {
        fn poll_write(
            self: Pin<&mut Self>,
            _cx: &mut Context<'_>,
            _buf: &[u8],
        ) -> Poll<std::io::Result<usize>> {
            Poll::Ready(Err(Error::new(ErrorKind::BrokenPipe, "writer closed")))
        }

        fn poll_flush(self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<std::io::Result<()>> {
            Poll::Ready(Err(Error::new(ErrorKind::BrokenPipe, "writer closed")))
        }

        fn poll_shutdown(self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<std::io::Result<()>> {
            Poll::Ready(Ok(()))
        }
    }

    #[test]
    fn test_connection_handle_debug() {
        // Compile-time test to ensure Debug is implemented
        use std::fmt::Debug;
        fn assert_debug<T: Debug>() {}
        assert_debug::<ConnectionHandle>();
    }

    #[test]
    fn test_buffer_config_validation() {
        // Should reject buffers < 256KB
        let result = BufferConfig::new(100 * 1024);
        assert!(result.is_err());

        // Should accept valid sizes
        let config = BufferConfig::new(512 * 1024).unwrap();
        assert_eq!(config.tcp_buffer_size(), 512 * 1024);

        // Streaming threshold should be buffer_size - 1KB
        assert_eq!(config.streaming_threshold(), 511 * 1024);
    }

    #[test]
    fn test_streaming_threshold_calculation() {
        let config = BufferConfig::new(1024 * 1024).unwrap();

        // 1MB buffer should have ~1MB-1KB threshold
        let threshold = config.streaming_threshold();
        assert!(threshold < config.tcp_buffer_size());
        assert!(threshold > 1020 * 1024); // At least 1020KB
        assert_eq!(threshold, 1023 * 1024); // Exactly 1023KB
    }

    #[test]
    fn test_buffer_config_default() {
        let config = BufferConfig::default();
        assert_eq!(config.tcp_buffer_size(), 1024 * 1024); // 1MB
        assert_eq!(config.streaming_threshold(), 1023 * 1024); // 1MB - 1KB
        assert_eq!(
            config.ask_inflight_limit(),
            crate::config::DEFAULT_ASK_INFLIGHT_LIMIT
        );
    }

    #[test]
    fn test_buffer_config_minimum_size() {
        // Test exactly at minimum boundary
        let config = BufferConfig::new(256 * 1024).unwrap();
        assert_eq!(config.tcp_buffer_size(), 256 * 1024);
        assert_eq!(config.streaming_threshold(), 255 * 1024);

        // Test just below minimum (should fail)
        let result = BufferConfig::new(256 * 1024 - 1);
        assert!(result.is_err());
    }

    #[test]
    fn test_streaming_threshold_saturation() {
        // Test that streaming_threshold handles edge cases properly
        let config = BufferConfig::new(256 * 1024).unwrap(); // Minimum buffer (256KB)
        // Should be 255KB (256KB - 1KB)
        assert_eq!(config.streaming_threshold(), 255 * 1024);

        // Test with exactly 1KB buffer would be rejected by validation,
        // but we can verify saturating_sub behavior directly
        let large_config = BufferConfig::new(2 * 1024 * 1024).unwrap(); // 2MB
        assert_eq!(large_config.streaming_threshold(), 2 * 1024 * 1024 - 1024);
    }

    #[test]
    fn test_should_flush_rules() {
        assert!(!should_flush(
            0,
            Duration::from_millis(1),
            4 * 1024,
            WRITER_MAX_LATENCY
        ));
        assert!(should_flush(
            4096,
            Duration::from_millis(1),
            4 * 1024,
            WRITER_MAX_LATENCY
        ));
        assert!(should_flush(
            1,
            Duration::from_millis(1),
            4 * 1024,
            WRITER_MAX_LATENCY
        ));
        assert!(should_flush(
            1,
            WRITER_MAX_LATENCY + Duration::from_micros(1),
            4 * 1024,
            WRITER_MAX_LATENCY
        ));
    }

    #[tokio::test]
    async fn test_connection_pool_new() {
        let pool = ConnectionPool::new(10, Duration::from_secs(5));
        assert_eq!(pool.connection_count(), 0);
        assert_eq!(pool.max_connections, 10);
        assert_eq!(pool.connection_timeout, Duration::from_secs(5));
    }

    #[tokio::test]
    async fn test_set_registry() {
        use crate::{GossipConfig, KeyPair, registry::GossipRegistry};
        let pool = ConnectionPool::new(10, Duration::from_secs(5));
        let registry = Arc::new(GossipRegistry::new(
            "127.0.0.1:8080".parse().unwrap(),
            GossipConfig {
                key_pair: Some(KeyPair::new_for_testing("conn_pool_registry")),
                ..Default::default()
            },
        ));

        pool.set_registry(registry.clone());
        assert!(pool.registry.load().upgrade().is_some());
    }

    #[test]
    fn test_connection_handle_send_data() {
        run_multi_thread_test(async {
            let (writer, recorded) = RecordingWriter::new();

            let (stream_handle, _writer_task) = LockFreeStreamHandle::new(
                writer,
                "127.0.0.1:8080".parse().unwrap(),
                ChannelId::Global,
                BufferConfig::default(),
                None,
                None,
            );
            let stream_handle = Arc::new(stream_handle);

            let handle = ConnectionHandle {
                addr: "127.0.0.1:8080".parse().unwrap(),
                stream_handle,
                correlation: CorrelationTracker::new(),
            };

            let data = vec![1, 2, 3, 4];
            handle.send_data(data.clone()).await.unwrap();

            // Allow the background writer to drain the queue
            sleep(Duration::from_millis(10)).await;

            let recorded = recorded.lock().unwrap().clone();
            assert_eq!(recorded, data);
        });
    }

    #[test]
    fn test_writer_owner_batch_preserves_order() {
        run_multi_thread_test(async {
            let (writer, recorded) = RecordingWriter::new();

            let (stream_handle, _writer_task) = LockFreeStreamHandle::new(
                writer,
                "127.0.0.1:8080".parse().unwrap(),
                ChannelId::Global,
                BufferConfig::default(),
                None,
                None,
            );

            let payloads = [
                bytes::Bytes::from_static(b"one"),
                bytes::Bytes::from_static(b"two"),
                bytes::Bytes::from_static(b"three"),
            ];

            for payload in &payloads {
                stream_handle
                    .write_bytes_nonblocking(payload.clone())
                    .expect("enqueue payload");
            }

            // Allow the background writer to drain the queue
            sleep(Duration::from_millis(10)).await;

            let recorded = recorded.lock().unwrap().clone();
            let expected = payloads.concat();
            assert_eq!(recorded, expected);
        });
    }

    #[test]
    fn test_writer_vectored_sequence_header_payload() {
        run_multi_thread_test(async {
            let (writer, recorded) = RecordingWriter::new();

            let (stream_handle, _writer_task) = LockFreeStreamHandle::new(
                writer,
                "127.0.0.1:8080".parse().unwrap(),
                ChannelId::Global,
                BufferConfig::default(),
                None,
                None,
            );

            let first = bytes::Bytes::from_static(b"first");
            let second = bytes::Bytes::from_static(b"second");
            let header = bytes::Bytes::from_static(b"HEAD");
            let payload = bytes::Bytes::from_static(b"PAYLOAD");

            stream_handle
                .write_bytes_nonblocking(first.clone())
                .expect("enqueue first");
            stream_handle
                .write_bytes_nonblocking(second.clone())
                .expect("enqueue second");
            stream_handle
                .write_header_and_payload_nonblocking(header.clone(), payload.clone())
                .expect("enqueue header+payload");

            // Allow the background writer to drain the queue
            sleep(Duration::from_millis(10)).await;

            let recorded = recorded.lock().unwrap().clone();
            let mut expected = Vec::new();
            expected.extend_from_slice(&first);
            expected.extend_from_slice(&second);
            expected.extend_from_slice(&header);
            expected.extend_from_slice(&payload);
            assert_eq!(recorded, expected);
        });
    }

    #[test]
    fn parse_direct_message_payload_success() {
        let mut frame = vec![crate::MessageType::DirectAsk as u8, 0x12, 0x34];
        frame.extend_from_slice(&(4u32).to_be_bytes()); /* ALLOW_COPY */
        frame.extend_from_slice(&[0u8; 5]); /* ALLOW_COPY */
        frame.extend_from_slice(b"PING"); /* ALLOW_COPY */

        let payload = super::parse_direct_message_payload(&frame).expect("parse ok");
        assert_eq!(payload, b"PING");
    }

    #[test]
    fn parse_direct_message_payload_truncated() {
        let mut frame = vec![crate::MessageType::DirectAsk as u8, 0x12, 0x34];
        frame.extend_from_slice(&(4u32).to_be_bytes()); /* ALLOW_COPY */
        frame.extend_from_slice(&[0u8; 5]); /* ALLOW_COPY */
        frame.extend_from_slice(b"PI"); /* ALLOW_COPY */

        match super::parse_direct_message_payload(&frame) {
            Err(super::DirectPayloadError::PayloadTruncated {
                expected,
                available,
            }) => {
                assert_eq!(expected, 4);
                assert_eq!(available, 2);
            }
            other => panic!("unexpected parse result: {:?}", other),
        }
    }

    #[test]
    fn parse_direct_message_payload_header_too_short() {
        let frame = vec![0u8; 3];
        assert_eq!(
            super::parse_direct_message_payload(&frame),
            Err(super::DirectPayloadError::HeaderTooShort)
        );
    }

    #[test]
    fn test_connection_handle_send_data_closed() {
        run_multi_thread_test(async {
            let (stream_handle, _writer_task) = LockFreeStreamHandle::new(
                ClosedWriter,
                "127.0.0.1:8080".parse().unwrap(),
                ChannelId::Global,
                BufferConfig::default(),
                None,
                None,
            );
            let stream_handle = Arc::new(stream_handle);

            let handle = ConnectionHandle {
                addr: "127.0.0.1:8080".parse().unwrap(),
                stream_handle,
                correlation: CorrelationTracker::new(),
            };

            let result = handle.send_data(vec![1, 2, 3]).await;
            assert!(result.is_ok());
        });
    }

    #[tokio::test]
    async fn test_task_tracker_aborts_on_drop() {
        use std::sync::atomic::{AtomicBool, Ordering};

        let task_started = Arc::new(AtomicBool::new(false));
        let task_completed = Arc::new(AtomicBool::new(false));
        let started_clone = task_started.clone();
        let completed_clone = task_completed.clone();

        let handle = tokio::spawn(async move {
            started_clone.store(true, Ordering::SeqCst);
            // Long sleep that should be aborted
            tokio::time::sleep(std::time::Duration::from_secs(60)).await;
            completed_clone.store(true, Ordering::SeqCst);
        });

        // Give task time to start
        tokio::time::sleep(std::time::Duration::from_millis(10)).await;
        assert!(
            task_started.load(Ordering::SeqCst),
            "Task should have started"
        );

        // Create tracker and set the handle
        let tracker = TaskTracker::new();
        tracker.set_writer(handle.abort_handle());

        // Drop the tracker - this should abort the task
        drop(tracker);

        // Give task time to be aborted
        tokio::time::sleep(std::time::Duration::from_millis(50)).await;

        // Task should NOT have completed (it was aborted)
        assert!(
            !task_completed.load(Ordering::SeqCst),
            "Task should have been aborted, not completed"
        );
    }

    #[tokio::test]
    async fn test_task_tracker_replaces_old_handle() {
        use std::sync::atomic::{AtomicBool, Ordering};

        let task2_started = Arc::new(AtomicBool::new(false));

        let handle1 = tokio::spawn(async move {
            // Long sleep that should be aborted when handle2 replaces it
            tokio::time::sleep(std::time::Duration::from_secs(60)).await;
        });

        let started_clone = task2_started.clone();
        let handle2 = tokio::spawn(async move {
            started_clone.store(true, Ordering::SeqCst);
            tokio::time::sleep(std::time::Duration::from_secs(60)).await;
        });

        let tracker = TaskTracker::new();

        // Set first handle
        tracker.set_writer(handle1.abort_handle());

        // Set second handle - first should be aborted
        tracker.set_writer(handle2.abort_handle());

        // Give task2 time to start
        tokio::time::sleep(std::time::Duration::from_millis(20)).await;
        assert!(
            task2_started.load(Ordering::SeqCst),
            "Second task should have started"
        );

        // Clean up
        drop(tracker);
    }

    #[tokio::test]
    async fn test_wait_for_response_returns_on_cancelled_slot() {
        let tracker = CorrelationTracker::new();
        let correlation_id = tracker.allocate();

        // Simulate a connection drop cancelling all pending requests.
        tracker.cancel_all();

        let res = tokio::time::timeout(
            std::time::Duration::from_millis(200),
            tracker.wait_for_response(correlation_id, std::time::Duration::from_millis(50)),
        )
        .await;

        let err = res
            .expect("wait_for_response hung")
            .expect_err("expected error");
        assert!(matches!(err, GossipError::Timeout));
    }
}
